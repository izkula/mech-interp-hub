<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Mechanistic Interpretability Hub</title>
    <link>https://izkula.github.io/cc</link>
    <description>Latest research in mechanistic interpretability - understanding how neural networks work internally</description>
    <language>en-us</language>
    <lastBuildDate>Tue, 20 Jan 2026 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://izkula.github.io/cc/feed.xml" rel="self" type="application/rss+xml"/>

    <item>
      <title>Research on the instability mechanism of transformer winding short-circuit faults based on magneto-mechanical coupling analysis</title>
      <link>https://www.semanticscholar.org/paper/48681d875dd4a7fd38cd0b9f05262cccabefe846</link>
      <description><![CDATA[<p><strong>Authors:</strong> Zhihao Gao, Jing Zhou, Li Zhu, Yuefeng Hao, Zhanlong Zhang, et al.</p><p><strong>Tags:</strong> circuits</p>]]></description>
      <pubDate>Sun, 01 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://www.semanticscholar.org/paper/48681d875dd4a7fd38cd0b9f05262cccabefe846</guid>
    </item>
    <item>
      <title>From Interpretability to Performance: Optimizing Retrieval Heads for Long-Context Language Models</title>
      <link>https://arxiv.org/abs/2601.11020v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Youmi Ma, Naoaki Okazaki</p><p>Advances in mechanistic interpretability have identified special attention heads, known as retrieval heads, that are responsible for retrieving information from the context. However, the role of these retrieval heads in improving model performance remains unexplored. This work investigates whether retrieval heads can be leveraged to enhance the long-context capabilities of LLMs. Specifically, we propose RetMask, a method that generates training signals by contrasting normal model outputs with th</p><p><strong>Tags:</strong> attention</p>]]></description>
      <pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.11020v1</guid>
    </item>
    <item>
      <title>From Knots to Knobs: Towards Steerable Collaborative Filtering Using Sparse Autoencoders</title>
      <link>https://arxiv.org/abs/2601.11182v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Martin Spišák, Ladislav Peška, Petr Škoda, Vojtěch Vančura, Rodrigo Alves</p><p>Sparse autoencoders (SAEs) have recently emerged as pivotal tools for introspection into large language models. SAEs can uncover high-quality, interpretable features at different levels of granularity and enable targeted steering of the generation process by selectively activating specific neurons in their latent activations. Our paper is the first to apply this approach to collaborative filtering, aiming to extract similarly interpretable features from representations learned purely from intera</p><p><strong>Tags:</strong> SAE, features, steering</p>]]></description>
      <pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.11182v1</guid>
    </item>
    <item>
      <title>Finding the Translation Switch: Discovering and Exploiting the Task-Initiation Features in LLMs</title>
      <link>https://arxiv.org/abs/2601.11019v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Xinwei Wu, Heng Liu, Xiaohu Zhao, Yuqi Ren, Linlong Xu, et al.</p><p>Large Language Models (LLMs) frequently exhibit strong translation abilities, even without task-specific fine-tuning. However, the internal mechanisms governing this innate capability remain largely opaque. To demystify this process, we leverage Sparse Autoencoders (SAEs) and introduce a novel framework for identifying task-specific features. Our method first recalls features that are frequently co-activated on translation inputs and then filters them for functional coherence using a PCA-based c</p><p><strong>Tags:</strong> SAE, features</p>]]></description>
      <pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.11019v1</guid>
    </item>
    <item>
      <title>Spurious Rewards Paradox: Mechanistically Understanding How RLVR Activates Memorization Shortcuts in LLMs</title>
      <link>https://arxiv.org/abs/2601.11061v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Lecheng Yan, Ruizhe Li, Guanhua Chen, Qing Li, Jiahui Geng, et al.</p><p>Reinforcement Learning with Verifiable Rewards (RLVR) is highly effective for enhancing LLM reasoning, yet recent evidence shows models like Qwen 2.5 achieve significant gains even with spurious or incorrect rewards. We investigate this phenomenon and identify a &quot;Perplexity Paradox&quot;: spurious RLVR triggers a divergence where answer-token perplexity drops while prompt-side coherence degrades, suggesting the model is bypassing reasoning in favor of memorization. Using Path Patching, Logit Lens, JS</p><p><strong>Tags:</strong> reasoning</p>]]></description>
      <pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.11061v1</guid>
    </item>
    <item>
      <title>TANDEM: Temporal-Aware Neural Detection for Multimodal Hate Speech</title>
      <link>https://arxiv.org/abs/2601.11178v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Girish A. Koushik, Helen Treharne, Diptesh Kanojia</p><p>Social media platforms are increasingly dominated by long-form multimodal content, where harmful narratives are constructed through a complex interplay of audio, visual, and textual cues. While automated systems can flag hate speech with high accuracy, they often function as &quot;black boxes&quot; that fail to provide the granular, interpretable evidence, such as precise timestamps and target identities, required for effective human-in-the-loop moderation. In this work, we introduce TANDEM, a unified fra</p><p><strong>Tags:</strong> safety, vision</p>]]></description>
      <pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.11178v1</guid>
    </item>
    <item>
      <title>Diagnosing Generalization Failures in Fine-Tuned LLMs: A Cross-Architectural Study on Phishing Detection</title>
      <link>https://arxiv.org/abs/2601.10524v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Frank Bobe, Gregory D. Vetaw, Chase Pavlick, Darshan Bryner, Matthew Cook, et al.</p><p>The practice of fine-tuning Large Language Models (LLMs) has achieved state-of-the-art performance on specialized tasks, yet diagnosing why these models become brittle and fail to generalize remains a critical open problem. To address this, we introduce and apply a multi-layered diagnostic framework to a cross-architectural study. We fine-tune Llama 3.1 8B, Gemma 2 9B, and Mistral models on a high-stakes phishing detection task and use SHAP analysis and mechanistic interpretability to uncover th</p>]]></description>
      <pubDate>Thu, 15 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.10524v1</guid>
    </item>
    <item>
      <title>Measuring Affinity between Attention-Head Weight Subspaces via the Projection Kernel</title>
      <link>https://arxiv.org/abs/2601.10266v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Hiroaki Yamagiwa, Yusuke Takase, Hidetoshi Shimodaira</p><p>Understanding relationships between attention heads is essential for interpreting the internal structure of Transformers, yet existing metrics do not capture this structure well. We focus on the subspaces spanned by attention-head weight matrices and quantify head-to-head relationships using the Projection Kernel (PK), a principal-angle-based measure of subspace similarity. Experiments show that PK reproduces known head-to-head interactions on the IOI task more clearly than prior metrics such as</p><p><strong>Tags:</strong> attention</p>]]></description>
      <pubDate>Thu, 15 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.10266v1</guid>
    </item>
    <item>
      <title>Reasoning Models Generate Societies of Thought</title>
      <link>https://arxiv.org/abs/2601.10825v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Junsol Kim, Shiyang Lai, Nino Scherrer, Blaise Agüera y Arcas, James Evans</p><p>Large language models have achieved remarkable capabilities across domains, yet mechanisms underlying sophisticated reasoning remain elusive. Recent reasoning models outperform comparable instruction-tuned models on complex cognitive tasks, attributed to extended computation through longer chains of thought. Here we show that enhanced reasoning emerges not from extended computation alone, but from simulating multi-agent-like interactions -- a society of thought -- which enables diversification a</p><p><strong>Tags:</strong> reasoning</p>]]></description>
      <pubDate>Thu, 15 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.10825v1</guid>
    </item>
    <item>
      <title>Where Knowledge Collides: A Mechanistic Study of Intra-Memory Knowledge Conflict in Language Models</title>
      <link>https://arxiv.org/abs/2601.09445v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Minh Vu Pham, Hsuvas Borkakoty, Yufang Hou</p><p>In language models (LMs), intra-memory knowledge conflict largely arises when inconsistent information about the same event is encoded within the model&#x27;s parametric knowledge. While prior work has primarily focused on resolving conflicts between a model&#x27;s internal knowledge and external resources through approaches such as fine-tuning or knowledge editing, the problem of localizing conflicts that originate during pre-training within the model&#x27;s internal representations remain unexplored. In this</p><p><strong>Tags:</strong> editing</p>]]></description>
      <pubDate>Wed, 14 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.09445v1</guid>
    </item>
    <item>
      <title>CLaS-Bench: A Cross-Lingual Alignment and Steering Benchmark</title>
      <link>https://arxiv.org/abs/2601.08331v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Daniil Gurgurov, Yusser Al Ghussin, Tanja Baeumel, Cheng-Ting Chou, Patrick Schramowski, et al.</p><p>Understanding and controlling the behavior of large language models (LLMs) is an increasingly important topic in multilingual NLP. Beyond prompting or fine-tuning, , i.e.,~manipulating internal representations during inference, has emerged as a more efficient and interpretable technique for adapting models to a target language. Yet, no dedicated benchmarks or evaluation protocols exist to quantify the effectiveness of steering techniques. We introduce CLaS-Bench, a lightweight parallel-question </p><p><strong>Tags:</strong> safety, steering</p>]]></description>
      <pubDate>Tue, 13 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.08331v1</guid>
    </item>
    <item>
      <title>Reasoning Beyond Chain-of-Thought: A Latent Computational Mode in Large Language Models</title>
      <link>https://arxiv.org/abs/2601.08058v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Zhenghao He, Guangzhi Xiong, Bohan Liu, Sanchit Sinha, Aidong Zhang</p><p>Chain-of-Thought (CoT) prompting has improved the reasoning performance of large language models (LLMs), but it remains unclear why it works and whether it is the unique mechanism for triggering reasoning in large language models. In this work, we study this question by directly analyzing and intervening on the internal representations of LLMs with Sparse Autoencoders (SAEs), identifying a small set of latent features that are causally associated with LLM reasoning behavior. Across multiple mode</p><p><strong>Tags:</strong> SAE, features, reasoning</p>]]></description>
      <pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.08058v1</guid>
    </item>
    <item>
      <title>Semantic Gravity Wells: Why Negative Constraints Backfire</title>
      <link>https://arxiv.org/abs/2601.08070v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Shailesh Rana</p><p>Negative constraints (instructions of the form &quot;do not use word X&quot;) represent a fundamental test of instruction-following capability in large language models. Despite their apparent simplicity, these constraints fail with striking regularity, and the conditions governing failure have remained poorly understood. This paper presents the first comprehensive mechanistic investigation of negative instruction failure. We introduce semantic pressure, a quantitative measure of the model&#x27;s intrinsic prob</p>]]></description>
      <pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.08070v1</guid>
    </item>
    <item>
      <title>Time Travel Engine: A Shared Latent Chronological Manifold Enables Historical Navigation in Large Language Models</title>
      <link>https://arxiv.org/abs/2601.06437v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Jingmin An, Wei Liu, Qian Wang, Fang Fang</p><p>Time functions as a fundamental dimension of human cognition, yet the mechanisms by which Large Language Models (LLMs) encode chronological progression remain opaque. We demonstrate that temporal information in their latent space is organized not as discrete clusters but as a continuous, traversable geometry. We introduce the Time Travel Engine (TTE), an interpretability-driven framework that projects diachronic linguistic patterns onto a shared chronological manifold. Unlike surface-level promp</p>]]></description>
      <pubDate>Sat, 10 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.06437v1</guid>
    </item>
    <item>
      <title>Deriving Decoder-Free Sparse Autoencoders from First Principles</title>
      <link>https://arxiv.org/abs/2601.06478v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Alan Oursland</p><p>Gradient descent on log-sum-exp (LSE) objectives performs implicit expectation--maximization (EM): the gradient with respect to each component output equals its responsibility. The same theory predicts collapse without volume control analogous to the log-determinant in Gaussian mixture models. We instantiate the theory in a single-layer encoder with an LSE objective and InfoMax regularization for volume control. Experiments confirm the theory&#x27;s predictions. The gradient--responsibility identity </p><p><strong>Tags:</strong> SAE, theory</p>]]></description>
      <pubDate>Sat, 10 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.06478v1</guid>
    </item>
    <item>
      <title>Do Sparse Autoencoders Identify Reasoning Features in Language Models?</title>
      <link>https://arxiv.org/abs/2601.05679v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> George Ma, Zhongyuan Liang, Irene Y. Chen, Somayeh Sojoudi</p><p>We investigate whether sparse autoencoders (SAEs) identify genuine reasoning features in large language models (LLMs). Starting from features selected using standard contrastive activation methods, we introduce a falsification-oriented framework that combines causal token injection experiments and LLM-guided falsification to test whether feature activation reflects reasoning processes or superficial linguistic correlates. Across 20 configurations spanning multiple model families, layers, and rea</p><p><strong>Tags:</strong> SAE, features, reasoning</p>]]></description>
      <pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.05679v1</guid>
    </item>
    <item>
      <title>Context-Aware Decoding for Faithful Vision-Language Generation</title>
      <link>https://arxiv.org/abs/2601.05939v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Mehrdad Fazli, Bowen Wei, Ziwei Zhu</p><p>Hallucinations, generating responses inconsistent with the visual input, remain a critical limitation of large vision-language models (LVLMs), especially in open-ended tasks such as image captioning and visual reasoning. In this work, we probe the layer-wise generation dynamics that drive hallucinations and propose a training-free mitigation strategy. Employing the Logit Lens, we examine how LVLMs construct next-token distributions across decoder layers, uncovering a pronounced commitment-depth </p><p><strong>Tags:</strong> probing, vision, reasoning</p>]]></description>
      <pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.05939v1</guid>
    </item>
    <item>
      <title>Circuit Mechanisms for Spatial Relation Generation in Diffusion Transformers</title>
      <link>https://arxiv.org/abs/2601.06338v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Binxu Wang, Jingxuan Fan, Xu Pan</p><p>Diffusion Transformers (DiTs) have greatly advanced text-to-image generation, but models still struggle to generate the correct spatial relations between objects as specified in the text prompt. In this study, we adopt a mechanistic interpretability approach to investigate how a DiT can generate correct spatial relations between objects. We train, from scratch, DiTs of different sizes with different text encoders to learn to generate images containing two objects whose attributes and spatial rel</p><p><strong>Tags:</strong> circuits, vision</p>]]></description>
      <pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.06338v1</guid>
    </item>
    <item>
      <title>LANGSAE EDITING: Improving Multilingual Information Retrieval via Post-hoc Language Identity Removal</title>
      <link>https://arxiv.org/abs/2601.04768v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Dongjun Kim, Jeongho Yoon, Chanjun Park, Heuiseok Lim</p><p>Dense retrieval in multilingual settings often searches over mixed-language collections, yet multilingual embeddings encode language identity alongside semantics. This language signal can inflate similarity for same-language pairs and crowd out relevant evidence written in other languages. We propose LANGSAE EDITING, a post-hoc sparse autoencoder trained on pooled embeddings that enables controllable removal of language-identity signal directly in vector space. The method identifies language-ass</p><p><strong>Tags:</strong> SAE</p>]]></description>
      <pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.04768v1</guid>
    </item>
    <item>
      <title>Learning Dynamics in RL Post-Training for Language Models</title>
      <link>https://arxiv.org/abs/2601.04670v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Akiyoshi Tomihari</p><p>Reinforcement learning (RL) post-training is a critical stage in modern language model development, playing a key role in improving alignment and reasoning ability. However, several phenomena remain poorly understood, including the reduction in output diversity. To gain a broader understanding of RL post-training, we analyze the learning dynamics of RL post-training from a perspective that has been studied in supervised learning but remains underexplored in RL. We adopt an empirical neural tange</p><p><strong>Tags:</strong> safety, reasoning</p>]]></description>
      <pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.04670v1</guid>
    </item>
    <item>
      <title>When Models Manipulate Manifolds: The Geometry of a Counting Task</title>
      <link>https://arxiv.org/abs/2601.04480v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Wes Gurnee, Emmanuel Ameisen, Isaac Kauvar, Julius Tarng, Adam Pearce, et al.</p><p>Language models can perceive visual properties of text despite receiving only sequences of tokens-we mechanistically investigate how Claude 3.5 Haiku accomplishes one such task: linebreaking in fixed-width text. We find that character counts are represented on low-dimensional curved manifolds discretized by sparse feature families, analogous to biological place cells. Accurate predictions emerge from a sequence of geometric transformations: token lengths are accumulated into character count mani</p><p><strong>Tags:</strong> features, vision, biology</p>]]></description>
      <pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.04480v1</guid>
    </item>
    <item>
      <title>Controllable LLM Reasoning via Sparse Autoencoder-Based Steering</title>
      <link>https://arxiv.org/abs/2601.03595v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Yi Fang, Wenjie Wang, Mingfeng Xue, Boyi Deng, Fengli Xu, et al.</p><p>Large Reasoning Models (LRMs) exhibit human-like cognitive reasoning strategies (e.g. backtracking, cross-verification) during reasoning process, which improves their performance on complex tasks. Currently, reasoning strategies are autonomously selected by LRMs themselves. However, such autonomous selection often produces inefficient or even erroneous reasoning paths. To make reasoning more reliable and flexible, it is important to develop methods for controlling reasoning strategies. Existing </p><p><strong>Tags:</strong> SAE, steering, reasoning</p>]]></description>
      <pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.03595v1</guid>
    </item>
    <item>
      <title>NeuronScope: A Multi-Agent Framework for Explaining Polysemantic Neurons in Language Models</title>
      <link>https://arxiv.org/abs/2601.03671v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Weiqi Liu, Yongliang Miao, Haiyan Zhao, Yanguang Liu, Mengnan Du</p><p>Neuron-level interpretation in large language models (LLMs) is fundamentally challenged by widespread polysemanticity, where individual neurons respond to multiple distinct semantic concepts. Existing single-pass interpretation methods struggle to faithfully capture such multi-concept behavior. In this work, we propose NeuronScope, a multi-agent framework that reformulates neuron interpretation as an iterative, activation-guided process. NeuronScope explicitly deconstructs neuron activations int</p><p><strong>Tags:</strong> features</p>]]></description>
      <pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.03671v1</guid>
    </item>
    <item>
      <title>Where meaning lives: Layer-wise accessibility of psycholinguistic features in encoder and decoder language models</title>
      <link>https://arxiv.org/abs/2601.03798v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Taisiia Tikhomirova, Dirk U. Wulff</p><p>Understanding where transformer language models encode psychologically meaningful aspects of meaning is essential for both theory and practice. We conduct a systematic layer-wise probing study of 58 psycholinguistic features across 10 transformer models, spanning encoder-only and decoder-only architectures, and compare three embedding extraction methods. We find that apparent localization of meaning is strongly method-dependent: contextualized embeddings yield higher feature-specific selectivity</p><p><strong>Tags:</strong> features, probing, theory</p>]]></description>
      <pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.03798v1</guid>
    </item>
    <item>
      <title>Interpreting Transformers Through Attention Head Intervention</title>
      <link>https://arxiv.org/abs/2601.04398v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Mason Kadem, Rong Zheng</p><p>Neural networks are growing more capable on their own, but we do not understand their neural mechanisms. Understanding these mechanisms&#x27; decision-making processes, or mechanistic interpretability, enables (1) accountability and control in high-stakes domains, (2) the study of digital brains and the emergence of cognition, and (3) discovery of new knowledge when AI systems outperform humans.</p><p><strong>Tags:</strong> attention</p>]]></description>
      <pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.04398v1</guid>
    </item>
    <item>
      <title>When the Coffee Feature Activates on Coffins: An Analysis of Feature Extraction and Steering for Mechanistic Interpretability</title>
      <link>https://arxiv.org/abs/2601.03047v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Raphael Ronge, Markus Maier, Frederick Eberhardt</p><p>Recent work by Anthropic on Mechanistic interpretability claims to understand and control Large Language Models by extracting human-interpretable features from their neural activation patterns using sparse autoencoders (SAEs). If successful, this approach offers one of the most promising routes for human oversight in AI safety. We conduct an initial stress-test of these claims by replicating their main results with open-source SAEs for Llama 3.1. While we successfully reproduce basic feature ext</p><p><strong>Tags:</strong> SAE, features, safety, steering</p>]]></description>
      <pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.03047v1</guid>
    </item>
    <item>
      <title>Mechanistic Interpretability of Large-Scale Counting in LLMs through a System-2 Strategy</title>
      <link>https://arxiv.org/abs/2601.02989v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Hosein Hasani, Mohammadali Banayeeanzade, Ali Nafisi, Sadegh Mohammadian, Fatemeh Askari, et al.</p><p>Large language models (LLMs), despite strong performance on complex mathematical problems, exhibit systematic limitations in counting tasks. This issue arises from architectural limits of transformers, where counting is performed across layers, leading to degraded precision for larger counting problems due to depth constraints. To address this limitation, we propose a simple test-time strategy inspired by System-2 cognitive processes that decomposes large counting tasks into smaller, independent</p>]]></description>
      <pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.02989v1</guid>
    </item>
    <item>
      <title>Mechanistic Knobs in LLMs: Retrieving and Steering High-Order Semantic Features via Sparse Autoencoders</title>
      <link>https://arxiv.org/abs/2601.02978v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Ruikang Zhang, Shuo Wang, Qi Su</p><p>Recent work in Mechanistic Interpretability (MI) has enabled the identification and intervention of internal features in Large Language Models (LLMs). However, a persistent challenge lies in linking such internal features to the reliable control of complex, behavior-level semantic attributes in language generation. In this paper, we propose a Sparse Autoencoder-based framework for retrieving and steering semantically interpretable internal features associated with high-level linguistic behaviors</p><p><strong>Tags:</strong> SAE, features, steering</p>]]></description>
      <pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.02978v1</guid>
    </item>
    <item>
      <title>Bridging Mechanistic Interpretability and Prompt Engineering with Gradient Ascent for Interpretable Persona Control</title>
      <link>https://arxiv.org/abs/2601.02896v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Harshvardhan Saini, Yiming Tang, Dianbo Liu</p><p>Controlling emergent behavioral personas (e.g., sycophancy, hallucination) in Large Language Models (LLMs) is critical for AI safety, yet remains a persistent challenge. Existing solutions face a dilemma: manual prompt engineering is intuitive but unscalable and imprecise, while automatic optimization methods are effective but operate as &quot;black boxes&quot; with no interpretable connection to model internals. We propose a novel framework that adapts gradient ascent to LLMs, enabling targeted prompt di</p><p><strong>Tags:</strong> safety</p>]]></description>
      <pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.02896v1</guid>
    </item>
    <item>
      <title>MAFS: Multi-head Attention Feature Selection for High-Dimensional Data via Deep Fusion of Filter Methods</title>
      <link>https://arxiv.org/abs/2601.02668v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Xiaoyan Sun, Qingyu Meng, Yalu Wen</p><p>Feature selection is essential for high-dimensional biomedical data, enabling stronger predictive performance, reduced computational cost, and improved interpretability in precision medicine applications. Existing approaches face notable challenges. Filter methods are highly scalable but cannot capture complex relationships or eliminate redundancy. Deep learning-based approaches can model nonlinear patterns but often lack stability, interpretability, and efficiency at scale. Single-head attentio</p><p><strong>Tags:</strong> features</p>]]></description>
      <pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.02668v1</guid>
    </item>
  </channel>
</rss>
