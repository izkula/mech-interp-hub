<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Mechanistic Interpretability Hub</title>
    <link>https://izkula.github.io/cc</link>
    <description>Latest research in mechanistic interpretability - understanding how neural networks work internally</description>
    <language>en-us</language>
    <lastBuildDate>Sun, 22 Feb 2026 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://izkula.github.io/cc/feed.xml" rel="self" type="application/rss+xml"/>

    <item>
      <title>Systematic Evaluation of Single-Cell Foundation Model Interpretability Reveals Attention Captures Co-Expression Rather Than Unique Regulatory Signal</title>
      <link>https://arxiv.org/abs/2602.17532v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Ihor Kendiukhov</p><p>We present a systematic evaluation framework - thirty-seven analyses, 153 statistical tests, four cell types, two perturbation modalities - for assessing mechanistic interpretability in single-cell foundation models. Applying this framework to scGPT and Geneformer, we find that attention patterns encode structured biological information with layer-specific organisation - protein-protein interactions in early layers, transcriptional regulation in late layers - but this structure provides no incre</p><p><strong>Tags:</strong> attention, biology</p>]]></description>
      <pubDate>Thu, 19 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.17532v1</guid>
    </item>
    <item>
      <title>Mechanistic Interpretability of Cognitive Complexity in LLMs via Linear Probing using Bloom&#x27;s Taxonomy</title>
      <link>https://arxiv.org/abs/2602.17229v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Bianca Raimondi, Maurizio Gabbrielli</p><p>The black-box nature of Large Language Models necessitates novel evaluation frameworks that transcend surface-level performance metrics. This study investigates the internal neural representations of cognitive complexity using Bloom&#x27;s Taxonomy as a hierarchical lens. By analyzing high-dimensional activation vectors from different LLMs, we probe whether different cognitive levels, ranging from basic recall (Remember) to abstract synthesis (Create), are linearly separable within the model&#x27;s residu</p><p><strong>Tags:</strong> probing</p>]]></description>
      <pubDate>Thu, 19 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.17229v1</guid>
    </item>
    <item>
      <title>Discovering Universal Activation Directions for PII Leakage in Language Models</title>
      <link>https://arxiv.org/abs/2602.16980v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Leo Marchyok, Zachary Coalson, Sungho Keum, Sooel Son, Sanghyun Hong</p><p>Modern language models exhibit rich internal structure, yet little is known about how privacy-sensitive behaviors, such as personally identifiable information (PII) leakage, are represented and modulated within their hidden states. We present UniLeak, a mechanistic-interpretability framework that identifies universal activation directions: latent directions in a model&#x27;s residual stream whose linear addition at inference time consistently increases the likelihood of generating PII across prompts.</p>]]></description>
      <pubDate>Thu, 19 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.16980v1</guid>
    </item>
    <item>
      <title>The Cascade Equivalence Hypothesis: When Do Speech LLMs Behave Like ASR$\rightarrow$LLM Pipelines?</title>
      <link>https://arxiv.org/abs/2602.17598v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Jayadev Billa</p><p>Current speech LLMs largely perform implicit ASR: on tasks solvable from a transcript, they are behaviorally and mechanistically equivalent to simple Whisper$\to$LLM cascades. We show this through matched-backbone testing across four speech LLMs and six tasks, controlling for the LLM backbone for the first time. Ultravox is statistically indistinguishable from its matched cascade ($κ{=}0.93$); logit lens reveals literal text emerging in hidden states; LEACE concept erasure confirms text represen</p>]]></description>
      <pubDate>Thu, 19 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.17598v1</guid>
    </item>
    <item>
      <title>ODESteer: A Unified ODE-Based Steering Framework for LLM Alignment</title>
      <link>https://arxiv.org/abs/2602.17560v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Hongjue Zhao, Haosen Sun, Jiangtao Kong, Xiaochang Li, Qineng Wang, et al.</p><p>Activation steering, or representation engineering, offers a lightweight approach to align large language models (LLMs) by manipulating their internal activations at inference time. However, current methods suffer from two key limitations: \textit{(i)} the lack of a unified theoretical framework for guiding the design of steering directions, and \textit{(ii)} an over-reliance on \textit{one-step steering} that fail to capture complex patterns of activation distributions. In this work, we propose</p><p><strong>Tags:</strong> safety, steering, theory</p>]]></description>
      <pubDate>Thu, 19 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.17560v1</guid>
    </item>
    <item>
      <title>Causality is Key for Interpretability Claims to Generalise</title>
      <link>https://arxiv.org/abs/2602.16698v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Shruti Joshi, Aaron Mueller, David Klindt, Wieland Brendel, Patrik Reizinger, et al.</p><p>Interpretability research on large language models (LLMs) has yielded important insights into model behaviour, yet recurring pitfalls persist: findings that do not generalise, and causal interpretations that outrun the evidence. Our position is that causal inference specifies what constitutes a valid mapping from model activations to invariant high-level structures, the data or assumptions needed to achieve it, and the inferences it can support. Specifically, Pearl&#x27;s causal hierarchy clarifies w</p>]]></description>
      <pubDate>Wed, 18 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.16698v1</guid>
    </item>
    <item>
      <title>On the Mechanism and Dynamics of Modular Addition: Fourier Features, Lottery Ticket, and Grokking</title>
      <link>https://arxiv.org/abs/2602.16849v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Jianliang He, Leda Wang, Siyu Chen, Zhuoran Yang</p><p>We present a comprehensive analysis of how two-layer neural networks learn features to solve the modular addition task. Our work provides a full mechanistic interpretation of the learned model and a theoretical explanation of its training dynamics. While prior work has identified that individual neurons learn single-frequency Fourier features and phase alignment, it does not fully explain how these features combine into a global solution. We bridge this gap by formalizing a diversification condi</p><p><strong>Tags:</strong> features, safety, theory</p>]]></description>
      <pubDate>Wed, 18 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.16849v1</guid>
    </item>
    <item>
      <title>Formal Mechanistic Interpretability: Automated Circuit Discovery with Provable Guarantees</title>
      <link>https://arxiv.org/abs/2602.16823v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Itamar Hadad, Guy Katz, Shahaf Bassan</p><p>*Automated circuit discovery* is a central tool in mechanistic interpretability for identifying the internal components of neural networks responsible for specific behaviors. While prior methods have made significant progress, they typically depend on heuristics or approximations and do not offer provable guarantees over continuous input domains for the resulting circuits. In this work, we leverage recent advances in neural network verification to propose a suite of automated algorithms that yie</p><p><strong>Tags:</strong> circuits</p>]]></description>
      <pubDate>Wed, 18 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.16823v1</guid>
    </item>
    <item>
      <title>What Do Neurons Listen To? A Neuron-level Dissection of a General-purpose Audio Model</title>
      <link>https://arxiv.org/abs/2602.15307v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Takao Kawamura, Daisuke Niizumi, Nobutaka Ono</p><p>In this paper, we analyze the internal representations of a general-purpose audio self-supervised learning (SSL) model from a neuron-level perspective. Despite their strong empirical performance as feature extractors, the internal mechanisms underlying the robust generalization of SSL audio models remain unclear. Drawing on the framework of mechanistic interpretability, we identify and examine class-specific neurons by analyzing conditional activation patterns across diverse tasks. Our analysis </p><p><strong>Tags:</strong> features</p>]]></description>
      <pubDate>Tue, 17 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.15307v1</guid>
    </item>
    <item>
      <title>Causal Effect Estimation with Latent Textual Treatments</title>
      <link>https://arxiv.org/abs/2602.15730v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Omri Feldman, Amar Venugopal, Jann Spiess, Amir Feder</p><p>Understanding the causal effects of text on downstream outcomes is a central task in many applications. Estimating such effects requires researchers to run controlled experiments that systematically vary textual features. While large language models (LLMs) hold promise for generating text, producing and evaluating controlled variation requires more careful attention. In this paper, we present an end-to-end pipeline for the generation and causal estimation of latent textual interventions. Our wor</p><p><strong>Tags:</strong> features</p>]]></description>
      <pubDate>Tue, 17 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.15730v1</guid>
    </item>
    <item>
      <title>Quantifying LLM Attention-Head Stability: Implications for Circuit Universality</title>
      <link>https://arxiv.org/abs/2602.16740v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Karan Bali, Jack Stanley, Praneet Suresh, Danilo Bzdok</p><p>In mechanistic interpretability, recent work scrutinizes transformer &quot;circuits&quot; - sparse, mono or multi layer sub computations, that may reflect human understandable functions. Yet, these network circuits are rarely acid-tested for their stability across different instances of the same deep learning architecture. Without this, it remains unclear whether reported circuits emerge universally across labs or turn out to be idiosyncratic to a particular estimation instance, potentially limiting confi</p><p><strong>Tags:</strong> circuits</p>]]></description>
      <pubDate>Tue, 17 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.16740v1</guid>
    </item>
    <item>
      <title>SynthSAEBench: Evaluating Sparse Autoencoders on Scalable Realistic Synthetic Data</title>
      <link>https://arxiv.org/abs/2602.14687v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> David Chanin, Adrià Garriga-Alonso</p><p>Improving Sparse Autoencoders (SAEs) requires benchmarks that can precisely validate architectural innovations. However, current SAE benchmarks on LLMs are often too noisy to differentiate architectural improvements, and current synthetic data experiments are too small-scale and unrealistic to provide meaningful comparisons. We introduce SynthSAEBench, a toolkit for generating large-scale synthetic data with realistic feature characteristics including correlation, hierarchy, and superposition, a</p><p><strong>Tags:</strong> SAE, superposition, features</p>]]></description>
      <pubDate>Mon, 16 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.14687v1</guid>
    </item>
    <item>
      <title>Concept Influence: Leveraging Interpretability to Improve Performance and Efficiency in Training Data Attribution</title>
      <link>https://arxiv.org/abs/2602.14869v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Matthew Kowal, Goncalo Paulo, Louis Jaburi, Tom Tseng, Lev E McKinney, et al.</p><p>As large language models are increasingly trained and fine-tuned, practitioners need methods to identify which training data drive specific behaviors, particularly unintended ones. Training Data Attribution (TDA) methods address this by estimating datapoint influence. Existing approaches like influence functions are both computationally expensive and attribute based on single test examples, which can bias results toward syntactic rather than semantic similarity. To address these issues of scalab</p>]]></description>
      <pubDate>Mon, 16 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.14869v1</guid>
    </item>
    <item>
      <title>Sanity Checks for Sparse Autoencoders: Do SAEs Beat Random Baselines?</title>
      <link>https://arxiv.org/abs/2602.14111v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Anton Korznikov, Andrey Galichin, Alexey Dontsov, Oleg Rogov, Ivan Oseledets, et al.</p><p>Sparse Autoencoders (SAEs) have emerged as a promising tool for interpreting neural networks by decomposing their activations into sparse sets of human-interpretable features. Recent work has introduced multiple SAE variants and successfully scaled them to frontier models. Despite much excitement, a growing number of negative results in downstream tasks casts doubt on whether SAEs recover meaningful features. To directly investigate this, we perform two complementary evaluations. On a synthetic </p><p><strong>Tags:</strong> SAE, features</p>]]></description>
      <pubDate>Sun, 15 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.14111v1</guid>
    </item>
    <item>
      <title>DistillLens: Symmetric Knowledge Distillation Through Logit Lens</title>
      <link>https://arxiv.org/abs/2602.13567v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Manish Dhakal, Uthman Jinadu, Anjila Budathoki, Rajshekhar Sunderraman, Yi Ding</p><p>Standard Knowledge Distillation (KD) compresses Large Language Models (LLMs) by optimizing final outputs, yet it typically treats the teacher&#x27;s intermediate layer&#x27;s thought process as a black box. While feature-based distillation attempts to bridge this gap, existing methods (e.g., MSE and asymmetric KL divergence) ignore the rich uncertainty profiles required for the final output. In this paper, we introduce DistillLens, a framework that symmetrically aligns the evolving thought processes of st</p><p><strong>Tags:</strong> features</p>]]></description>
      <pubDate>Sat, 14 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.13567v1</guid>
    </item>
    <item>
      <title>Singular Vectors of Attention Heads Align with Features</title>
      <link>https://arxiv.org/abs/2602.13524v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Gabriel Franco, Carson Loughridge, Mark Crovella</p><p>Identifying feature representations in language models is a central task in mechanistic interpretability. Several recent studies have made an implicit assumption that feature representations can be inferred in some cases from singular vectors of attention matrices. However, sound justification for this assumption is lacking. In this paper we address that question, asking: why and when do singular vectors align with features? First, we demonstrate that singular vectors robustly align with feature</p><p><strong>Tags:</strong> features, attention</p>]]></description>
      <pubDate>Fri, 13 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.13524v1</guid>
    </item>
    <item>
      <title>Finding Highly Interpretable Prompt-Specific Circuits in Language Models</title>
      <link>https://arxiv.org/abs/2602.13483v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Gabriel Franco, Lucas M. Tassis, Azalea Rohr, Mark Crovella</p><p>Understanding the internal circuits that language models use to solve tasks remains a central challenge in mechanistic interpretability. Most prior work identifies circuits at the task level by averaging across many prompts, implicitly assuming a single stable mechanism per task. We show that this assumption can obscure a crucial source of structure: circuits are prompt-specific, even within a fixed task. Building on attention causal communication (ACC) (Franco &amp; Crovella, 2025), we introduce AC</p><p><strong>Tags:</strong> circuits</p>]]></description>
      <pubDate>Fri, 13 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.13483v1</guid>
    </item>
    <item>
      <title>Protein Circuit Tracing via Cross-layer Transcoders</title>
      <link>https://arxiv.org/abs/2602.12026v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Darin Tsui, Kunal Talreja, Daniel Saeedi, Amirali Aghazadeh</p><p>Protein language models (pLMs) have emerged as powerful predictors of protein structure and function. However, the computational circuits underlying their predictions remain poorly understood. Recent mechanistic interpretability methods decompose pLM representations into interpretable features, but they treat each layer independently and thus fail to capture cross-layer computation, limiting their ability to approximate the full model. We introduce ProtoMech, a framework for discovering computat</p><p><strong>Tags:</strong> circuits, features, biology</p>]]></description>
      <pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.12026v1</guid>
    </item>
    <item>
      <title>From Atoms to Trees: Building a Structured Feature Forest with Hierarchical Sparse Autoencoders</title>
      <link>https://arxiv.org/abs/2602.11881v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Yifan Luo, Yang Zhan, Jiedong Jiang, Tianyang Liu, Mingrui Wu, et al.</p><p>Sparse autoencoders (SAEs) have proven effective for extracting monosemantic features from large language models (LLMs), yet these features are typically identified in isolation. However, broad evidence suggests that LLMs capture the intrinsic structure of natural language, where the phenomenon of &quot;feature splitting&quot; in particular indicates that such structure is hierarchical. To capture this, we propose the Hierarchical Sparse Autoencoder (HSAE), which jointly learns a series of SAEs and the pa</p><p><strong>Tags:</strong> SAE, features</p>]]></description>
      <pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.11881v1</guid>
    </item>
    <item>
      <title>EM-Aware Physical Synthesis: Neural Inductor Modeling and Intelligent Placement &amp; Routing for RF Circuits</title>
      <link>https://arxiv.org/abs/2602.11461v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Yilun Huang, Asal Mehradfar, Salman Avestimehr, Hamidreza Aghasi</p><p>This paper presents an ML-driven framework for automated RF physical synthesis that transforms circuit netlists into manufacturable GDSII layouts. While recent ML approaches demonstrate success in topology selection and parameter optimization, they fail to produce manufacturable layouts due to oversimplified component models and lack of routing capabilities. Our framework addresses these limitations through three key innovations: (1) a neural network framework trained on 18,210 inductor geometri</p><p><strong>Tags:</strong> circuits</p>]]></description>
      <pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.11461v1</guid>
    </item>
    <item>
      <title>TADA! Tuning Audio Diffusion Models through Activation Steering</title>
      <link>https://arxiv.org/abs/2602.11910v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Łukasz Staniszewski, Katarzyna Zaleska, Mateusz Modrzejewski, Kamil Deja</p><p>Audio diffusion models can synthesize high-fidelity music from text, yet their internal mechanisms for representing high-level concepts remain poorly understood. In this work, we use activation patching to demonstrate that distinct semantic musical concepts, such as the presence of specific instruments, vocals, or genre characteristics, are controlled by a small, shared subset of attention layers in state-of-the-art audio diffusion architectures. Next, we demonstrate that applying Contrastive Ac</p><p><strong>Tags:</strong> steering</p>]]></description>
      <pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.11910v1</guid>
    </item>
    <item>
      <title>SafeNeuron: Neuron-Level Safety Alignment for Large Language Models</title>
      <link>https://arxiv.org/abs/2602.12158v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Zhaoxin Wang, Jiaming Liang, Fengbin Zhu, Weixiang Zhao, Junfeng Fang, et al.</p><p>Large language models (LLMs) and multimodal LLMs are typically safety-aligned before release to prevent harmful content generation. However, recent studies show that safety behaviors are concentrated in a small subset of parameters, making alignment brittle and easily bypassed through neuron-level attacks. Moreover, most existing alignment methods operate at the behavioral level, offering limited control over the model&#x27;s internal safety mechanisms. In this work, we propose SafeNeuron, a neuron-l</p><p><strong>Tags:</strong> safety, vision</p>]]></description>
      <pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.12158v1</guid>
    </item>
    <item>
      <title>Cross-Architecture Model Diffing with Crosscoders: Unsupervised Discovery of Differences Between LLMs</title>
      <link>https://arxiv.org/abs/2602.11729v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Thomas Jiralerspong, Trenton Bricken</p><p>Model diffing, the process of comparing models&#x27; internal representations to identify their differences, is a promising approach for uncovering safety-critical behaviors in new models. However, its application has so far been primarily focused on comparing a base model with its finetune. Since new LLM releases are often novel architectures, cross-architecture methods are essential to make model diffing widely applicable. Crosscoders are one solution capable of cross-architecture model diffing but</p><p><strong>Tags:</strong> safety</p>]]></description>
      <pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.11729v1</guid>
    </item>
    <item>
      <title>Sparse Autoencoders are Capable LLM Jailbreak Mitigators</title>
      <link>https://arxiv.org/abs/2602.12418v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Yannick Assogba, Jacopo Cortellazzi, Javier Abad, Pau Rodriguez, Xavier Suau, et al.</p><p>Jailbreak attacks remain a persistent threat to large language model safety. We propose Context-Conditioned Delta Steering (CC-Delta), an SAE-based defense that identifies jailbreak-relevant sparse features by comparing token-level representations of the same harmful request with and without jailbreak context. Using paired harmful/jailbreak prompts, CC-Delta selects features via statistical testing and applies inference-time mean-shift steering in SAE latent space. Across four aligned instructio</p><p><strong>Tags:</strong> SAE, features, safety, steering</p>]]></description>
      <pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.12418v1</guid>
    </item>
    <item>
      <title>MonoLoss: A Training Objective for Interpretable Monosemantic Representations</title>
      <link>https://arxiv.org/abs/2602.12403v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Ali Nasiri-Sarvi, Anh Tien Nguyen, Hassan Rivaz, Dimitris Samaras, Mahdi S. Hosseini</p><p>Sparse autoencoders (SAEs) decompose polysemantic neural representations, where neurons respond to multiple unrelated concepts, into monosemantic features that capture single, interpretable concepts. However, standard training objectives only weakly encourage this decomposition, and existing monosemanticity metrics require pairwise comparisons across all dataset samples, making them inefficient during training and evaluation. We study a recent MonoScore metric and derive a single-pass algorithm </p><p><strong>Tags:</strong> SAE, features</p>]]></description>
      <pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.12403v1</guid>
    </item>
    <item>
      <title>From Circuits to Dynamics: Understanding and Stabilizing Failure in 3D Diffusion Transformers</title>
      <link>https://arxiv.org/abs/2602.11130v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Maximilian Plattner, Fabian Paischer, Johannes Brandstetter, Arturs Berzins</p><p>Reliable surface completion from sparse point clouds underpins many applications spanning content creation and robotics. While 3D diffusion transformers attain state-of-the-art results on this task, we uncover that they exhibit a catastrophic mode of failure: arbitrarily small on-surface perturbations to the input point cloud can fracture the output into multiple disconnected pieces -- a phenomenon we call Meltdown. Using activation-patching from mechanistic interpretability, we localize Meltdow</p><p><strong>Tags:</strong> circuits</p>]]></description>
      <pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.11130v1</guid>
    </item>
    <item>
      <title>Control Reinforcement Learning: Token-Level Mechanistic Analysis via Learned SAE Feature Steering</title>
      <link>https://arxiv.org/abs/2602.10437v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Seonglae Cho, Zekun Wu, Adriano Koshiyama</p><p>Sparse autoencoders (SAEs) decompose language model activations into interpretable features, but existing methods reveal only which features activate, not which change model outputs when amplified. We introduce Control Reinforcement Learning (CRL), which trains a policy to select SAE features for steering at each token, producing interpretable intervention logs: the learned policy identifies features that change model outputs when amplified. Adaptive Feature Masking encourages diverse feature di</p><p><strong>Tags:</strong> SAE, features, steering</p>]]></description>
      <pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.10437v1</guid>
    </item>
    <item>
      <title>Less is Enough: Synthesizing Diverse Data in Feature Space of LLMs</title>
      <link>https://arxiv.org/abs/2602.10388v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Zhongzhi Li, Xuansheng Wu, Yijiang Li, Lijie Hu, Ninghao Liu</p><p>The diversity of post-training data is critical for effective downstream performance in large language models (LLMs). Many existing approaches to constructing post-training data quantify diversity using text-based metrics that capture linguistic variation, but such metrics provide only weak signals for the task-relevant features that determine downstream performance. In this work, we introduce Feature Activation Coverage (FAC) which measures data diversity in an interpretable feature space. Buil</p><p><strong>Tags:</strong> features</p>]]></description>
      <pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.10388v1</guid>
    </item>
    <item>
      <title>Med-SegLens: Latent-Level Model Diffing for Interpretable Medical Image Segmentation</title>
      <link>https://arxiv.org/abs/2602.10508v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Salma J. Ahmed, Emad A. Mohammed, Azam Asilian Bidgoli</p><p>Modern segmentation models achieve strong predictive performance but remain largely opaque, limiting our ability to diagnose failures, understand dataset shift, or intervene in a principled manner. We introduce Med-SegLens, a model-diffing framework that decomposes segmentation model activations into interpretable latent features using sparse autoencoders trained on SegFormer and U-Net. Through cross-architecture and cross-dataset latent alignment across healthy, adult, pediatric, and sub-Sahara</p><p><strong>Tags:</strong> SAE, features, safety, vision</p>]]></description>
      <pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.10508v1</guid>
    </item>
    <item>
      <title>Triggers Hijack Language Circuits: A Mechanistic Analysis of Backdoor Behaviors in Large Language Models</title>
      <link>https://arxiv.org/abs/2602.10382v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Théo Lasnier, Wissam Antoun, Francis Kulumba, Djamé Seddah</p><p>Backdoor attacks pose significant security risks for Large Language Models (LLMs), yet the internal mechanisms by which triggers operate remain poorly understood. We present the first mechanistic analysis of language-switching backdoors, studying the GAPperon model family (1B, 8B, 24B parameters) which contains triggers injected during pretraining that cause output language switching. Using activation patching, we localize trigger formation to early layers (7.5-25% of model depth) and identify w</p><p><strong>Tags:</strong> circuits</p>]]></description>
      <pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.10382v1</guid>
    </item>
  </channel>
</rss>
