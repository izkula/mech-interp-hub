<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Mechanistic Interpretability Hub</title>
    <link>https://izkula.github.io/cc</link>
    <description>Latest research in mechanistic interpretability - understanding how neural networks work internally</description>
    <language>en-us</language>
    <lastBuildDate>Fri, 30 Jan 2026 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://izkula.github.io/cc/feed.xml" rel="self" type="application/rss+xml"/>

    <item>
      <title>Research on the instability mechanism of transformer winding short-circuit faults based on magneto-mechanical coupling analysis</title>
      <link>https://www.semanticscholar.org/paper/48681d875dd4a7fd38cd0b9f05262cccabefe846</link>
      <description><![CDATA[<p><strong>Authors:</strong> Zhihao Gao, Jing Zhou, Li Zhu, Yuefeng Hao, Zhanlong Zhang, et al.</p><p><strong>Tags:</strong> circuits</p>]]></description>
      <pubDate>Sun, 01 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://www.semanticscholar.org/paper/48681d875dd4a7fd38cd0b9f05262cccabefe846</guid>
    </item>
    <item>
      <title>Putting a Face to Forgetting: Continual Learning meets Mechanistic Interpretability</title>
      <link>https://arxiv.org/abs/2601.22012v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Sergi Masip, Gido M. van de Ven, Javier Ferrando, Tinne Tuytelaars</p><p>Catastrophic forgetting in continual learning is often measured at the performance or last-layer representation level, overlooking the underlying mechanisms. We introduce a mechanistic framework that offers a geometric interpretation of catastrophic forgetting as the result of transformations to the encoding of individual features. These transformations can lead to forgetting by reducing the allocated capacity of features (worse representation) and disrupting their readout by downstream computat</p><p><strong>Tags:</strong> features</p>]]></description>
      <pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.22012v1</guid>
    </item>
    <item>
      <title>Mechanistic Data Attribution: Tracing the Training Origins of Interpretable LLM Units</title>
      <link>https://arxiv.org/abs/2601.21996v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Jianhui Chen, Yuzhang Luo, Liangming Pan</p><p>While Mechanistic Interpretability has identified interpretable circuits in LLMs, their causal origins in training data remain elusive. We introduce Mechanistic Data Attribution (MDA), a scalable framework that employs Influence Functions to trace interpretable units back to specific training samples. Through extensive experiments on the Pythia family, we causally validate that targeted intervention--removing or augmenting a small fraction of high-influence samples--significantly modulates the e</p><p><strong>Tags:</strong> circuits</p>]]></description>
      <pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.21996v1</guid>
    </item>
    <item>
      <title>Concept Component Analysis: A Principled Approach for Concept Extraction in LLMs</title>
      <link>https://arxiv.org/abs/2601.20420v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Yuhang Liu, Erdun Gao, Dong Gong, Anton van den Hengel, Javen Qinfeng Shi</p><p>Developing human understandable interpretation of large language models (LLMs) becomes increasingly critical for their deployment in essential domains. Mechanistic interpretability seeks to mitigate the issues through extracts human-interpretable process and concepts from LLMs&#x27; activations. Sparse autoencoders (SAEs) have emerged as a popular approach for extracting interpretable and monosemantic concepts by decomposing the LLM internal representations into a dictionary. Despite their empirical </p><p><strong>Tags:</strong> SAE, features</p>]]></description>
      <pubDate>Wed, 28 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.20420v1</guid>
    </item>
    <item>
      <title>How Do Transformers Learn to Associate Tokens: Gradient Leading Terms Bring Mechanistic Interpretability</title>
      <link>https://arxiv.org/abs/2601.19208v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Shawn Im, Changdae Oh, Zhen Fang, Sharon Li</p><p>Semantic associations such as the link between &quot;bird&quot; and &quot;flew&quot; are foundational for language modeling as they enable models to go beyond memorization and instead generalize and generate coherent text. Understanding how these associations are learned and represented in language models is essential for connecting deep learning with linguistic theory and developing a mechanistic foundation for large language models. In this work, we analyze how these associations emerge from natural language data</p><p><strong>Tags:</strong> theory</p>]]></description>
      <pubDate>Tue, 27 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.19208v1</guid>
    </item>
    <item>
      <title>Sparse CLIP: Co-Optimizing Interpretability and Performance in Contrastive Learning</title>
      <link>https://arxiv.org/abs/2601.20075v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Chuan Qin, Constantin Venhoff, Sonia Joseph, Fanyi Xiao, Stefan Scherer</p><p>Contrastive Language-Image Pre-training (CLIP) has become a cornerstone in vision-language representation learning, powering diverse downstream tasks and serving as the default vision backbone in multimodal large language models (MLLMs). Despite its success, CLIP&#x27;s dense and opaque latent representations pose significant interpretability challenges. A common assumption is that interpretability and performance are in tension: enforcing sparsity during training degrades accuracy, motivating recent</p><p><strong>Tags:</strong> vision</p>]]></description>
      <pubDate>Tue, 27 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.20075v1</guid>
    </item>
    <item>
      <title>Decomposing multimodal embedding spaces with group-sparse autoencoders</title>
      <link>https://arxiv.org/abs/2601.20028v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Chiraag Kaushik, Davis Barch, Andrea Fanelli</p><p>The Linear Representation Hypothesis asserts that the embeddings learned by neural networks can be understood as linear combinations of features corresponding to high-level concepts. Based on this ansatz, sparse autoencoders (SAEs) have recently become a popular method for decomposing embeddings into a sparse combination of linear directions, which have been shown empirically to often correspond to human-interpretable semantics. However, recent attempts to apply SAEs to multimodal embedding spac</p><p><strong>Tags:</strong> SAE, features, vision</p>]]></description>
      <pubDate>Tue, 27 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.20028v1</guid>
    </item>
    <item>
      <title>Funny or Persuasive, but Not Both: Evaluating Fine-Grained Multi-Concept Control in LLMs</title>
      <link>https://arxiv.org/abs/2601.18483v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Arya Labroo, Ivaxi Sheth, Vyas Raina, Amaani Ahmed, Mario Fritz</p><p>Large Language Models (LLMs) offer strong generative capabilities, but many applications require explicit and \textit{fine-grained} control over specific textual concepts, such as humor, persuasiveness, or formality. Prior approaches in prompting and representation engineering can provide coarse or single-attribute control, but systematic evaluation of multi-attribute settings remains limited. We introduce an evaluation framework for fine-grained controllability for both single- and dual-concept</p>]]></description>
      <pubDate>Mon, 26 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.18483v1</guid>
    </item>
    <item>
      <title>A Few Bad Neurons: Isolating and Surgically Correcting Sycophancy</title>
      <link>https://arxiv.org/abs/2601.18939v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Claire O&#x27;Brien, Jessica Seto, Dristi Roy, Aditya Dwivedi, Sunishchal Dev, et al.</p><p>Behavioral alignment in large language models (LLMs) is often achieved through broad fine-tuning, which can result in undesired side effects like distributional shift and low interpretability. We propose a method for alignment that identifies and updates only the neurons most responsible for a given behavior, a targeted approach that allows for fine-tuning with significantly less data. Using sparse autoencoders (SAEs) and linear probes, we isolate the 3% of MLP neurons most predictive of a targe</p><p><strong>Tags:</strong> SAE, safety, probing</p>]]></description>
      <pubDate>Mon, 26 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.18939v1</guid>
    </item>
    <item>
      <title>A Monosemantic Attribution Framework for Stable Interpretability in Clinical Neuroscience Large Language Models</title>
      <link>https://arxiv.org/abs/2601.17952v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Michail Mamalakis, Tiago Azevedo, Cristian Cosentino, Chiara D&#x27;Ercoli, Subati Abulikemu, et al.</p><p>Interpretability remains a key challenge for deploying large language models (LLMs) in clinical settings such as Alzheimer&#x27;s disease progression diagnosis, where early and trustworthy predictions are essential. Existing attribution methods exhibit high inter-method variability and unstable explanations due to the polysemantic nature of LLM representations, while mechanistic interpretability approaches lack direct alignment with model inputs and outputs and do not provide explicit importance scor</p><p><strong>Tags:</strong> features, safety</p>]]></description>
      <pubDate>Sun, 25 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.17952v1</guid>
    </item>
    <item>
      <title>TensorLens: End-to-End Transformer Analysis via High-Order Attention Tensors</title>
      <link>https://arxiv.org/abs/2601.17958v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Ido Andrew Atad, Itamar Zimerman, Shahar Katz, Lior Wolf</p><p>Attention matrices are fundamental to transformer research, supporting a broad range of applications including interpretability, visualization, manipulation, and distillation. Yet, most existing analyses focus on individual attention heads or layers, failing to account for the model&#x27;s global behavior. While prior efforts have extended attention formulations across multiple heads via averaging and matrix multiplications or incorporated components such as normalization and FFNs, a unified and comp</p><p><strong>Tags:</strong> attention, vision</p>]]></description>
      <pubDate>Sun, 25 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.17958v1</guid>
    </item>
    <item>
      <title>Implementing Tensor Logic: Unifying Datalog and Neural Reasoning via Tensor Contraction</title>
      <link>https://arxiv.org/abs/2601.17188v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Swapn Shah, Wlodek Zadrozny</p><p>The unification of symbolic reasoning and neural networks remains a central challenge in artificial intelligence. Symbolic systems offer reliability and interpretability but lack scalability, while neural networks provide learning capabilities but sacrifice transparency. Tensor Logic, proposed by Domingos, suggests that logical rules and Einstein summation are mathematically equivalent, offering a principled path toward unification. This paper provides empirical validation of this framework thro</p><p><strong>Tags:</strong> reasoning</p>]]></description>
      <pubDate>Fri, 23 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.17188v1</guid>
    </item>
    <item>
      <title>Attributing and Exploiting Safety Vectors through Global Optimization in Large Language Models</title>
      <link>https://arxiv.org/abs/2601.15801v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Fengheng Chu, Jiahao Chen, Yuhong Wang, Jun Wang, Zhihui Fu, et al.</p><p>While Large Language Models (LLMs) are aligned to mitigate risks, their safety guardrails remain fragile against jailbreak attacks. This reveals limited understanding of components governing safety. Existing methods rely on local, greedy attribution that assumes independent component contributions. However, they overlook the cooperative interactions between different components in LLMs, such as attention heads, which jointly contribute to safety mechanisms. We propose \textbf{G}lobal \textbf{O}p</p><p><strong>Tags:</strong> safety, attention</p>]]></description>
      <pubDate>Thu, 22 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.15801v1</guid>
    </item>
    <item>
      <title>From Insight to Intervention: Interpretable Neuron Steering for Controlling Popularity Bias in Recommender Systems</title>
      <link>https://arxiv.org/abs/2601.15122v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Parviz Ahmadov, Masoud Mansoury</p><p>Popularity bias is a pervasive challenge in recommender systems, where a few popular items dominate attention while the majority of less popular items remain underexposed. This imbalance can reduce recommendation quality and lead to unfair item exposure. Although existing mitigation methods address this issue to some extent, they often lack transparency in how they operate. In this paper, we propose a post-hoc approach, PopSteer, that leverages a Sparse Autoencoder (SAE) to both interpret and mi</p><p><strong>Tags:</strong> SAE, steering</p>]]></description>
      <pubDate>Wed, 21 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.15122v1</guid>
    </item>
    <item>
      <title>Mechanism Shift During Post-training from Autoregressive to Masked Diffusion Language Models</title>
      <link>https://arxiv.org/abs/2601.14758v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Injin Kong, Hyoungjoon Lee, Yohan Jo</p><p>Post-training pretrained Autoregressive models (ARMs) into Masked Diffusion models (MDMs) has emerged as a cost-effective strategy to overcome the limitations of sequential generation. However, the internal algorithmic transformations induced by this paradigm shift remain unexplored, leaving it unclear whether post-trained MDMs acquire genuine bidirectional reasoning capabilities or merely repackage autoregressive heuristics. In this work, we address this question by conducting a comparative cir</p><p><strong>Tags:</strong> reasoning</p>]]></description>
      <pubDate>Wed, 21 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.14758v1</guid>
    </item>
    <item>
      <title>CASL: Concept-Aligned Sparse Latents for Interpreting Diffusion Models</title>
      <link>https://arxiv.org/abs/2601.15441v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Zhenghao He, Guangzhi Xiong, Boyang Wang, Sanchit Sinha, Aidong Zhang</p><p>Internal activations of diffusion models encode rich semantic information, but interpreting such representations remains challenging. While Sparse Autoencoders (SAEs) have shown promise in disentangling latent representations, existing SAE-based methods for diffusion model understanding rely on unsupervised approaches that fail to align sparse features with human-understandable concepts. This limits their ability to provide reliable semantic control over generated images. We introduce CASL (Conc</p><p><strong>Tags:</strong> SAE, features, vision</p>]]></description>
      <pubDate>Wed, 21 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.15441v1</guid>
    </item>
    <item>
      <title>PRISM: Deriving the Transformer as a Signal-Denoising Operator via Maximum Coding Rate Reduction</title>
      <link>https://arxiv.org/abs/2601.15540v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Dongchen Huang</p><p>Deep learning models, particularly Transformers, are often criticized as &quot;black boxes&quot; and lack interpretability. We propose Prism, a white-box attention-based architecture derived from the principles of Maximizing Coding Rate Reduction ($\text{MCR}^2$). By modeling the attention mechanism as a gradient ascent process on a distinct signal-noise manifold, we introduce two physical constraints: an overcomplete dictionary to expand the representational phase space, and an irrational frequency separ</p>]]></description>
      <pubDate>Wed, 21 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.15540v1</guid>
    </item>
    <item>
      <title>Locate, Steer, and Improve: A Practical Survey of Actionable Mechanistic Interpretability in Large Language Models</title>
      <link>https://arxiv.org/abs/2601.14004v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Hengyuan Zhang, Zhihao Zhang, Mingyang Wang, Zunhai Su, Yiwei Wang, et al.</p><p>Mechanistic Interpretability (MI) has emerged as a vital approach to demystify the opaque decision-making of Large Language Models (LLMs). However, existing reviews primarily treat MI as an observational science, summarizing analytical insights while lacking a systematic framework for actionable intervention. To bridge this gap, we present a practical survey structured around the pipeline: &quot;Locate, Steer, and Improve.&quot; We formally categorize Localizing (diagnosis) and Steering (intervention) met</p><p><strong>Tags:</strong> steering, survey</p>]]></description>
      <pubDate>Tue, 20 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.14004v1</guid>
    </item>
    <item>
      <title>Patterning: The Dual of Interpretability</title>
      <link>https://arxiv.org/abs/2601.13548v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> George Wang, Daniel Murfet</p><p>Mechanistic interpretability aims to understand how neural networks generalize beyond their training data by reverse-engineering their internal structures. We introduce patterning as the dual problem: given a desired form of generalization, determine what training data produces it. Our approach is based on susceptibilities, which measure how posterior expectation values of observables respond to infinitesimal shifts in the data distribution. Inverting this linear response relationship yields the</p>]]></description>
      <pubDate>Tue, 20 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.13548v1</guid>
    </item>
    <item>
      <title>Hierarchical Sparse Circuit Extraction from Billion-Parameter Language Models through Scalable Attribution Graph Decomposition</title>
      <link>https://arxiv.org/abs/2601.12879v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Mohammed Mudassir Uddin, Shahnawaz Alam, Mohammed Kaif Pasha</p><p>Mechanistic interpretability seeks to reverse-engineer neural network computations into human-understandable algorithms, yet extracting sparse computational circuits from billion-parameter language models remains challenging due to exponential search complexity and pervasive polysemanticity. The proposed Hierarchical Attribution Graph Decomposition (HAGD) framework reduces circuit discovery complexity from O(2^n) exhaustive enumeration to O(n^2 log n) through multi-resolution abstraction hierarc</p><p><strong>Tags:</strong> circuits, features</p>]]></description>
      <pubDate>Mon, 19 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.12879v1</guid>
    </item>
    <item>
      <title>Conversational Context Classification: A Representation Engineering Approach</title>
      <link>https://arxiv.org/abs/2601.12286v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Jonathan Pan</p><p>The increasing prevalence of Large Language Models (LLMs) demands effective safeguards for their operation, particularly concerning their tendency to generate out-of-context responses. A key challenge is accurately detecting when LLMs stray from expected conversational norms, manifesting as topic shifts, factual inaccuracies, or outright hallucinations. Traditional anomaly detection struggles to directly apply within contextual semantics. This paper outlines our experiment in exploring the use o</p>]]></description>
      <pubDate>Sun, 18 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.12286v1</guid>
    </item>
    <item>
      <title>From Interpretability to Performance: Optimizing Retrieval Heads for Long-Context Language Models</title>
      <link>https://arxiv.org/abs/2601.11020v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Youmi Ma, Naoaki Okazaki</p><p>Advances in mechanistic interpretability have identified special attention heads, known as retrieval heads, that are responsible for retrieving information from the context. However, the role of these retrieval heads in improving model performance remains unexplored. This work investigates whether retrieval heads can be leveraged to enhance the long-context capabilities of LLMs. Specifically, we propose RetMask, a method that generates training signals by contrasting normal model outputs with th</p><p><strong>Tags:</strong> attention</p>]]></description>
      <pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.11020v1</guid>
    </item>
    <item>
      <title>From Knots to Knobs: Towards Steerable Collaborative Filtering Using Sparse Autoencoders</title>
      <link>https://arxiv.org/abs/2601.11182v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Martin Spišák, Ladislav Peška, Petr Škoda, Vojtěch Vančura, Rodrigo Alves</p><p>Sparse autoencoders (SAEs) have recently emerged as pivotal tools for introspection into large language models. SAEs can uncover high-quality, interpretable features at different levels of granularity and enable targeted steering of the generation process by selectively activating specific neurons in their latent activations. Our paper is the first to apply this approach to collaborative filtering, aiming to extract similarly interpretable features from representations learned purely from intera</p><p><strong>Tags:</strong> SAE, features, steering</p>]]></description>
      <pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.11182v1</guid>
    </item>
    <item>
      <title>Finding the Translation Switch: Discovering and Exploiting the Task-Initiation Features in LLMs</title>
      <link>https://arxiv.org/abs/2601.11019v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Xinwei Wu, Heng Liu, Xiaohu Zhao, Yuqi Ren, Linlong Xu, et al.</p><p>Large Language Models (LLMs) frequently exhibit strong translation abilities, even without task-specific fine-tuning. However, the internal mechanisms governing this innate capability remain largely opaque. To demystify this process, we leverage Sparse Autoencoders (SAEs) and introduce a novel framework for identifying task-specific features. Our method first recalls features that are frequently co-activated on translation inputs and then filters them for functional coherence using a PCA-based c</p><p><strong>Tags:</strong> SAE, features</p>]]></description>
      <pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.11019v1</guid>
    </item>
    <item>
      <title>Spurious Rewards Paradox: Mechanistically Understanding How RLVR Activates Memorization Shortcuts in LLMs</title>
      <link>https://arxiv.org/abs/2601.11061v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Lecheng Yan, Ruizhe Li, Guanhua Chen, Qing Li, Jiahui Geng, et al.</p><p>Reinforcement Learning with Verifiable Rewards (RLVR) is highly effective for enhancing LLM reasoning, yet recent evidence shows models like Qwen 2.5 achieve significant gains even with spurious or incorrect rewards. We investigate this phenomenon and identify a &quot;Perplexity Paradox&quot;: spurious RLVR triggers a divergence where answer-token perplexity drops while prompt-side coherence degrades, suggesting the model is bypassing reasoning in favor of memorization. Using Path Patching, Logit Lens, JS</p><p><strong>Tags:</strong> reasoning</p>]]></description>
      <pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.11061v1</guid>
    </item>
    <item>
      <title>TANDEM: Temporal-Aware Neural Detection for Multimodal Hate Speech</title>
      <link>https://arxiv.org/abs/2601.11178v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Girish A. Koushik, Helen Treharne, Diptesh Kanojia</p><p>Social media platforms are increasingly dominated by long-form multimodal content, where harmful narratives are constructed through a complex interplay of audio, visual, and textual cues. While automated systems can flag hate speech with high accuracy, they often function as &quot;black boxes&quot; that fail to provide the granular, interpretable evidence, such as precise timestamps and target identities, required for effective human-in-the-loop moderation. In this work, we introduce TANDEM, a unified fra</p><p><strong>Tags:</strong> safety, vision</p>]]></description>
      <pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.11178v1</guid>
    </item>
    <item>
      <title>Attesting Model Lineage by Consisted Knowledge Evolution with Fine-Tuning Trajectory</title>
      <link>https://arxiv.org/abs/2601.11683v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Zhuoyi Shang, Jiasen Li, Pengzhen Chen, Yanwei Liu, Xiaoyan Gu, et al.</p><p>The fine-tuning technique in deep learning gives rise to an emerging lineage relationship among models. This lineage provides a promising perspective for addressing security concerns such as unauthorized model redistribution and false claim of model provenance, which are particularly pressing in \textcolor{blue}{open-weight model} libraries where robust lineage verification mechanisms are often lacking. Existing approaches to model lineage detection primarily rely on static architectural similar</p>]]></description>
      <pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.11683v1</guid>
    </item>
    <item>
      <title>Diagnosing Generalization Failures in Fine-Tuned LLMs: A Cross-Architectural Study on Phishing Detection</title>
      <link>https://arxiv.org/abs/2601.10524v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Frank Bobe, Gregory D. Vetaw, Chase Pavlick, Darshan Bryner, Matthew Cook, et al.</p><p>The practice of fine-tuning Large Language Models (LLMs) has achieved state-of-the-art performance on specialized tasks, yet diagnosing why these models become brittle and fail to generalize remains a critical open problem. To address this, we introduce and apply a multi-layered diagnostic framework to a cross-architectural study. We fine-tune Llama 3.1 8B, Gemma 2 9B, and Mistral models on a high-stakes phishing detection task and use SHAP analysis and mechanistic interpretability to uncover th</p>]]></description>
      <pubDate>Thu, 15 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.10524v1</guid>
    </item>
    <item>
      <title>Measuring Affinity between Attention-Head Weight Subspaces via the Projection Kernel</title>
      <link>https://arxiv.org/abs/2601.10266v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Hiroaki Yamagiwa, Yusuke Takase, Hidetoshi Shimodaira</p><p>Understanding relationships between attention heads is essential for interpreting the internal structure of Transformers, yet existing metrics do not capture this structure well. We focus on the subspaces spanned by attention-head weight matrices and quantify head-to-head relationships using the Projection Kernel (PK), a principal-angle-based measure of subspace similarity. Experiments show that PK reproduces known head-to-head interactions on the IOI task more clearly than prior metrics such as</p><p><strong>Tags:</strong> attention</p>]]></description>
      <pubDate>Thu, 15 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.10266v1</guid>
    </item>
    <item>
      <title>Reasoning Models Generate Societies of Thought</title>
      <link>https://arxiv.org/abs/2601.10825v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Junsol Kim, Shiyang Lai, Nino Scherrer, Blaise Agüera y Arcas, James Evans</p><p>Large language models have achieved remarkable capabilities across domains, yet mechanisms underlying sophisticated reasoning remain elusive. Recent reasoning models outperform comparable instruction-tuned models on complex cognitive tasks, attributed to extended computation through longer chains of thought. Here we show that enhanced reasoning emerges not from extended computation alone, but from simulating multi-agent-like interactions -- a society of thought -- which enables diversification a</p><p><strong>Tags:</strong> reasoning</p>]]></description>
      <pubDate>Thu, 15 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.10825v1</guid>
    </item>
  </channel>
</rss>
