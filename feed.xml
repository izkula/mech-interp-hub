<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Mechanistic Interpretability Hub</title>
    <link>https://izkula.github.io/cc</link>
    <description>Latest research in mechanistic interpretability - understanding how neural networks work internally</description>
    <language>en-us</language>
    <lastBuildDate>Sun, 11 Jan 2026 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://izkula.github.io/cc/feed.xml" rel="self" type="application/rss+xml"/>

    <item>
      <title>Research on the instability mechanism of transformer winding short-circuit faults based on magneto-mechanical coupling analysis</title>
      <link>https://www.semanticscholar.org/paper/48681d875dd4a7fd38cd0b9f05262cccabefe846</link>
      <description><![CDATA[<p><strong>Authors:</strong> Zhihao Gao, Jing Zhou, Li Zhu, Yuefeng Hao, Zhanlong Zhang, et al.</p><p><strong>Tags:</strong> circuits</p>]]></description>
      <pubDate>Sun, 01 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://www.semanticscholar.org/paper/48681d875dd4a7fd38cd0b9f05262cccabefe846</guid>
    </item>
    <item>
      <title>LANGSAE EDITING: Improving Multilingual Information Retrieval via Post-hoc Language Identity Removal</title>
      <link>https://arxiv.org/abs/2601.04768v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Dongjun Kim, Jeongho Yoon, Chanjun Park, Heuiseok Lim</p><p>Dense retrieval in multilingual settings often searches over mixed-language collections, yet multilingual embeddings encode language identity alongside semantics. This language signal can inflate similarity for same-language pairs and crowd out relevant evidence written in other languages. We propose LANGSAE EDITING, a post-hoc sparse autoencoder trained on pooled embeddings that enables controllable removal of language-identity signal directly in vector space. The method identifies language-ass</p><p><strong>Tags:</strong> SAE</p>]]></description>
      <pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.04768v1</guid>
    </item>
    <item>
      <title>Learning Dynamics in RL Post-Training for Language Models</title>
      <link>https://arxiv.org/abs/2601.04670v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Akiyoshi Tomihari</p><p>Reinforcement learning (RL) post-training is a critical stage in modern language model development, playing a key role in improving alignment and reasoning ability. However, several phenomena remain poorly understood, including the reduction in output diversity. To gain a broader understanding of RL post-training, we analyze the learning dynamics of RL post-training from a perspective that has been studied in supervised learning but remains underexplored in RL. We adopt an empirical neural tange</p><p><strong>Tags:</strong> safety, reasoning</p>]]></description>
      <pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.04670v1</guid>
    </item>
    <item>
      <title>When Models Manipulate Manifolds: The Geometry of a Counting Task</title>
      <link>https://arxiv.org/abs/2601.04480v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Wes Gurnee, Emmanuel Ameisen, Isaac Kauvar, Julius Tarng, Adam Pearce, et al.</p><p>Language models can perceive visual properties of text despite receiving only sequences of tokens-we mechanistically investigate how Claude 3.5 Haiku accomplishes one such task: linebreaking in fixed-width text. We find that character counts are represented on low-dimensional curved manifolds discretized by sparse feature families, analogous to biological place cells. Accurate predictions emerge from a sequence of geometric transformations: token lengths are accumulated into character count mani</p><p><strong>Tags:</strong> features, vision, biology</p>]]></description>
      <pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.04480v1</guid>
    </item>
    <item>
      <title>Controllable LLM Reasoning via Sparse Autoencoder-Based Steering</title>
      <link>https://arxiv.org/abs/2601.03595v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Yi Fang, Wenjie Wang, Mingfeng Xue, Boyi Deng, Fengli Xu, et al.</p><p>Large Reasoning Models (LRMs) exhibit human-like cognitive reasoning strategies (e.g. backtracking, cross-verification) during reasoning process, which improves their performance on complex tasks. Currently, reasoning strategies are autonomously selected by LRMs themselves. However, such autonomous selection often produces inefficient or even erroneous reasoning paths. To make reasoning more reliable and flexible, it is important to develop methods for controlling reasoning strategies. Existing </p><p><strong>Tags:</strong> SAE, steering, reasoning</p>]]></description>
      <pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.03595v1</guid>
    </item>
    <item>
      <title>NeuronScope: A Multi-Agent Framework for Explaining Polysemantic Neurons in Language Models</title>
      <link>https://arxiv.org/abs/2601.03671v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Weiqi Liu, Yongliang Miao, Haiyan Zhao, Yanguang Liu, Mengnan Du</p><p>Neuron-level interpretation in large language models (LLMs) is fundamentally challenged by widespread polysemanticity, where individual neurons respond to multiple distinct semantic concepts. Existing single-pass interpretation methods struggle to faithfully capture such multi-concept behavior. In this work, we propose NeuronScope, a multi-agent framework that reformulates neuron interpretation as an iterative, activation-guided process. NeuronScope explicitly deconstructs neuron activations int</p><p><strong>Tags:</strong> features</p>]]></description>
      <pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.03671v1</guid>
    </item>
    <item>
      <title>Where meaning lives: Layer-wise accessibility of psycholinguistic features in encoder and decoder language models</title>
      <link>https://arxiv.org/abs/2601.03798v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Taisiia Tikhomirova, Dirk U. Wulff</p><p>Understanding where transformer language models encode psychologically meaningful aspects of meaning is essential for both theory and practice. We conduct a systematic layer-wise probing study of 58 psycholinguistic features across 10 transformer models, spanning encoder-only and decoder-only architectures, and compare three embedding extraction methods. We find that apparent localization of meaning is strongly method-dependent: contextualized embeddings yield higher feature-specific selectivity</p><p><strong>Tags:</strong> features, probing, theory</p>]]></description>
      <pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.03798v1</guid>
    </item>
    <item>
      <title>Interpreting Transformers Through Attention Head Intervention</title>
      <link>https://arxiv.org/abs/2601.04398v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Mason Kadem, Rong Zheng</p><p>Neural networks are growing more capable on their own, but we do not understand their neural mechanisms. Understanding these mechanisms&#x27; decision-making processes, or mechanistic interpretability, enables (1) accountability and control in high-stakes domains, (2) the study of digital brains and the emergence of cognition, and (3) discovery of new knowledge when AI systems outperform humans.</p><p><strong>Tags:</strong> attention</p>]]></description>
      <pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.04398v1</guid>
    </item>
    <item>
      <title>When the Coffee Feature Activates on Coffins: An Analysis of Feature Extraction and Steering for Mechanistic Interpretability</title>
      <link>https://arxiv.org/abs/2601.03047v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Raphael Ronge, Markus Maier, Frederick Eberhardt</p><p>Recent work by Anthropic on Mechanistic interpretability claims to understand and control Large Language Models by extracting human-interpretable features from their neural activation patterns using sparse autoencoders (SAEs). If successful, this approach offers one of the most promising routes for human oversight in AI safety. We conduct an initial stress-test of these claims by replicating their main results with open-source SAEs for Llama 3.1. While we successfully reproduce basic feature ext</p><p><strong>Tags:</strong> SAE, features, safety, steering</p>]]></description>
      <pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.03047v1</guid>
    </item>
    <item>
      <title>Mechanistic Interpretability of Large-Scale Counting in LLMs through a System-2 Strategy</title>
      <link>https://arxiv.org/abs/2601.02989v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Hosein Hasani, Mohammadali Banayeeanzade, Ali Nafisi, Sadegh Mohammadian, Fatemeh Askari, et al.</p><p>Large language models (LLMs), despite strong performance on complex mathematical problems, exhibit systematic limitations in counting tasks. This issue arises from architectural limits of transformers, where counting is performed across layers, leading to degraded precision for larger counting problems due to depth constraints. To address this limitation, we propose a simple test-time strategy inspired by System-2 cognitive processes that decomposes large counting tasks into smaller, independent</p>]]></description>
      <pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.02989v1</guid>
    </item>
    <item>
      <title>Mechanistic Knobs in LLMs: Retrieving and Steering High-Order Semantic Features via Sparse Autoencoders</title>
      <link>https://arxiv.org/abs/2601.02978v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Ruikang Zhang, Shuo Wang, Qi Su</p><p>Recent work in Mechanistic Interpretability (MI) has enabled the identification and intervention of internal features in Large Language Models (LLMs). However, a persistent challenge lies in linking such internal features to the reliable control of complex, behavior-level semantic attributes in language generation. In this paper, we propose a Sparse Autoencoder-based framework for retrieving and steering semantically interpretable internal features associated with high-level linguistic behaviors</p><p><strong>Tags:</strong> SAE, features, steering</p>]]></description>
      <pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.02978v1</guid>
    </item>
    <item>
      <title>Bridging Mechanistic Interpretability and Prompt Engineering with Gradient Ascent for Interpretable Persona Control</title>
      <link>https://arxiv.org/abs/2601.02896v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Harshvardhan Saini, Yiming Tang, Dianbo Liu</p><p>Controlling emergent behavioral personas (e.g., sycophancy, hallucination) in Large Language Models (LLMs) is critical for AI safety, yet remains a persistent challenge. Existing solutions face a dilemma: manual prompt engineering is intuitive but unscalable and imprecise, while automatic optimization methods are effective but operate as &quot;black boxes&quot; with no interpretable connection to model internals. We propose a novel framework that adapts gradient ascent to LLMs, enabling targeted prompt di</p><p><strong>Tags:</strong> safety</p>]]></description>
      <pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.02896v1</guid>
    </item>
    <item>
      <title>MAFS: Multi-head Attention Feature Selection for High-Dimensional Data via Deep Fusion of Filter Methods</title>
      <link>https://arxiv.org/abs/2601.02668v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Xiaoyan Sun, Qingyu Meng, Yalu Wen</p><p>Feature selection is essential for high-dimensional biomedical data, enabling stronger predictive performance, reduced computational cost, and improved interpretability in precision medicine applications. Existing approaches face notable challenges. Filter methods are highly scalable but cannot capture complex relationships or eliminate redundancy. Deep learning-based approaches can model nonlinear patterns but often lack stability, interpretability, and efficiency at scale. Single-head attentio</p><p><strong>Tags:</strong> features</p>]]></description>
      <pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.02668v1</guid>
    </item>
    <item>
      <title>TRYLOCK: Defense-in-Depth Against LLM Jailbreaks via Layered Preference and Representation Engineering</title>
      <link>https://arxiv.org/abs/2601.03300v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Scott Thornton</p><p>Large language models remain vulnerable to jailbreak attacks, and single-layer defenses often trade security for usability. We present TRYLOCK, the first defense-in-depth architecture that combines four heterogeneous mechanisms across the inference stack: weight-level safety alignment via DPO, activation-level control via Representation Engineering (RepE) steering, adaptive steering strength selected by a lightweight sidecar classifier, and input canonicalization to neutralize encoding-based byp</p><p><strong>Tags:</strong> safety, probing, steering</p>]]></description>
      <pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.03300v1</guid>
    </item>
    <item>
      <title>Central Dogma Transformer: Towards Mechanism-Oriented AI for Cellular Understanding</title>
      <link>https://arxiv.org/abs/2601.01089v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Nobuyuki Ota</p><p>Understanding cellular mechanisms requires integrating information across DNA, RNA, and protein - the three molecular systems linked by the Central Dogma of molecular biology. While domain-specific foundation models have achieved success for each modality individually, they remain isolated, limiting our ability to model integrated cellular processes. Here we present the Central Dogma Transformer (CDT), an architecture that integrates pre-trained language models for DNA, RNA, and protein followin</p><p><strong>Tags:</strong> biology</p>]]></description>
      <pubDate>Sat, 03 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.01089v1</guid>
    </item>
    <item>
      <title>Explainability-Guided Defense: Attribution-Aware Model Refinement Against Adversarial Data Attacks</title>
      <link>https://arxiv.org/abs/2601.00968v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Longwei Wang, Mohammad Navid Nayyem, Abdullah Al Rakin, KC Santosh, Chaowei Zhang, et al.</p><p>The growing reliance on deep learning models in safety-critical domains such as healthcare and autonomous navigation underscores the need for defenses that are both robust to adversarial perturbations and transparent in their decision-making. In this paper, we identify a connection between interpretability and robustness that can be directly leveraged during training. Specifically, we observe that spurious, unstable, or semantically irrelevant features identified through Local Interpretable Mode</p><p><strong>Tags:</strong> features, safety</p>]]></description>
      <pubDate>Fri, 02 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.00968v1</guid>
    </item>
    <item>
      <title>ActErase: A Training-Free Paradigm for Precise Concept Erasure via Activation Patching</title>
      <link>https://arxiv.org/abs/2601.00267v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Yi Sun, Xinhao Zhong, Hongyan Li, Yimin Zhou, Junhao Li, et al.</p><p>Recent advances in text-to-image diffusion models have demonstrated remarkable generation capabilities, yet they raise significant concerns regarding safety, copyright, and ethical implications. Existing concept erasure methods address these risks by removing sensitive concepts from pre-trained models, but most of them rely on data-intensive and computationally expensive fine-tuning, which poses a critical limitation. To overcome these challenges, inspired by the observation that the model&#x27;s act</p><p><strong>Tags:</strong> safety, vision</p>]]></description>
      <pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.00267v1</guid>
    </item>
    <item>
      <title>Triangulation as an Acceptance Rule for Multilingual Mechanistic Interpretability</title>
      <link>https://arxiv.org/abs/2512.24842v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Yanan Long</p><p>Multilingual language models achieve strong aggregate performance yet often behave unpredictably across languages, scripts, and cultures. We argue that mechanistic explanations for such models should satisfy a \emph{causal} standard: claims must survive causal interventions and must \emph{cross-reference} across environments that perturb surface form while preserving meaning. We formalize \emph{reference families} as predicate-preserving variants and introduce \emph{triangulation}, an acceptance</p>]]></description>
      <pubDate>Wed, 31 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2512.24842v1</guid>
    </item>
    <item>
      <title>Attribution-Guided Distillation of Matryoshka Sparse Autoencoders</title>
      <link>https://arxiv.org/abs/2512.24975v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Cristina P. Martin-Linares, Jonathan P. Ling</p><p>Sparse autoencoders (SAEs) aim to disentangle model activations into monosemantic, human-interpretable features. In practice, learned features are often redundant and vary across training runs and sparsity levels, which makes interpretations difficult to transfer and reuse. We introduce Distilled Matryoshka Sparse Autoencoders (DMSAEs), a training pipeline that distills a compact core of consistently useful features and reuses it to train new SAEs. DMSAEs run an iterative distillation cycle: tra</p><p><strong>Tags:</strong> SAE, features</p>]]></description>
      <pubDate>Wed, 31 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2512.24975v1</guid>
    </item>
    <item>
      <title>MEIC-DT: Memory-Efficient Incremental Clustering for Long-Text Coreference Resolution with Dual-Threshold Constraints</title>
      <link>https://arxiv.org/abs/2512.24711v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Kangyang Luo, Shuzheng Si, Yuzhuo Bai, Cheng Gao, Zhitong Wang, et al.</p><p>In the era of large language models (LLMs), supervised neural methods remain the state-of-the-art (SOTA) for Coreference Resolution. Yet, their full potential is underexplored, particularly in incremental clustering, which faces the critical challenge of balancing efficiency with performance for long texts. To address the limitation, we propose \textbf{MEIC-DT}, a novel dual-threshold, memory-efficient incremental clustering approach based on a lightweight Transformer. MEIC-DT features a dual-th</p><p><strong>Tags:</strong> features</p>]]></description>
      <pubDate>Wed, 31 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2512.24711v1</guid>
    </item>
    <item>
      <title>Fantastic Reasoning Behaviors and Where to Find Them: Unsupervised Discovery of the Reasoning Process</title>
      <link>https://arxiv.org/abs/2512.23988v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Zhenyu Zhang, Shujian Zhang, John Lambert, Wenxuan Zhou, Zhangyang Wang, et al.</p><p>Despite the growing reasoning capabilities of recent large language models (LLMs), their internal mechanisms during the reasoning process remain underexplored. Prior approaches often rely on human-defined concepts (e.g., overthinking, reflection) at the word level to analyze reasoning in a supervised manner. However, such methods are limited, as it is infeasible to capture the full spectrum of potential reasoning behaviors, many of which are difficult to define in token space. In this work, we p</p><p><strong>Tags:</strong> reasoning</p>]]></description>
      <pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2512.23988v1</guid>
    </item>
    <item>
      <title>Adversarial Lens: Exploiting Attention Layers to Generate Adversarial Examples for Evaluation</title>
      <link>https://arxiv.org/abs/2512.23837v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Kaustubh Dhole</p><p>Recent advances in mechanistic interpretability suggest that intermediate attention layers encode token-level hypotheses that are iteratively refined toward the final output. In this work, we exploit this property to generate adversarial examples directly from attention-layer token distributions. Unlike prompt-based or gradient-based attacks, our approach leverages model-internal token predictions, producing perturbations that are both plausible and internally consistent with the model&#x27;s own gen</p>]]></description>
      <pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2512.23837v1</guid>
    </item>
    <item>
      <title>Interpretable Safety Alignment via SAE-Constructed Low-Rank Subspace Adaptation</title>
      <link>https://arxiv.org/abs/2512.23260v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Dianyun Wang, Qingsen Ma, Yuhu Shang, Zhifeng Lu, Lechen Ning, et al.</p><p>Parameter-efficient fine-tuning has become the dominant paradigm for adapting large language models to downstream tasks. Low-rank adaptation methods such as LoRA operate under the assumption that task-relevant weight updates reside in a low-rank subspace, yet this subspace is learned implicitly from data in a black-box manner, offering no interpretability or direct control. We hypothesize that this difficulty stems from polysemanticity--individual dimensions encoding multiple entangled concepts.</p><p><strong>Tags:</strong> features, safety</p>]]></description>
      <pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2512.23260v1</guid>
    </item>
    <item>
      <title>Analysis of Short-Circuit Impedance in Multi-Winding Transformer</title>
      <link>https://www.semanticscholar.org/paper/fe33646738431bec8b7421c6b91815e74f327618</link>
      <description><![CDATA[<p><strong>Authors:</strong> Emir YÃ¼kselen</p><p>Due to green energy policies, investments in renewable energy are rapidly increasing, which in turn raises the demand for efficient power plant integration into electrical grids. Multi-winding distribution transformers are crucial in this process, as they enable operation at various voltage levels to match different grid requirements. However, designing such transformers, mainly calculating short-circuit impedance in accordance with industry standards, is a highly complex and challenging task wh</p><p><strong>Tags:</strong> circuits</p>]]></description>
      <pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://www.semanticscholar.org/paper/fe33646738431bec8b7421c6b91815e74f327618</guid>
    </item>
    <item>
      <title>Mechanistic Analysis of Circuit Preservation in Federated Learning</title>
      <link>https://arxiv.org/abs/2512.23043v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Muhammad Haseeb, Salaar Masood, Muhammad Abdullah Sohail</p><p>Federated Learning (FL) enables collaborative training of models on decentralized data, but its performance degrades significantly under Non-IID (non-independent and identically distributed) data conditions. While this accuracy loss is well-documented, the internal mechanistic causes remain a black box. This paper investigates the canonical FedAvg algorithm through the lens of Mechanistic Interpretability (MI) to diagnose this failure mode. We hypothesize that the aggregation of conflicting clie</p><p><strong>Tags:</strong> circuits</p>]]></description>
      <pubDate>Sun, 28 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2512.23043v1</guid>
    </item>
    <item>
      <title>Decomposing Task Vectors for Refined Model Editing</title>
      <link>https://arxiv.org/abs/2512.22511v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Hamed Damirchi, Ehsan Abbasnejad, Zhen Zhang, Javen Shi</p><p>Large pre-trained models have transformed machine learning, yet adapting these models effectively to exhibit precise, concept-specific behaviors remains a significant challenge. Task vectors, defined as the difference between fine-tuned and pre-trained model parameters, provide a mechanism for steering neural networks toward desired behaviors. This has given rise to large repositories dedicated to task vectors tailored for specific behaviors. The arithmetic operation of these task vectors allows</p><p><strong>Tags:</strong> steering, editing</p>]]></description>
      <pubDate>Sat, 27 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2512.22511v1</guid>
    </item>
    <item>
      <title>The Deepfake Detective: Interpreting Neural Forensics Through Sparse Features and Manifolds</title>
      <link>https://arxiv.org/abs/2512.21670v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Subramanyam Sahoo, Jared Junkin</p><p>Deepfake detection models have achieved high accuracy in identifying synthetic media, but their decision processes remain largely opaque. In this paper we present a mechanistic interpretability framework for deepfake detection applied to a vision-language model. Our approach combines a sparse autoencoder (SAE) analysis of internal network representations with a novel forensic manifold analysis that probes how the model&#x27;s features respond to controlled forensic artifact manipulations. We demonstr</p><p><strong>Tags:</strong> SAE, features, probing, vision</p>]]></description>
      <pubDate>Thu, 25 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2512.21670v1</guid>
    </item>
    <item>
      <title>Omni-Weather: Unified Multimodal Foundation Model for Weather Generation and Understanding</title>
      <link>https://arxiv.org/abs/2512.21643v2</link>
      <description><![CDATA[<p><strong>Authors:</strong> Zhiwang Zhou, Yuandong Pu, Xuming He, Yidi Liu, Yixin Chen, et al.</p><p>Weather modeling requires both accurate prediction and mechanistic interpretation, yet existing methods treat these goals in isolation, separating generation from understanding. To address this gap, we present Omni-Weather, the first multimodal foundation model that unifies weather generation and understanding within a single architecture. Omni-Weather integrates a radar encoder for weather generation tasks, followed by unified processing using a shared self-attention mechanism. Moreover, we con</p><p><strong>Tags:</strong> vision</p>]]></description>
      <pubDate>Thu, 25 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2512.21643v2</guid>
    </item>
    <item>
      <title>Learning from Negative Examples: Why Warning-Framed Training Data Teaches What It Warns Against</title>
      <link>https://arxiv.org/abs/2512.22293v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Tsogt-Ochir Enkhbayar</p><p>Warning-framed content in training data (e.g., &quot;DO NOT USE - this code is vulnerable&quot;) does not, it turns out, teach language models to avoid the warned-against behavior. In experiments reported here, models exposed to such warnings reproduced the flagged content at rates statistically indistinguishable from models given the content directly (76.7% vs. 83.3%). Why? Sparse autoencoder analysis points to a failure of orthogonalization: &quot;describing X&quot; and &quot;performing X&quot; activate overlapping latent </p><p><strong>Tags:</strong> SAE</p>]]></description>
      <pubDate>Thu, 25 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2512.22293v1</guid>
    </item>
    <item>
      <title>Mechanistic Interpretability of Fine-Tuned Protein Language Models for Nanobody Thermostability Prediction</title>
      <link>https://www.semanticscholar.org/paper/0544168cf2012e31794a69547a7537994417b47d</link>
      <description><![CDATA[<p><strong>Authors:</strong> Taihei Murakami, Yuki Hashidate, Yasuhiro Matsunaga</p><p><strong>Tags:</strong> biology</p>]]></description>
      <pubDate>Wed, 24 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://www.semanticscholar.org/paper/0544168cf2012e31794a69547a7537994417b47d</guid>
    </item>
  </channel>
</rss>
