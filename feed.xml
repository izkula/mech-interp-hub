<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Mechanistic Interpretability Hub</title>
    <link>https://izkula.github.io/cc</link>
    <description>Latest research in mechanistic interpretability - understanding how neural networks work internally</description>
    <language>en-us</language>
    <lastBuildDate>Mon, 26 Jan 2026 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://izkula.github.io/cc/feed.xml" rel="self" type="application/rss+xml"/>

    <item>
      <title>Research on the instability mechanism of transformer winding short-circuit faults based on magneto-mechanical coupling analysis</title>
      <link>https://www.semanticscholar.org/paper/48681d875dd4a7fd38cd0b9f05262cccabefe846</link>
      <description><![CDATA[<p><strong>Authors:</strong> Zhihao Gao, Jing Zhou, Li Zhu, Yuefeng Hao, Zhanlong Zhang, et al.</p><p><strong>Tags:</strong> circuits</p>]]></description>
      <pubDate>Sun, 01 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://www.semanticscholar.org/paper/48681d875dd4a7fd38cd0b9f05262cccabefe846</guid>
    </item>
    <item>
      <title>Attributing and Exploiting Safety Vectors through Global Optimization in Large Language Models</title>
      <link>https://arxiv.org/abs/2601.15801v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Fengheng Chu, Jiahao Chen, Yuhong Wang, Jun Wang, Zhihui Fu, et al.</p><p>While Large Language Models (LLMs) are aligned to mitigate risks, their safety guardrails remain fragile against jailbreak attacks. This reveals limited understanding of components governing safety. Existing methods rely on local, greedy attribution that assumes independent component contributions. However, they overlook the cooperative interactions between different components in LLMs, such as attention heads, which jointly contribute to safety mechanisms. We propose \textbf{G}lobal \textbf{O}p</p><p><strong>Tags:</strong> safety, attention</p>]]></description>
      <pubDate>Thu, 22 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.15801v1</guid>
    </item>
    <item>
      <title>From Insight to Intervention: Interpretable Neuron Steering for Controlling Popularity Bias in Recommender Systems</title>
      <link>https://arxiv.org/abs/2601.15122v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Parviz Ahmadov, Masoud Mansoury</p><p>Popularity bias is a pervasive challenge in recommender systems, where a few popular items dominate attention while the majority of less popular items remain underexposed. This imbalance can reduce recommendation quality and lead to unfair item exposure. Although existing mitigation methods address this issue to some extent, they often lack transparency in how they operate. In this paper, we propose a post-hoc approach, PopSteer, that leverages a Sparse Autoencoder (SAE) to both interpret and mi</p><p><strong>Tags:</strong> SAE, steering</p>]]></description>
      <pubDate>Wed, 21 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.15122v1</guid>
    </item>
    <item>
      <title>Mechanism Shift During Post-training from Autoregressive to Masked Diffusion Language Models</title>
      <link>https://arxiv.org/abs/2601.14758v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Injin Kong, Hyoungjoon Lee, Yohan Jo</p><p>Post-training pretrained Autoregressive models (ARMs) into Masked Diffusion models (MDMs) has emerged as a cost-effective strategy to overcome the limitations of sequential generation. However, the internal algorithmic transformations induced by this paradigm shift remain unexplored, leaving it unclear whether post-trained MDMs acquire genuine bidirectional reasoning capabilities or merely repackage autoregressive heuristics. In this work, we address this question by conducting a comparative cir</p><p><strong>Tags:</strong> reasoning</p>]]></description>
      <pubDate>Wed, 21 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.14758v1</guid>
    </item>
    <item>
      <title>CASL: Concept-Aligned Sparse Latents for Interpreting Diffusion Models</title>
      <link>https://arxiv.org/abs/2601.15441v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Zhenghao He, Guangzhi Xiong, Boyang Wang, Sanchit Sinha, Aidong Zhang</p><p>Internal activations of diffusion models encode rich semantic information, but interpreting such representations remains challenging. While Sparse Autoencoders (SAEs) have shown promise in disentangling latent representations, existing SAE-based methods for diffusion model understanding rely on unsupervised approaches that fail to align sparse features with human-understandable concepts. This limits their ability to provide reliable semantic control over generated images. We introduce CASL (Conc</p><p><strong>Tags:</strong> SAE, features, vision</p>]]></description>
      <pubDate>Wed, 21 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.15441v1</guid>
    </item>
    <item>
      <title>PRISM: Deriving the Transformer as a Signal-Denoising Operator via Maximum Coding Rate Reduction</title>
      <link>https://arxiv.org/abs/2601.15540v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Dongchen Huang</p><p>Deep learning models, particularly Transformers, are often criticized as &quot;black boxes&quot; and lack interpretability. We propose Prism, a white-box attention-based architecture derived from the principles of Maximizing Coding Rate Reduction ($\text{MCR}^2$). By modeling the attention mechanism as a gradient ascent process on a distinct signal-noise manifold, we introduce two physical constraints: an overcomplete dictionary to expand the representational phase space, and an irrational frequency separ</p>]]></description>
      <pubDate>Wed, 21 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.15540v1</guid>
    </item>
    <item>
      <title>Locate, Steer, and Improve: A Practical Survey of Actionable Mechanistic Interpretability in Large Language Models</title>
      <link>https://arxiv.org/abs/2601.14004v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Hengyuan Zhang, Zhihao Zhang, Mingyang Wang, Zunhai Su, Yiwei Wang, et al.</p><p>Mechanistic Interpretability (MI) has emerged as a vital approach to demystify the opaque decision-making of Large Language Models (LLMs). However, existing reviews primarily treat MI as an observational science, summarizing analytical insights while lacking a systematic framework for actionable intervention. To bridge this gap, we present a practical survey structured around the pipeline: &quot;Locate, Steer, and Improve.&quot; We formally categorize Localizing (diagnosis) and Steering (intervention) met</p><p><strong>Tags:</strong> steering, survey</p>]]></description>
      <pubDate>Tue, 20 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.14004v1</guid>
    </item>
    <item>
      <title>Patterning: The Dual of Interpretability</title>
      <link>https://arxiv.org/abs/2601.13548v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> George Wang, Daniel Murfet</p><p>Mechanistic interpretability aims to understand how neural networks generalize beyond their training data by reverse-engineering their internal structures. We introduce patterning as the dual problem: given a desired form of generalization, determine what training data produces it. Our approach is based on susceptibilities, which measure how posterior expectation values of observables respond to infinitesimal shifts in the data distribution. Inverting this linear response relationship yields the</p>]]></description>
      <pubDate>Tue, 20 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.13548v1</guid>
    </item>
    <item>
      <title>Hierarchical Sparse Circuit Extraction from Billion-Parameter Language Models through Scalable Attribution Graph Decomposition</title>
      <link>https://arxiv.org/abs/2601.12879v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Mohammed Mudassir Uddin, Shahnawaz Alam, Mohammed Kaif Pasha</p><p>Mechanistic interpretability seeks to reverse-engineer neural network computations into human-understandable algorithms, yet extracting sparse computational circuits from billion-parameter language models remains challenging due to exponential search complexity and pervasive polysemanticity. The proposed Hierarchical Attribution Graph Decomposition (HAGD) framework reduces circuit discovery complexity from O(2^n) exhaustive enumeration to O(n^2 log n) through multi-resolution abstraction hierarc</p><p><strong>Tags:</strong> circuits, features</p>]]></description>
      <pubDate>Mon, 19 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.12879v1</guid>
    </item>
    <item>
      <title>Conversational Context Classification: A Representation Engineering Approach</title>
      <link>https://arxiv.org/abs/2601.12286v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Jonathan Pan</p><p>The increasing prevalence of Large Language Models (LLMs) demands effective safeguards for their operation, particularly concerning their tendency to generate out-of-context responses. A key challenge is accurately detecting when LLMs stray from expected conversational norms, manifesting as topic shifts, factual inaccuracies, or outright hallucinations. Traditional anomaly detection struggles to directly apply within contextual semantics. This paper outlines our experiment in exploring the use o</p>]]></description>
      <pubDate>Sun, 18 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.12286v1</guid>
    </item>
    <item>
      <title>From Interpretability to Performance: Optimizing Retrieval Heads for Long-Context Language Models</title>
      <link>https://arxiv.org/abs/2601.11020v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Youmi Ma, Naoaki Okazaki</p><p>Advances in mechanistic interpretability have identified special attention heads, known as retrieval heads, that are responsible for retrieving information from the context. However, the role of these retrieval heads in improving model performance remains unexplored. This work investigates whether retrieval heads can be leveraged to enhance the long-context capabilities of LLMs. Specifically, we propose RetMask, a method that generates training signals by contrasting normal model outputs with th</p><p><strong>Tags:</strong> attention</p>]]></description>
      <pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.11020v1</guid>
    </item>
    <item>
      <title>From Knots to Knobs: Towards Steerable Collaborative Filtering Using Sparse Autoencoders</title>
      <link>https://arxiv.org/abs/2601.11182v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Martin Spišák, Ladislav Peška, Petr Škoda, Vojtěch Vančura, Rodrigo Alves</p><p>Sparse autoencoders (SAEs) have recently emerged as pivotal tools for introspection into large language models. SAEs can uncover high-quality, interpretable features at different levels of granularity and enable targeted steering of the generation process by selectively activating specific neurons in their latent activations. Our paper is the first to apply this approach to collaborative filtering, aiming to extract similarly interpretable features from representations learned purely from intera</p><p><strong>Tags:</strong> SAE, features, steering</p>]]></description>
      <pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.11182v1</guid>
    </item>
    <item>
      <title>Finding the Translation Switch: Discovering and Exploiting the Task-Initiation Features in LLMs</title>
      <link>https://arxiv.org/abs/2601.11019v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Xinwei Wu, Heng Liu, Xiaohu Zhao, Yuqi Ren, Linlong Xu, et al.</p><p>Large Language Models (LLMs) frequently exhibit strong translation abilities, even without task-specific fine-tuning. However, the internal mechanisms governing this innate capability remain largely opaque. To demystify this process, we leverage Sparse Autoencoders (SAEs) and introduce a novel framework for identifying task-specific features. Our method first recalls features that are frequently co-activated on translation inputs and then filters them for functional coherence using a PCA-based c</p><p><strong>Tags:</strong> SAE, features</p>]]></description>
      <pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.11019v1</guid>
    </item>
    <item>
      <title>Spurious Rewards Paradox: Mechanistically Understanding How RLVR Activates Memorization Shortcuts in LLMs</title>
      <link>https://arxiv.org/abs/2601.11061v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Lecheng Yan, Ruizhe Li, Guanhua Chen, Qing Li, Jiahui Geng, et al.</p><p>Reinforcement Learning with Verifiable Rewards (RLVR) is highly effective for enhancing LLM reasoning, yet recent evidence shows models like Qwen 2.5 achieve significant gains even with spurious or incorrect rewards. We investigate this phenomenon and identify a &quot;Perplexity Paradox&quot;: spurious RLVR triggers a divergence where answer-token perplexity drops while prompt-side coherence degrades, suggesting the model is bypassing reasoning in favor of memorization. Using Path Patching, Logit Lens, JS</p><p><strong>Tags:</strong> reasoning</p>]]></description>
      <pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.11061v1</guid>
    </item>
    <item>
      <title>TANDEM: Temporal-Aware Neural Detection for Multimodal Hate Speech</title>
      <link>https://arxiv.org/abs/2601.11178v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Girish A. Koushik, Helen Treharne, Diptesh Kanojia</p><p>Social media platforms are increasingly dominated by long-form multimodal content, where harmful narratives are constructed through a complex interplay of audio, visual, and textual cues. While automated systems can flag hate speech with high accuracy, they often function as &quot;black boxes&quot; that fail to provide the granular, interpretable evidence, such as precise timestamps and target identities, required for effective human-in-the-loop moderation. In this work, we introduce TANDEM, a unified fra</p><p><strong>Tags:</strong> safety, vision</p>]]></description>
      <pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.11178v1</guid>
    </item>
    <item>
      <title>Attesting Model Lineage by Consisted Knowledge Evolution with Fine-Tuning Trajectory</title>
      <link>https://arxiv.org/abs/2601.11683v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Zhuoyi Shang, Jiasen Li, Pengzhen Chen, Yanwei Liu, Xiaoyan Gu, et al.</p><p>The fine-tuning technique in deep learning gives rise to an emerging lineage relationship among models. This lineage provides a promising perspective for addressing security concerns such as unauthorized model redistribution and false claim of model provenance, which are particularly pressing in \textcolor{blue}{open-weight model} libraries where robust lineage verification mechanisms are often lacking. Existing approaches to model lineage detection primarily rely on static architectural similar</p>]]></description>
      <pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.11683v1</guid>
    </item>
    <item>
      <title>Diagnosing Generalization Failures in Fine-Tuned LLMs: A Cross-Architectural Study on Phishing Detection</title>
      <link>https://arxiv.org/abs/2601.10524v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Frank Bobe, Gregory D. Vetaw, Chase Pavlick, Darshan Bryner, Matthew Cook, et al.</p><p>The practice of fine-tuning Large Language Models (LLMs) has achieved state-of-the-art performance on specialized tasks, yet diagnosing why these models become brittle and fail to generalize remains a critical open problem. To address this, we introduce and apply a multi-layered diagnostic framework to a cross-architectural study. We fine-tune Llama 3.1 8B, Gemma 2 9B, and Mistral models on a high-stakes phishing detection task and use SHAP analysis and mechanistic interpretability to uncover th</p>]]></description>
      <pubDate>Thu, 15 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.10524v1</guid>
    </item>
    <item>
      <title>Measuring Affinity between Attention-Head Weight Subspaces via the Projection Kernel</title>
      <link>https://arxiv.org/abs/2601.10266v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Hiroaki Yamagiwa, Yusuke Takase, Hidetoshi Shimodaira</p><p>Understanding relationships between attention heads is essential for interpreting the internal structure of Transformers, yet existing metrics do not capture this structure well. We focus on the subspaces spanned by attention-head weight matrices and quantify head-to-head relationships using the Projection Kernel (PK), a principal-angle-based measure of subspace similarity. Experiments show that PK reproduces known head-to-head interactions on the IOI task more clearly than prior metrics such as</p><p><strong>Tags:</strong> attention</p>]]></description>
      <pubDate>Thu, 15 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.10266v1</guid>
    </item>
    <item>
      <title>Reasoning Models Generate Societies of Thought</title>
      <link>https://arxiv.org/abs/2601.10825v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Junsol Kim, Shiyang Lai, Nino Scherrer, Blaise Agüera y Arcas, James Evans</p><p>Large language models have achieved remarkable capabilities across domains, yet mechanisms underlying sophisticated reasoning remain elusive. Recent reasoning models outperform comparable instruction-tuned models on complex cognitive tasks, attributed to extended computation through longer chains of thought. Here we show that enhanced reasoning emerges not from extended computation alone, but from simulating multi-agent-like interactions -- a society of thought -- which enables diversification a</p><p><strong>Tags:</strong> reasoning</p>]]></description>
      <pubDate>Thu, 15 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.10825v1</guid>
    </item>
    <item>
      <title>Where Knowledge Collides: A Mechanistic Study of Intra-Memory Knowledge Conflict in Language Models</title>
      <link>https://arxiv.org/abs/2601.09445v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Minh Vu Pham, Hsuvas Borkakoty, Yufang Hou</p><p>In language models (LMs), intra-memory knowledge conflict largely arises when inconsistent information about the same event is encoded within the model&#x27;s parametric knowledge. While prior work has primarily focused on resolving conflicts between a model&#x27;s internal knowledge and external resources through approaches such as fine-tuning or knowledge editing, the problem of localizing conflicts that originate during pre-training within the model&#x27;s internal representations remain unexplored. In this</p><p><strong>Tags:</strong> editing</p>]]></description>
      <pubDate>Wed, 14 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.09445v1</guid>
    </item>
    <item>
      <title>CLaS-Bench: A Cross-Lingual Alignment and Steering Benchmark</title>
      <link>https://arxiv.org/abs/2601.08331v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Daniil Gurgurov, Yusser Al Ghussin, Tanja Baeumel, Cheng-Ting Chou, Patrick Schramowski, et al.</p><p>Understanding and controlling the behavior of large language models (LLMs) is an increasingly important topic in multilingual NLP. Beyond prompting or fine-tuning, , i.e.,~manipulating internal representations during inference, has emerged as a more efficient and interpretable technique for adapting models to a target language. Yet, no dedicated benchmarks or evaluation protocols exist to quantify the effectiveness of steering techniques. We introduce CLaS-Bench, a lightweight parallel-question </p><p><strong>Tags:</strong> safety, steering</p>]]></description>
      <pubDate>Tue, 13 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.08331v1</guid>
    </item>
    <item>
      <title>Reasoning Beyond Chain-of-Thought: A Latent Computational Mode in Large Language Models</title>
      <link>https://arxiv.org/abs/2601.08058v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Zhenghao He, Guangzhi Xiong, Bohan Liu, Sanchit Sinha, Aidong Zhang</p><p>Chain-of-Thought (CoT) prompting has improved the reasoning performance of large language models (LLMs), but it remains unclear why it works and whether it is the unique mechanism for triggering reasoning in large language models. In this work, we study this question by directly analyzing and intervening on the internal representations of LLMs with Sparse Autoencoders (SAEs), identifying a small set of latent features that are causally associated with LLM reasoning behavior. Across multiple mode</p><p><strong>Tags:</strong> SAE, features, reasoning</p>]]></description>
      <pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.08058v1</guid>
    </item>
    <item>
      <title>Semantic Gravity Wells: Why Negative Constraints Backfire</title>
      <link>https://arxiv.org/abs/2601.08070v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Shailesh Rana</p><p>Negative constraints (instructions of the form &quot;do not use word X&quot;) represent a fundamental test of instruction-following capability in large language models. Despite their apparent simplicity, these constraints fail with striking regularity, and the conditions governing failure have remained poorly understood. This paper presents the first comprehensive mechanistic investigation of negative instruction failure. We introduce semantic pressure, a quantitative measure of the model&#x27;s intrinsic prob</p>]]></description>
      <pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.08070v1</guid>
    </item>
    <item>
      <title>Dynamical Systems Analysis Reveals Functional Regimes in Large Language Models</title>
      <link>https://arxiv.org/abs/2601.11622v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Hassan Ugail, Newton Howard</p><p>Large language models perform text generation through high-dimensional internal dynamics, yet the temporal organisation of these dynamics remains poorly understood. Most interpretability approaches emphasise static representations or causal interventions, leaving temporal structure largely unexplored. Drawing on neuroscience, where temporal integration and metastability are core markers of neural organisation, we adapt these concepts to transformer models and discuss a composite dynamical metric</p>]]></description>
      <pubDate>Sun, 11 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.11622v1</guid>
    </item>
    <item>
      <title>Time Travel Engine: A Shared Latent Chronological Manifold Enables Historical Navigation in Large Language Models</title>
      <link>https://arxiv.org/abs/2601.06437v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Jingmin An, Wei Liu, Qian Wang, Fang Fang</p><p>Time functions as a fundamental dimension of human cognition, yet the mechanisms by which Large Language Models (LLMs) encode chronological progression remain opaque. We demonstrate that temporal information in their latent space is organized not as discrete clusters but as a continuous, traversable geometry. We introduce the Time Travel Engine (TTE), an interpretability-driven framework that projects diachronic linguistic patterns onto a shared chronological manifold. Unlike surface-level promp</p>]]></description>
      <pubDate>Sat, 10 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.06437v1</guid>
    </item>
    <item>
      <title>Deriving Decoder-Free Sparse Autoencoders from First Principles</title>
      <link>https://arxiv.org/abs/2601.06478v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Alan Oursland</p><p>Gradient descent on log-sum-exp (LSE) objectives performs implicit expectation--maximization (EM): the gradient with respect to each component output equals its responsibility. The same theory predicts collapse without volume control analogous to the log-determinant in Gaussian mixture models. We instantiate the theory in a single-layer encoder with an LSE objective and InfoMax regularization for volume control. Experiments confirm the theory&#x27;s predictions. The gradient--responsibility identity </p><p><strong>Tags:</strong> SAE, theory</p>]]></description>
      <pubDate>Sat, 10 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.06478v1</guid>
    </item>
    <item>
      <title>Do Sparse Autoencoders Identify Reasoning Features in Language Models?</title>
      <link>https://arxiv.org/abs/2601.05679v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> George Ma, Zhongyuan Liang, Irene Y. Chen, Somayeh Sojoudi</p><p>We investigate whether sparse autoencoders (SAEs) identify genuine reasoning features in large language models (LLMs). Starting from features selected using standard contrastive activation methods, we introduce a falsification-oriented framework that combines causal token injection experiments and LLM-guided falsification to test whether feature activation reflects reasoning processes or superficial linguistic correlates. Across 20 configurations spanning multiple model families, layers, and rea</p><p><strong>Tags:</strong> SAE, features, reasoning</p>]]></description>
      <pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.05679v1</guid>
    </item>
    <item>
      <title>Context-Aware Decoding for Faithful Vision-Language Generation</title>
      <link>https://arxiv.org/abs/2601.05939v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Mehrdad Fazli, Bowen Wei, Ziwei Zhu</p><p>Hallucinations, generating responses inconsistent with the visual input, remain a critical limitation of large vision-language models (LVLMs), especially in open-ended tasks such as image captioning and visual reasoning. In this work, we probe the layer-wise generation dynamics that drive hallucinations and propose a training-free mitigation strategy. Employing the Logit Lens, we examine how LVLMs construct next-token distributions across decoder layers, uncovering a pronounced commitment-depth </p><p><strong>Tags:</strong> probing, vision, reasoning</p>]]></description>
      <pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.05939v1</guid>
    </item>
    <item>
      <title>Circuit Mechanisms for Spatial Relation Generation in Diffusion Transformers</title>
      <link>https://arxiv.org/abs/2601.06338v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Binxu Wang, Jingxuan Fan, Xu Pan</p><p>Diffusion Transformers (DiTs) have greatly advanced text-to-image generation, but models still struggle to generate the correct spatial relations between objects as specified in the text prompt. In this study, we adopt a mechanistic interpretability approach to investigate how a DiT can generate correct spatial relations between objects. We train, from scratch, DiTs of different sizes with different text encoders to learn to generate images containing two objects whose attributes and spatial rel</p><p><strong>Tags:</strong> circuits, vision</p>]]></description>
      <pubDate>Fri, 09 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.06338v1</guid>
    </item>
    <item>
      <title>LANGSAE EDITING: Improving Multilingual Information Retrieval via Post-hoc Language Identity Removal</title>
      <link>https://arxiv.org/abs/2601.04768v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Dongjun Kim, Jeongho Yoon, Chanjun Park, Heuiseok Lim</p><p>Dense retrieval in multilingual settings often searches over mixed-language collections, yet multilingual embeddings encode language identity alongside semantics. This language signal can inflate similarity for same-language pairs and crowd out relevant evidence written in other languages. We propose LANGSAE EDITING, a post-hoc sparse autoencoder trained on pooled embeddings that enables controllable removal of language-identity signal directly in vector space. The method identifies language-ass</p><p><strong>Tags:</strong> SAE</p>]]></description>
      <pubDate>Thu, 08 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.04768v1</guid>
    </item>
  </channel>
</rss>
