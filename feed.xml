<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Mechanistic Interpretability Hub</title>
    <link>https://izkula.github.io/cc</link>
    <description>Latest research in mechanistic interpretability - understanding how neural networks work internally</description>
    <language>en-us</language>
    <lastBuildDate>Sat, 04 Jan 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://izkula.github.io/cc/feed.xml" rel="self" type="application/rss+xml"/>

    <item>
      <title>Open Problems in Mechanistic Interpretability</title>
      <link>https://arxiv.org/abs/2501.16496</link>
      <description><![CDATA[<p><strong>Authors:</strong> Lee Sharkey, Bilal Chughtai, Joshua Batson, Jack Lindamood, et al.</p><p>A comprehensive survey of open problems requiring solutions before scientific and practical benefits of mechanistic interpretability can be realized, covering methodological, application, and socio-technical challenges.</p><p><strong>Tags:</strong> survey, open-problems, methodology</p>]]></description>
      <pubDate>Mon, 27 Jan 2025 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2501.16496</guid>
    </item>
    <item>
      <title>Circuit Tracing: Revealing Computational Graphs in Language Models</title>
      <link>https://transformer-circuits.pub/2025/attribution-graphs/methods.html</link>
      <description><![CDATA[<p><strong>Authors:</strong> Anthropic Interpretability Team</p><p>A new method for understanding how features interact to produce model outputs through attribution graphs, enabling analysis of computational structure on individual prompts.</p><p><strong>Tags:</strong> circuits, attribution, methods</p>]]></description>
      <pubDate>Wed, 15 Jan 2025 00:00:00 +0000</pubDate>
      <guid>https://transformer-circuits.pub/2025/attribution-graphs/methods.html</guid>
    </item>
    <item>
      <title>Circuits Updates - July 2025</title>
      <link>https://transformer-circuits.pub/2025/july-update/index.html</link>
      <description><![CDATA[<p><strong>Authors:</strong> Anthropic Interpretability Team</p><p>Latest research updates including findings on arithmetic heuristics, lookup table features, and applications to biology including Evo 2 DNA foundation models.</p><p><strong>Tags:</strong> circuits, arithmetic, biology</p>]]></description>
      <pubDate>Tue, 01 Jul 2025 00:00:00 +0000</pubDate>
      <guid>https://transformer-circuits.pub/2025/july-update/index.html</guid>
    </item>
    <item>
      <title>Beyond Components: Singular Vector-Based Interpretability of Transformer Circuits</title>
      <link>https://arxiv.org/abs/2511.20273</link>
      <description><![CDATA[<p><strong>Authors:</strong> Various</p><p>Demonstrates that transformer components&#x27; computation is distributed along a small number of interpretable, low-rank axes that can be independently manipulated.</p><p><strong>Tags:</strong> circuits, linear-algebra, low-rank</p>]]></description>
      <pubDate>Sun, 30 Nov 2025 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2511.20273</guid>
    </item>
    <item>
      <title>Interpretability in Parameter Space: Attribution-based Parameter Decomposition</title>
      <link>https://arxiv.org/abs/2501.14926</link>
      <description><![CDATA[<p><strong>Authors:</strong> Various</p><p>A new approach that identifies minimal circuits in superposition, separates compressed computations, and identifies cross-layer distributed representations.</p><p><strong>Tags:</strong> methods, superposition, circuits</p>]]></description>
      <pubDate>Fri, 24 Jan 2025 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2501.14926</guid>
    </item>
    <item>
      <title>A Practical Review of Mechanistic Interpretability for Transformer-Based Language Models</title>
      <link>https://arxiv.org/abs/2407.02646</link>
      <description><![CDATA[<p><strong>Authors:</strong> Rai, Zhou, et al.</p><p>A practical guide to mechanistic interpretability research, covering the workflow from problem formulation to validation, with focus on open problems.</p><p><strong>Tags:</strong> survey, tutorial, methodology</p>]]></description>
      <pubDate>Wed, 03 Jul 2024 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2407.02646</guid>
    </item>
    <item>
      <title>Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet</title>
      <link>https://transformer-circuits.pub/2024/scaling-monosemanticity/</link>
      <description><![CDATA[<p><strong>Authors:</strong> Anthropic Interpretability Team</p><p>Demonstrates extraction of millions of interpretable features from Claude 3 Sonnet using sparse autoencoders, finding multilingual, multimodal, and safety-relevant features.</p><p><strong>Tags:</strong> SAE, features, scaling, safety</p>]]></description>
      <pubDate>Tue, 21 May 2024 00:00:00 +0000</pubDate>
      <guid>https://transformer-circuits.pub/2024/scaling-monosemanticity/</guid>
    </item>
    <item>
      <title>Crosscoders: Finding Shared Features Across Models</title>
      <link>https://transformer-circuits.pub/2024/crosscoders/</link>
      <description><![CDATA[<p><strong>Authors:</strong> Anthropic Interpretability Team</p><p>Extends dictionary learning to find features shared across different models, providing evidence for feature universality.</p><p><strong>Tags:</strong> SAE, universality, features</p>]]></description>
      <pubDate>Tue, 15 Oct 2024 00:00:00 +0000</pubDate>
      <guid>https://transformer-circuits.pub/2024/crosscoders/</guid>
    </item>
    <item>
      <title>Circuits Updates - July 2024</title>
      <link>https://transformer-circuits.pub/2024/july-update/index.html</link>
      <description><![CDATA[<p><strong>Authors:</strong> Anthropic Interpretability Team</p><p>Research updates on factual recall mechanisms, detokenization in early layers, and information transmission via attention heads.</p><p><strong>Tags:</strong> circuits, factual-recall, attention</p>]]></description>
      <pubDate>Mon, 01 Jul 2024 00:00:00 +0000</pubDate>
      <guid>https://transformer-circuits.pub/2024/july-update/index.html</guid>
    </item>
    <item>
      <title>Circuits Updates - January 2024</title>
      <link>https://transformer-circuits.pub/2024/jan-update/index.html</link>
      <description><![CDATA[<p><strong>Authors:</strong> Anthropic Interpretability Team</p><p>Updates on interpretability research including progress on understanding model computations.</p><p><strong>Tags:</strong> circuits, update</p>]]></description>
      <pubDate>Mon, 15 Jan 2024 00:00:00 +0000</pubDate>
      <guid>https://transformer-circuits.pub/2024/jan-update/index.html</guid>
    </item>
    <item>
      <title>Mechanistic Interpretability for AI Safety - A Review</title>
      <link>https://arxiv.org/abs/2404.14082</link>
      <description><![CDATA[<p><strong>Authors:</strong> Leonard Bereska, et al.</p><p>Comprehensive review of mechanistic interpretability from an AI safety perspective, covering methods, applications, and open challenges.</p><p><strong>Tags:</strong> survey, safety, review</p>]]></description>
      <pubDate>Mon, 22 Apr 2024 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2404.14082</guid>
    </item>
    <item>
      <title>Representation Engineering: A Top-Down Approach to AI Transparency</title>
      <link>https://arxiv.org/abs/2310.01405</link>
      <description><![CDATA[<p><strong>Authors:</strong> Dan Hendrycks, Collin Burns, et al.</p><p>Introduces representation engineering for identifying and manipulating high-level concepts like honesty in neural networks.</p><p><strong>Tags:</strong> representation, concepts, methods</p>]]></description>
      <pubDate>Mon, 02 Oct 2023 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2310.01405</guid>
    </item>
    <item>
      <title>Towards Monosemanticity: Decomposing Language Models With Dictionary Learning</title>
      <link>https://transformer-circuits.pub/2023/monosemantic-features/</link>
      <description><![CDATA[<p><strong>Authors:</strong> Anthropic Interpretability Team</p><p>Demonstrates that sparse autoencoders can extract monosemantic features from a one-layer transformer, laying groundwork for scaling to larger models.</p><p><strong>Tags:</strong> SAE, features, dictionary-learning</p>]]></description>
      <pubDate>Wed, 04 Oct 2023 00:00:00 +0000</pubDate>
      <guid>https://transformer-circuits.pub/2023/monosemantic-features/</guid>
    </item>
    <item>
      <title>Toy Models of Superposition</title>
      <link>https://transformer-circuits.pub/2022/toy_model/index.html</link>
      <description><![CDATA[<p><strong>Authors:</strong> Anthropic Interpretability Team</p><p>Introduces and analyzes superposition: how neural networks represent more features than dimensions by using nearly-orthogonal directions.</p><p><strong>Tags:</strong> superposition, theory, foundational</p>]]></description>
      <pubDate>Wed, 14 Sep 2022 00:00:00 +0000</pubDate>
      <guid>https://transformer-circuits.pub/2022/toy_model/index.html</guid>
    </item>
    <item>
      <title>In-context Learning and Induction Heads</title>
      <link>https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/</link>
      <description><![CDATA[<p><strong>Authors:</strong> Anthropic Interpretability Team</p><p>Identifies induction heads as a key circuit for in-context learning, showing they form via a phase transition during training.</p><p><strong>Tags:</strong> circuits, in-context-learning, attention</p>]]></description>
      <pubDate>Tue, 08 Mar 2022 00:00:00 +0000</pubDate>
      <guid>https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/</guid>
    </item>
    <item>
      <title>A Mathematical Framework for Transformer Circuits</title>
      <link>https://transformer-circuits.pub/2021/framework/</link>
      <description><![CDATA[<p><strong>Authors:</strong> Anthropic Interpretability Team</p><p>Foundational paper establishing the mathematical framework for analyzing transformer circuits, including residual stream and attention head analysis.</p><p><strong>Tags:</strong> theory, foundational, circuits</p>]]></description>
      <pubDate>Wed, 22 Dec 2021 00:00:00 +0000</pubDate>
      <guid>https://transformer-circuits.pub/2021/framework/</guid>
    </item>
    <item>
      <title>Sparse Autoencoders Uncover Biologically Interpretable Features in Protein Language Models</title>
      <link>https://pmc.ncbi.nlm.nih.gov/articles/PMC12403088/</link>
      <description><![CDATA[<p><strong>Authors:</strong> Various</p><p>Applies SAEs to protein language models, successfully disentangling biological concepts and revealing missing database annotations.</p><p><strong>Tags:</strong> SAE, biology, proteins</p>]]></description>
      <pubDate>Sun, 15 Jun 2025 00:00:00 +0000</pubDate>
      <guid>https://pmc.ncbi.nlm.nih.gov/articles/PMC12403088/</guid>
    </item>
    <item>
      <title>Eliciting Latent Predictions from Transformers with the Tuned Lens</title>
      <link>https://arxiv.org/abs/2303.08112</link>
      <description><![CDATA[<p><strong>Authors:</strong> Nostalgebraist, et al.</p><p>Introduces the tuned lens for projecting intermediate activations to vocabulary space, improving on the logit lens.</p><p><strong>Tags:</strong> methods, probing, predictions</p>]]></description>
      <pubDate>Tue, 14 Mar 2023 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2303.08112</guid>
    </item>
    <item>
      <title>Locating and Editing Factual Associations in GPT</title>
      <link>https://arxiv.org/abs/2202.05262</link>
      <description><![CDATA[<p><strong>Authors:</strong> Kevin Meng, David Bau, Alex Andonian, Yonatan Belinkov</p><p>Introduces causal tracing and ROME for locating and editing factual knowledge in language models.</p><p><strong>Tags:</strong> editing, factual-knowledge, methods</p>]]></description>
      <pubDate>Thu, 10 Feb 2022 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2202.05262</guid>
    </item>
    <item>
      <title>Transcoders Find Interpretable LLM Feature Circuits</title>
      <link>https://arxiv.org/abs/2406.11944</link>
      <description><![CDATA[<p><strong>Authors:</strong> Various</p><p>Introduces transcoders for mapping between activation spaces and finding interpretable circuits.</p><p><strong>Tags:</strong> SAE, circuits, methods</p>]]></description>
      <pubDate>Mon, 17 Jun 2024 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2406.11944</guid>
    </item>
    <item>
      <title>Attribution Patching: Activation Patching At Industrial Scale</title>
      <link>https://arxiv.org/abs/2310.10348</link>
      <description><![CDATA[<p><strong>Authors:</strong> Various</p><p>Scales activation patching using gradient-based attribution, enabling efficient circuit discovery in large models.</p><p><strong>Tags:</strong> methods, circuits, scaling</p>]]></description>
      <pubDate>Mon, 16 Oct 2023 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2310.10348</guid>
    </item>
    <item>
      <title>The Linear Representation Hypothesis and the Geometry of Large Language Models</title>
      <link>https://arxiv.org/abs/2311.03658</link>
      <description><![CDATA[<p><strong>Authors:</strong> Various</p><p>Investigates the linear representation hypothesis and its implications for understanding how LLMs encode concepts.</p><p><strong>Tags:</strong> theory, representations, geometry</p>]]></description>
      <pubDate>Mon, 06 Nov 2023 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2311.03658</guid>
    </item>
    <item>
      <title>Survey on the Role of Mechanistic Interpretability in Generative AI</title>
      <link>https://www.mdpi.com/2504-2289/9/8/193</link>
      <description><![CDATA[<p><strong>Authors:</strong> Various</p><p>Survey covering the role of mechanistic interpretability in understanding and improving generative AI systems.</p><p><strong>Tags:</strong> survey, generative-ai</p>]]></description>
      <pubDate>Sun, 15 Dec 2024 00:00:00 +0000</pubDate>
      <guid>https://www.mdpi.com/2504-2289/9/8/193</guid>
    </item>
    <item>
      <title>Mechanistic Interpretability in the Presence of Architectural Degeneracy</title>
      <link>https://arxiv.org/abs/2506.18053</link>
      <description><![CDATA[<p><strong>Authors:</strong> Various</p><p>Addresses challenges of mechanistic interpretability when multiple architectures can implement the same function.</p><p><strong>Tags:</strong> theory, challenges</p>]]></description>
      <pubDate>Mon, 23 Jun 2025 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2506.18053</guid>
    </item>
  </channel>
</rss>
