<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Mechanistic Interpretability Hub</title>
    <link>https://izkula.github.io/cc</link>
    <description>Latest research in mechanistic interpretability - understanding how neural networks work internally</description>
    <language>en-us</language>
    <lastBuildDate>Sat, 07 Feb 2026 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://izkula.github.io/cc/feed.xml" rel="self" type="application/rss+xml"/>

    <item>
      <title>DLM-Scope: Mechanistic Interpretability of Diffusion Language Models via Sparse Autoencoders</title>
      <link>https://arxiv.org/abs/2602.05859v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Xu Wang, Bingqing Jiang, Yu Wan, Baosong Yang, Lingpeng Kong, et al.</p><p>Sparse autoencoders (SAEs) have become a standard tool for mechanistic interpretability in autoregressive large language models (LLMs), enabling researchers to extract sparse, human-interpretable features and intervene on model behavior. Recently, as diffusion language models (DLMs) have become an increasingly promising alternative to the autoregressive LLMs, it is essential to develop tailored mechanistic interpretability tools for this emerging class of models. In this work, we present DLM-Sco</p><p><strong>Tags:</strong> SAE, features</p>]]></description>
      <pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.05859v1</guid>
    </item>
    <item>
      <title>Split Personality Training: Revealing Latent Knowledge Through Alternate Personalities</title>
      <link>https://arxiv.org/abs/2602.05532v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Florian Dietz, William Wale, Oscar Gilg, Robert McCarthy, Felix Michalak, et al.</p><p>Detecting misalignment in large language models is challenging because models may learn to conceal misbehavior during training. Standard auditing techniques fall short: black-box methods often cannot distinguish misaligned outputs from benign ones, and mechanistic interpretability does not scale with model capabilities. We introduce Split Personality Training (SPT), which fine-tunes a second ``honest persona&#x27;&#x27; into LoRA parameters that remain inactive during normal operation. After the main mode</p><p><strong>Tags:</strong> safety</p>]]></description>
      <pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.05532v1</guid>
    </item>
    <item>
      <title>Causal Front-Door Adjustment for Robust Jailbreak Attacks on LLMs</title>
      <link>https://arxiv.org/abs/2602.05444v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Yao Zhou, Zeen Song, Wenwen Qiang, Fengge Wu, Shuyi Zhou, et al.</p><p>Safety alignment mechanisms in Large Language Models (LLMs) often operate as latent internal states, obscuring the model&#x27;s inherent capabilities. Building on this observation, we model the safety mechanism as an unobserved confounder from a causal perspective. Then, we propose the \textbf{C}ausal \textbf{F}ront-Door \textbf{A}djustment \textbf{A}ttack ({\textbf{CFA}}$^2$) to jailbreak LLM, which is a framework that leverages Pearl&#x27;s Front-Door Criterion to sever the confounding associations for </p><p><strong>Tags:</strong> safety</p>]]></description>
      <pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.05444v1</guid>
    </item>
    <item>
      <title>Advancing Opinion Dynamics Modeling with Neural Diffusion-Convection-Reaction Equation</title>
      <link>https://arxiv.org/abs/2602.05403v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Chenghua Gong, Yihang Jiang, Hao Li, Rui Sun, Juyuan Zhang, et al.</p><p>Advanced opinion dynamics modeling is vital for deciphering social behavior, emphasizing its role in mitigating polarization and securing cyberspace. To synergize mechanistic interpretability with data-driven flexibility, recent studies have explored the integration of Physics-Informed Neural Networks (PINNs) for opinion modeling. Despite this promise, existing methods are tailored to incomplete priors, lacking a comprehensive physical system to integrate dynamics from local, global, and endogen</p>]]></description>
      <pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.05403v1</guid>
    </item>
    <item>
      <title>Data-Centric Interpretability for LLM-based Multi-Agent Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2602.05183v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> John Yan, Michael Yu, Yuqi Sun, Alexander Duffy, Tyler Marques, et al.</p><p>Large language models (LLMs) are increasingly trained in complex Reinforcement Learning, multi-agent environments, making it difficult to understand how behavior changes over training. Sparse Autoencoders (SAEs) have recently shown to be useful for data-centric interpretability. In this work, we analyze large-scale reinforcement learning training runs from the sophisticated environment of Full-Press Diplomacy by applying pretrained SAEs, alongside LLM-summarizer methods. We introduce Meta-Autoin</p><p><strong>Tags:</strong> SAE</p>]]></description>
      <pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.05183v1</guid>
    </item>
    <item>
      <title>Disentangling meaning from language in LLM-based machine translation</title>
      <link>https://arxiv.org/abs/2602.04613v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Théo Lasnier, Armel Zebaze, Djamé Seddah, Rachel Bawden, Benoît Sagot</p><p>Mechanistic Interpretability (MI) seeks to explain how neural networks implement their capabilities, but the scale of Large Language Models (LLMs) has limited prior MI work in Machine Translation (MT) to word-level analyses. We study sentence-level MT from a mechanistic perspective by analyzing attention heads to understand how LLMs internally encode and distribute translation functions. We decompose MT into two subtasks: producing text in the target language (i.e. target language identification</p><p><strong>Tags:</strong> attention</p>]]></description>
      <pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.04613v1</guid>
    </item>
    <item>
      <title>Identifying Intervenable and Interpretable Features via Orthogonality Regularization</title>
      <link>https://arxiv.org/abs/2602.04718v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Moritz Miller, Florent Draye, Bernhard Schölkopf</p><p>With recent progress on fine-tuning language models around a fixed sparse autoencoder, we disentangle the decoder matrix into almost orthogonal features. This reduces interference and superposition between the features, while keeping performance on the target dataset essentially unchanged. Our orthogonality penalty leads to identifiable features, ensuring the uniqueness of the decomposition. Further, we find that the distance between embedded feature explanations increases with stricter orthogon</p><p><strong>Tags:</strong> SAE, superposition, features</p>]]></description>
      <pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.04718v1</guid>
    </item>
    <item>
      <title>Decomposing Query-Key Feature Interactions Using Contrastive Covariances</title>
      <link>https://arxiv.org/abs/2602.04752v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Andrew Lee, Yonatan Belinkov, Fernanda Viégas, Martin Wattenberg</p><p>Despite the central role of attention heads in Transformers, we lack tools to understand why a model attends to a particular token. To address this, we study the query-key (QK) space -- the bilinear joint embedding space between queries and keys. We present a contrastive covariance method to decompose the QK space into low-rank, human-interpretable components. It is when features in keys and queries align in these low-rank subspaces that high attention scores are produced. We first study our met</p><p><strong>Tags:</strong> features, attention</p>]]></description>
      <pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.04752v1</guid>
    </item>
    <item>
      <title>AudioSAE: Towards Understanding of Audio-Processing Models with Sparse AutoEncoders</title>
      <link>https://arxiv.org/abs/2602.05027v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Georgii Aparin, Tasnima Sadekova, Alexey Rukhovich, Assel Yermekova, Laida Kushnareva, et al.</p><p>Sparse Autoencoders (SAEs) are powerful tools for interpreting neural representations, yet their use in audio remains underexplored. We train SAEs across all encoder layers of Whisper and HuBERT, provide an extensive evaluation of their stability, interpretability, and show their practical utility. Over 50% of the features remain consistent across random seeds, and reconstruction quality is preserved. SAE features capture general acoustic and semantic information as well as specific events, incl</p><p><strong>Tags:</strong> SAE, features</p>]]></description>
      <pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.05027v1</guid>
    </item>
    <item>
      <title>Explaining the Explainer: Understanding the Inner Workings of Transformer-based Symbolic Regression Models</title>
      <link>https://arxiv.org/abs/2602.03506v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Arco van Breda, Erman Acar</p><p>Following their success across many domains, transformers have also proven effective for symbolic regression (SR); however, the internal mechanisms underlying their generation of mathematical operators remain largely unexplored. Although mechanistic interpretability has successfully identified circuits in language and vision models, it has not yet been applied to SR. In this article, we introduce PATCHES, an evolutionary circuit discovery algorithm that identifies compact and correct circuits fo</p><p><strong>Tags:</strong> circuits, vision</p>]]></description>
      <pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.03506v1</guid>
    </item>
    <item>
      <title>CSR-Bench: A Benchmark for Evaluating the Cross-modal Safety and Reliability of MLLMs</title>
      <link>https://arxiv.org/abs/2602.03263v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Yuxuan Liu, Yuntian Shi, Kun Wang, Haoting Shen, Kun Yang</p><p>Multimodal large language models (MLLMs) enable interaction over both text and images, but their safety behavior can be driven by unimodal shortcuts instead of true joint intent understanding. We introduce CSR-Bench, a benchmark for evaluating cross-modal reliability through four stress-testing interaction patterns spanning Safety, Over-rejection, Bias, and Hallucination, covering 61 fine-grained types. Each instance is constructed to require integrated image-text interpretation, and we addition</p><p><strong>Tags:</strong> safety, vision</p>]]></description>
      <pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.03263v1</guid>
    </item>
    <item>
      <title>Momentum Attention: The Physics of In-Context Learning and Spectral Forensics for Mechanistic Interpretability</title>
      <link>https://arxiv.org/abs/2602.04902v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Kingsuk Maitra</p><p>The Mechanistic Interpretability (MI) program has mapped the Transformer as a precise computational graph. We extend this graph with a conservation law and time-varying AC dynamics, viewing it as a physical circuit. We introduce Momentum Attention, a symplectic augmentation embedding physical priors via the kinematic difference operator $p_t = q_t - q_{t-1}$, implementing the symplectic shear $\hat{q}_t = q_t + γp_t$ on queries and keys. We identify a fundamental Symplectic-Filter Duality: the p</p><p><strong>Tags:</strong> circuits</p>]]></description>
      <pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.04902v1</guid>
    </item>
    <item>
      <title>Mind the Performance Gap: Capability-Behavior Trade-offs in Feature Steering</title>
      <link>https://arxiv.org/abs/2602.04903v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Eitan Sprejer, Oscar Agustín Stanchi, María Victoria Carro, Denise Alejandra Mester, Iván Arcuschin</p><p>Feature steering has emerged as a promising approach for controlling LLM behavior through direct manipulation of internal representations, offering advantages over prompt engineering. However, its practical effectiveness in real-world applications remains poorly understood, particularly regarding potential trade-offs with output quality. We show that feature steering methods substantially degrade model performance even when successfully controlling target behaviors, a critical trade-off. Specifi</p><p><strong>Tags:</strong> features, steering</p>]]></description>
      <pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.04903v1</guid>
    </item>
    <item>
      <title>Universal Redundancies in Time Series Foundation Models</title>
      <link>https://arxiv.org/abs/2602.01605v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Anthony Bao, Venkata Hasith Vattikuti, Jeffrey Lai, William Gilpin</p><p>Time Series Foundation Models (TSFMs) leverage extensive pretraining to accurately predict unseen time series during inference, without the need for task-specific fine-tuning. Through large-scale evaluations on standard benchmarks, we find that leading transformer-based TSFMs exhibit redundant components in their intermediate layers. We introduce a set of tools for mechanistic interpretability of TSFMs, including ablations of specific components and direct logit attribution on the residual strea</p>]]></description>
      <pubDate>Mon, 02 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.01605v1</guid>
    </item>
    <item>
      <title>Mechanistic Indicators of Steering Effectiveness in Large Language Models</title>
      <link>https://arxiv.org/abs/2602.01716v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Mehdi Jafari, Hao Xue, Flora Salim</p><p>Activation-based steering enables Large Language Models (LLMs) to exhibit targeted behaviors by intervening on intermediate activations without retraining. Despite its widespread use, the mechanistic factors that govern when steering succeeds or fails remain poorly understood, as prior work has relied primarily on black-box outputs or LLM-based judges. In this study, we investigate whether the reliability of steering can be diagnosed using internal model signals. We focus on two information-theo</p><p><strong>Tags:</strong> steering</p>]]></description>
      <pubDate>Mon, 02 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.01716v1</guid>
    </item>
    <item>
      <title>Spectral Superposition: A Theory of Feature Geometry</title>
      <link>https://arxiv.org/abs/2602.02224v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Georgi Ivanov, Narmeen Oozeer, Shivam Raval, Tasana Pejovic, Shriyash Upadhyay, et al.</p><p>Neural networks represent more features than they have dimensions via superposition, forcing features to share representational space. Current methods decompose activations into sparse linear features but discard geometric structure. We develop a theory for studying the geometric structre of features by analyzing the spectra (eigenvalues, eigenspaces, etc.) of weight derived matrices. In particular, we introduce the frame operator $F = WW^\top$, which gives us a spectral measure that describes h</p><p><strong>Tags:</strong> superposition, features, theory</p>]]></description>
      <pubDate>Mon, 02 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.02224v1</guid>
    </item>
    <item>
      <title>From Latent Signals to Reflection Behavior: Tracing Meta-Cognitive Activation Trajectory in R1-Style LLMs</title>
      <link>https://arxiv.org/abs/2602.01999v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Yanrui Du, Yibo Gao, Sendong Zhao, Jiayun Li, Haochun Wang, et al.</p><p>R1-style LLMs have attracted growing attention for their capacity for self-reflection, yet the internal mechanisms underlying such behavior remain unclear. To bridge this gap, we anchor on the onset of reflection behavior and trace its layer-wise activation trajectory. Using the logit lens to read out token-level semantics, we uncover a structured progression: (i) Latent-control layers, where an approximate linear direction encodes the semantics of thinking budget; (ii) Semantic-pivot layers, wh</p>]]></description>
      <pubDate>Mon, 02 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.01999v1</guid>
    </item>
    <item>
      <title>Preserving Localized Patch Semantics in VLMs</title>
      <link>https://arxiv.org/abs/2602.01530v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Parsa Esmaeilkhani, Longin Jan Latecki</p><p>Logit Lens has been proposed for visualizing tokens that contribute most to LLM answers. Recently, Logit Lens was also shown to be applicable in autoregressive Vision-Language Models (VLMs), where it illustrates the conceptual content of image tokens in the form of heatmaps, e.g., which image tokens are likely to depict the concept of cat in a given image. However, the visual content of image tokens often gets diffused to language tokens, and consequently, the locality of visual information gets</p><p><strong>Tags:</strong> vision</p>]]></description>
      <pubDate>Mon, 02 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.01530v1</guid>
    </item>
    <item>
      <title>Beyond Dense States: Elevating Sparse Transcoders to Active Operators for Latent Reasoning</title>
      <link>https://arxiv.org/abs/2602.01695v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Yadong Wang, Haodong Chen, Yu Tian, Chuanxing Geng, Dong Liang, et al.</p><p>Latent reasoning compresses the chain-of-thought (CoT) into continuous hidden states, yet existing methods rely on dense latent transitions that remain difficult to interpret and control. Meanwhile, sparse representation models uncover human-interpretable semantic features but remain largely confined to post-hoc analysis. We reconcile this tension by proposing LSTR (Latent Sparse Transcoder Reasoning), a latent reasoning framework that elevates functional sparse transcoders into active reasoning</p><p><strong>Tags:</strong> features, reasoning</p>]]></description>
      <pubDate>Mon, 02 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.01695v1</guid>
    </item>
    <item>
      <title>Research on the instability mechanism of transformer winding short-circuit faults based on magneto-mechanical coupling analysis</title>
      <link>https://www.semanticscholar.org/paper/48681d875dd4a7fd38cd0b9f05262cccabefe846</link>
      <description><![CDATA[<p><strong>Authors:</strong> Zhihao Gao, Jing Zhou, Li Zhu, Yuefeng Hao, Zhanlong Zhang, et al.</p><p><strong>Tags:</strong> circuits</p>]]></description>
      <pubDate>Sun, 01 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://www.semanticscholar.org/paper/48681d875dd4a7fd38cd0b9f05262cccabefe846</guid>
    </item>
    <item>
      <title>Mechanistic Interpretability of Brain-to-Speech Models Across Speech Modes</title>
      <link>https://arxiv.org/abs/2602.01247v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Maryam Maghsoudi, Ayushi Mishra</p><p>Brain-to-speech decoding models demonstrate robust performance in vocalized, mimed, and imagined speech; yet, the fundamental mechanisms via which these models capture and transmit information across different speech modalities are less explored. In this work, we use mechanistic interpretability to causally investigate the internal representations of a neural speech decoder. We perform cross-mode activation patching of internal activations across speech modes, and use tri-modal interpolation to </p>]]></description>
      <pubDate>Sun, 01 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.01247v1</guid>
    </item>
    <item>
      <title>PolySAE: Modeling Feature Interactions in Sparse Autoencoders via Polynomial Decoding</title>
      <link>https://arxiv.org/abs/2602.01322v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Panagiotis Koromilas, Andreas D. Demou, James Oldfield, Yannis Panagakis, Mihalis Nicolaou</p><p>Sparse autoencoders (SAEs) have emerged as a promising method for interpreting neural network representations by decomposing activations into sparse combinations of dictionary atoms. However, SAEs assume that features combine additively through linear reconstruction, an assumption that cannot capture compositional structure: linear models cannot distinguish whether &quot;Starbucks&quot; arises from the composition of &quot;star&quot; and &quot;coffee&quot; features or merely their co-occurrence. This forces SAEs to allocate </p><p><strong>Tags:</strong> SAE, features</p>]]></description>
      <pubDate>Sun, 01 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.01322v1</guid>
    </item>
    <item>
      <title>Neural FOXP2 -- Language Specific Neuron Steering for Targeted Language Improvement in LLMs</title>
      <link>https://arxiv.org/abs/2602.00945v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Anusa Saha, Tanmay Joshi, Vinija Jain, Aman Chadha, Amitava Das</p><p>LLMs are multilingual by training, yet their lingua franca is often English, reflecting English language dominance in pretraining. Other languages remain in parametric memory but are systematically suppressed. We argue that language defaultness is governed by a sparse, low-rank control circuit, language neurons, that can be mechanistically isolated and safely steered.   We introduce Neural FOXP2, that makes a chosen language (Hindi or Spanish) primary in a model by steering language-specific neu</p><p><strong>Tags:</strong> circuits, steering</p>]]></description>
      <pubDate>Sun, 01 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.00945v1</guid>
    </item>
    <item>
      <title>Supervised sparse auto-encoders as unconstrained feature models for semantic composition</title>
      <link>https://arxiv.org/abs/2602.00924v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Ouns El Harzli, Hugo Wallner, Yoonsoo Nam, Haixuan Xavier Tao</p><p>Sparse auto-encoders (SAEs) have re-emerged as a prominent method for mechanistic interpretability, yet they face two significant challenges: the non-smoothness of the $L_1$ penalty, which hinders reconstruction and scalability, and a lack of alignment between learned features and human semantics. In this paper, we address these limitations by adapting unconstrained feature models-a mathematical framework from neural collapse theory-and by supervising the task. We supervise (decoder-only) SAEs t</p><p><strong>Tags:</strong> SAE, features, safety, theory</p>]]></description>
      <pubDate>Sat, 31 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.00924v1</guid>
    </item>
    <item>
      <title>Towards Interpretable Hallucination Analysis and Mitigation in LVLMs via Contrastive Neuron Steering</title>
      <link>https://arxiv.org/abs/2602.00621v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Guangtao Lyu, Xinyi Cheng, Qi Liu, Chenghao Xu, Jiexi Yan, et al.</p><p>LVLMs achieve remarkable multimodal understanding and generation but remain susceptible to hallucinations. Existing mitigation methods predominantly focus on output-level adjustments, leaving the internal mechanisms that give rise to these hallucinations largely unexplored. To gain a deeper understanding, we adopt a representation-level perspective by introducing sparse autoencoders (SAEs) to decompose dense visual embeddings into sparse, interpretable neurons. Through neuron-level analysis, we </p><p><strong>Tags:</strong> SAE, steering, vision</p>]]></description>
      <pubDate>Sat, 31 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.00621v1</guid>
    </item>
    <item>
      <title>Do Latent-CoT Models Think Step-by-Step? A Mechanistic Study on Sequential Reasoning Tasks</title>
      <link>https://arxiv.org/abs/2602.00449v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Jia Liang, Liangming Pan</p><p>Latent Chain-of-Thought (Latent-CoT) aims to enable step-by-step computation without emitting long rationales, yet its mechanisms remain unclear. We study CODI, a continuous-thought teacher-student distillation model, on strictly sequential polynomial-iteration tasks. Using logit-lens decoding, linear probes, attention analysis, and activation patching, we localize intermediate-state representations and trace their routing to the final readout. On two- and three-hop tasks, CODI forms the full se</p><p><strong>Tags:</strong> probing, reasoning</p>]]></description>
      <pubDate>Sat, 31 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.00449v1</guid>
    </item>
    <item>
      <title>Safety-Efficacy Trade Off: Robustness against Data-Poisoning</title>
      <link>https://arxiv.org/abs/2602.00822v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Diego Granziol</p><p>Backdoor and data poisoning attacks can achieve high attack success while evading existing spectral and optimisation based defences. We show that this behaviour is not incidental, but arises from a fundamental geometric mechanism in input space. Using kernel ridge regression as an exact model of wide neural networks, we prove that clustered dirty label poisons induce a rank one spike in the input Hessian whose magnitude scales quadratically with attack efficacy. Crucially, for nonlinear kernels </p><p><strong>Tags:</strong> safety</p>]]></description>
      <pubDate>Sat, 31 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.00822v1</guid>
    </item>
    <item>
      <title>Sparse or Dense? A Mechanistic Estimation of Computation Density in Transformer-based LLMs</title>
      <link>https://arxiv.org/abs/2601.22795v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Corentin Kervadec, Iuliia Lysova, Marco Baroni, Gemma Boleda</p><p>Transformer-based large language models (LLMs) are comprised of billions of parameters arranged in deep and wide computational graphs. Several studies on LLM efficiency optimization argue that it is possible to prune a significant portion of the parameters, while only marginally impacting performance. This suggests that the computation is not uniformly distributed across the parameters. We introduce here a technique to systematically quantify computation density in LLMs. In particular, we design</p><p><strong>Tags:</strong> circuits</p>]]></description>
      <pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.22795v1</guid>
    </item>
    <item>
      <title>Language Model Circuits Are Sparse in the Neuron Basis</title>
      <link>https://arxiv.org/abs/2601.22594v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Aryaman Arora, Zhengxuan Wu, Jacob Steinhardt, Sarah Schwettmann</p><p>The high-level concepts that a neural network uses to perform computation need not be aligned to individual neurons (Smolensky, 1986). Language model interpretability research has thus turned to techniques such as \textit{sparse autoencoders} (SAEs) to decompose the neuron basis into more interpretable units of model computation, for tasks such as \textit{circuit tracing}. However, not all neuron-based representations are uninterpretable. For the first time, we empirically show that \textbf{MLP </p><p><strong>Tags:</strong> SAE, circuits</p>]]></description>
      <pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.22594v1</guid>
    </item>
    <item>
      <title>Beyond Activation Patterns: A Weight-Based Out-of-Context Explanation of Sparse Autoencoder Features</title>
      <link>https://arxiv.org/abs/2601.22447v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Yiting Liu, Zhi-Hong Deng</p><p>Sparse autoencoders (SAEs) have emerged as a powerful technique for decomposing language model representations into interpretable features. Current interpretation methods infer feature semantics from activation patterns, but overlook that features are trained to reconstruct activations that serve computational roles in the forward pass. We introduce a novel weight-based interpretation framework that measures functional effects through direct weight interactions, requiring no activation data. Thr</p><p><strong>Tags:</strong> SAE, features</p>]]></description>
      <pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.22447v1</guid>
    </item>
  </channel>
</rss>
