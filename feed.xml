<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Mechanistic Interpretability Hub</title>
    <link>https://izkula.github.io/cc</link>
    <description>Latest research in mechanistic interpretability - understanding how neural networks work internally</description>
    <language>en-us</language>
    <lastBuildDate>Mon, 16 Feb 2026 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://izkula.github.io/cc/feed.xml" rel="self" type="application/rss+xml"/>

    <item>
      <title>Protein Circuit Tracing via Cross-layer Transcoders</title>
      <link>https://arxiv.org/abs/2602.12026v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Darin Tsui, Kunal Talreja, Daniel Saeedi, Amirali Aghazadeh</p><p>Protein language models (pLMs) have emerged as powerful predictors of protein structure and function. However, the computational circuits underlying their predictions remain poorly understood. Recent mechanistic interpretability methods decompose pLM representations into interpretable features, but they treat each layer independently and thus fail to capture cross-layer computation, limiting their ability to approximate the full model. We introduce ProtoMech, a framework for discovering computat</p><p><strong>Tags:</strong> circuits, features, biology</p>]]></description>
      <pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.12026v1</guid>
    </item>
    <item>
      <title>From Atoms to Trees: Building a Structured Feature Forest with Hierarchical Sparse Autoencoders</title>
      <link>https://arxiv.org/abs/2602.11881v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Yifan Luo, Yang Zhan, Jiedong Jiang, Tianyang Liu, Mingrui Wu, et al.</p><p>Sparse autoencoders (SAEs) have proven effective for extracting monosemantic features from large language models (LLMs), yet these features are typically identified in isolation. However, broad evidence suggests that LLMs capture the intrinsic structure of natural language, where the phenomenon of &quot;feature splitting&quot; in particular indicates that such structure is hierarchical. To capture this, we propose the Hierarchical Sparse Autoencoder (HSAE), which jointly learns a series of SAEs and the pa</p><p><strong>Tags:</strong> SAE, features</p>]]></description>
      <pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.11881v1</guid>
    </item>
    <item>
      <title>EM-Aware Physical Synthesis: Neural Inductor Modeling and Intelligent Placement &amp; Routing for RF Circuits</title>
      <link>https://arxiv.org/abs/2602.11461v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Yilun Huang, Asal Mehradfar, Salman Avestimehr, Hamidreza Aghasi</p><p>This paper presents an ML-driven framework for automated RF physical synthesis that transforms circuit netlists into manufacturable GDSII layouts. While recent ML approaches demonstrate success in topology selection and parameter optimization, they fail to produce manufacturable layouts due to oversimplified component models and lack of routing capabilities. Our framework addresses these limitations through three key innovations: (1) a neural network framework trained on 18,210 inductor geometri</p><p><strong>Tags:</strong> circuits</p>]]></description>
      <pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.11461v1</guid>
    </item>
    <item>
      <title>TADA! Tuning Audio Diffusion Models through Activation Steering</title>
      <link>https://arxiv.org/abs/2602.11910v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Łukasz Staniszewski, Katarzyna Zaleska, Mateusz Modrzejewski, Kamil Deja</p><p>Audio diffusion models can synthesize high-fidelity music from text, yet their internal mechanisms for representing high-level concepts remain poorly understood. In this work, we use activation patching to demonstrate that distinct semantic musical concepts, such as the presence of specific instruments, vocals, or genre characteristics, are controlled by a small, shared subset of attention layers in state-of-the-art audio diffusion architectures. Next, we demonstrate that applying Contrastive Ac</p><p><strong>Tags:</strong> steering</p>]]></description>
      <pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.11910v1</guid>
    </item>
    <item>
      <title>SafeNeuron: Neuron-Level Safety Alignment for Large Language Models</title>
      <link>https://arxiv.org/abs/2602.12158v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Zhaoxin Wang, Jiaming Liang, Fengbin Zhu, Weixiang Zhao, Junfeng Fang, et al.</p><p>Large language models (LLMs) and multimodal LLMs are typically safety-aligned before release to prevent harmful content generation. However, recent studies show that safety behaviors are concentrated in a small subset of parameters, making alignment brittle and easily bypassed through neuron-level attacks. Moreover, most existing alignment methods operate at the behavioral level, offering limited control over the model&#x27;s internal safety mechanisms. In this work, we propose SafeNeuron, a neuron-l</p><p><strong>Tags:</strong> safety, vision</p>]]></description>
      <pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.12158v1</guid>
    </item>
    <item>
      <title>Cross-Architecture Model Diffing with Crosscoders: Unsupervised Discovery of Differences Between LLMs</title>
      <link>https://arxiv.org/abs/2602.11729v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Thomas Jiralerspong, Trenton Bricken</p><p>Model diffing, the process of comparing models&#x27; internal representations to identify their differences, is a promising approach for uncovering safety-critical behaviors in new models. However, its application has so far been primarily focused on comparing a base model with its finetune. Since new LLM releases are often novel architectures, cross-architecture methods are essential to make model diffing widely applicable. Crosscoders are one solution capable of cross-architecture model diffing but</p><p><strong>Tags:</strong> safety</p>]]></description>
      <pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.11729v1</guid>
    </item>
    <item>
      <title>Sparse Autoencoders are Capable LLM Jailbreak Mitigators</title>
      <link>https://arxiv.org/abs/2602.12418v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Yannick Assogba, Jacopo Cortellazzi, Javier Abad, Pau Rodriguez, Xavier Suau, et al.</p><p>Jailbreak attacks remain a persistent threat to large language model safety. We propose Context-Conditioned Delta Steering (CC-Delta), an SAE-based defense that identifies jailbreak-relevant sparse features by comparing token-level representations of the same harmful request with and without jailbreak context. Using paired harmful/jailbreak prompts, CC-Delta selects features via statistical testing and applies inference-time mean-shift steering in SAE latent space. Across four aligned instructio</p><p><strong>Tags:</strong> SAE, features, safety, steering</p>]]></description>
      <pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.12418v1</guid>
    </item>
    <item>
      <title>MonoLoss: A Training Objective for Interpretable Monosemantic Representations</title>
      <link>https://arxiv.org/abs/2602.12403v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Ali Nasiri-Sarvi, Anh Tien Nguyen, Hassan Rivaz, Dimitris Samaras, Mahdi S. Hosseini</p><p>Sparse autoencoders (SAEs) decompose polysemantic neural representations, where neurons respond to multiple unrelated concepts, into monosemantic features that capture single, interpretable concepts. However, standard training objectives only weakly encourage this decomposition, and existing monosemanticity metrics require pairwise comparisons across all dataset samples, making them inefficient during training and evaluation. We study a recent MonoScore metric and derive a single-pass algorithm </p><p><strong>Tags:</strong> SAE, features</p>]]></description>
      <pubDate>Thu, 12 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.12403v1</guid>
    </item>
    <item>
      <title>From Circuits to Dynamics: Understanding and Stabilizing Failure in 3D Diffusion Transformers</title>
      <link>https://arxiv.org/abs/2602.11130v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Maximilian Plattner, Fabian Paischer, Johannes Brandstetter, Arturs Berzins</p><p>Reliable surface completion from sparse point clouds underpins many applications spanning content creation and robotics. While 3D diffusion transformers attain state-of-the-art results on this task, we uncover that they exhibit a catastrophic mode of failure: arbitrarily small on-surface perturbations to the input point cloud can fracture the output into multiple disconnected pieces -- a phenomenon we call Meltdown. Using activation-patching from mechanistic interpretability, we localize Meltdow</p><p><strong>Tags:</strong> circuits</p>]]></description>
      <pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.11130v1</guid>
    </item>
    <item>
      <title>Control Reinforcement Learning: Token-Level Mechanistic Analysis via Learned SAE Feature Steering</title>
      <link>https://arxiv.org/abs/2602.10437v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Seonglae Cho, Zekun Wu, Adriano Koshiyama</p><p>Sparse autoencoders (SAEs) decompose language model activations into interpretable features, but existing methods reveal only which features activate, not which change model outputs when amplified. We introduce Control Reinforcement Learning (CRL), which trains a policy to select SAE features for steering at each token, producing interpretable intervention logs: the learned policy identifies features that change model outputs when amplified. Adaptive Feature Masking encourages diverse feature di</p><p><strong>Tags:</strong> SAE, features, steering</p>]]></description>
      <pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.10437v1</guid>
    </item>
    <item>
      <title>Less is Enough: Synthesizing Diverse Data in Feature Space of LLMs</title>
      <link>https://arxiv.org/abs/2602.10388v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Zhongzhi Li, Xuansheng Wu, Yijiang Li, Lijie Hu, Ninghao Liu</p><p>The diversity of post-training data is critical for effective downstream performance in large language models (LLMs). Many existing approaches to constructing post-training data quantify diversity using text-based metrics that capture linguistic variation, but such metrics provide only weak signals for the task-relevant features that determine downstream performance. In this work, we introduce Feature Activation Coverage (FAC) which measures data diversity in an interpretable feature space. Buil</p><p><strong>Tags:</strong> features</p>]]></description>
      <pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.10388v1</guid>
    </item>
    <item>
      <title>Med-SegLens: Latent-Level Model Diffing for Interpretable Medical Image Segmentation</title>
      <link>https://arxiv.org/abs/2602.10508v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Salma J. Ahmed, Emad A. Mohammed, Azam Asilian Bidgoli</p><p>Modern segmentation models achieve strong predictive performance but remain largely opaque, limiting our ability to diagnose failures, understand dataset shift, or intervene in a principled manner. We introduce Med-SegLens, a model-diffing framework that decomposes segmentation model activations into interpretable latent features using sparse autoencoders trained on SegFormer and U-Net. Through cross-architecture and cross-dataset latent alignment across healthy, adult, pediatric, and sub-Sahara</p><p><strong>Tags:</strong> SAE, features, safety, vision</p>]]></description>
      <pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.10508v1</guid>
    </item>
    <item>
      <title>Triggers Hijack Language Circuits: A Mechanistic Analysis of Backdoor Behaviors in Large Language Models</title>
      <link>https://arxiv.org/abs/2602.10382v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Théo Lasnier, Wissam Antoun, Francis Kulumba, Djamé Seddah</p><p>Backdoor attacks pose significant security risks for Large Language Models (LLMs), yet the internal mechanisms by which triggers operate remain poorly understood. We present the first mechanistic analysis of language-switching backdoors, studying the GAPperon model family (1B, 8B, 24B parameters) which contains triggers injected during pretraining that cause output language switching. Using activation patching, we localize trigger formation to early layers (7.5-25% of model depth) and identify w</p><p><strong>Tags:</strong> circuits</p>]]></description>
      <pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.10382v1</guid>
    </item>
    <item>
      <title>Control Reinforcement Learning: Interpretable Token-Level Steering of LLMs via Sparse Autoencoder Features</title>
      <link>https://arxiv.org/abs/2602.10437v2</link>
      <description><![CDATA[<p><strong>Authors:</strong> Seonglae Cho, Zekun Wu, Adriano Koshiyama</p><p>Sparse autoencoders (SAEs) decompose language model activations into interpretable features, but existing methods reveal only which features activate, not which change model outputs when amplified. We introduce Control Reinforcement Learning (CRL), which trains a policy to select SAE features for steering at each token, producing interpretable intervention logs: the learned policy identifies features that change model outputs when amplified. Adaptive Feature Masking encourages diverse feature di</p><p><strong>Tags:</strong> SAE, features, steering</p>]]></description>
      <pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.10437v2</guid>
    </item>
    <item>
      <title>Simple LLM Baselines are Competitive for Model Diffing</title>
      <link>https://arxiv.org/abs/2602.10371v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Elias Kempf, Simon Schrodi, Bartosz Cywiński, Thomas Brox, Neel Nanda, et al.</p><p>Standard LLM evaluations only test capabilities or dispositions that evaluators designed them for, missing unexpected differences such as behavioral shifts between model revisions or emergent misaligned tendencies. Model diffing addresses this limitation by automatically surfacing systematic behavioral differences. Recent approaches include LLM-based methods that generate natural language descriptions and sparse autoencoder (SAE)-based methods that identify interpretable features. However, no sy</p><p><strong>Tags:</strong> SAE, features, vision</p>]]></description>
      <pubDate>Tue, 10 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.10371v1</guid>
    </item>
    <item>
      <title>Learning Self-Interpretation from Interpretability Artifacts: Training Lightweight Adapters on Vector-Label Pairs</title>
      <link>https://arxiv.org/abs/2602.10352v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Keenan Pepper, Alex McKenzie, Florin Pop, Stijn Servaes, Martin Leitgab, et al.</p><p>Self-interpretation methods prompt language models to describe their own internal states, but remain unreliable due to hyperparameter sensitivity. We show that training lightweight adapters on interpretability artifacts, while keeping the LM entirely frozen, yields reliable self-interpretation across tasks and model families. A scalar affine adapter with just $d_\text{model}+1$ parameters suffices: trained adapters generate sparse autoencoder feature labels that outperform the training labels th</p><p><strong>Tags:</strong> SAE, features</p>]]></description>
      <pubDate>Tue, 10 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.10352v1</guid>
    </item>
    <item>
      <title>Why Linear Interpretability Works: Invariant Subspaces as a Result of Architectural Constraints</title>
      <link>https://arxiv.org/abs/2602.09783v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Andres Saurez, Yousung Lee, Dongsoo Har</p><p>Linear probes and sparse autoencoders consistently recover meaningful structure from transformer representations -- yet why should such simple methods succeed in deep, nonlinear systems? We show this is not merely an empirical regularity but a consequence of architectural necessity: transformers communicate information through linear interfaces (attention OV circuits, unembedding matrices), and any semantic feature decoded through such an interface must occupy a context-invariant linear subspace</p><p><strong>Tags:</strong> SAE, circuits, features, probing</p>]]></description>
      <pubDate>Tue, 10 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.09783v1</guid>
    </item>
    <item>
      <title>Circuit Fingerprints: How Answer Tokens Encode Their Geometrical Path</title>
      <link>https://arxiv.org/abs/2602.09784v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Andres Saurez, Neha Sengar, Dongsoo Har</p><p>Circuit discovery and activation steering in transformers have developed as separate research threads, yet both operate on the same representational space. Are they two views of the same underlying structure? We show they follow a single geometric principle: answer tokens, processed in isolation, encode the directions that would produce them. This Circuit Fingerprint hypothesis enables circuit discovery without gradients or causal intervention -- recovering comparable structure to gradient-based</p><p><strong>Tags:</strong> circuits, steering</p>]]></description>
      <pubDate>Tue, 10 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.09784v1</guid>
    </item>
    <item>
      <title>How Do Language Models Understand Tables? A Mechanistic Analysis of Cell Location</title>
      <link>https://arxiv.org/abs/2602.08548v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Xuanliang Zhang, Dingzirui Wang, Keyan Xu, Qingfu Zhu, Wanxiang Che</p><p>While Large Language Models (LLMs) are increasingly deployed for table-related tasks, the internal mechanisms enabling them to process linearized two-dimensional structured tables remain opaque. In this work, we investigate the process of table understanding by dissecting the atomic task of cell location. Through activation patching and complementary interpretability techniques, we delineate the table understanding mechanism into a sequential three-stage pipeline: Semantic Binding, Coordinate Lo</p>]]></description>
      <pubDate>Mon, 09 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.08548v1</guid>
    </item>
    <item>
      <title>Patches of Nonlinearity: Instruction Vectors in Large Language Models</title>
      <link>https://arxiv.org/abs/2602.07930v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Irina Bigoulaeva, Jonas Rohweder, Subhabrata Dutta, Iryna Gurevych</p><p>Despite the recent success of instruction-tuned language models and their ubiquitous usage, very little is known of how models process instructions internally. In this work, we address this gap from a mechanistic point of view by investigating how instruction-specific representations are constructed and utilized in different stages of post-training: Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO). Via causal mediation, we identify that instruction representation is fairly l</p>]]></description>
      <pubDate>Sun, 08 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.07930v1</guid>
    </item>
    <item>
      <title>LUCID-SAE: Learning Unified Vision-Language Sparse Codes for Interpretable Concept Discovery</title>
      <link>https://arxiv.org/abs/2602.07311v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Difei Gu, Yunhe Gao, Gerasimos Chatzoudis, Zihan Dong, Guoning Zhang, et al.</p><p>Sparse autoencoders (SAEs) offer a natural path toward comparable explanations across different representation spaces. However, current SAEs are trained per modality, producing dictionaries whose features are not directly understandable and whose explanations do not transfer across domains. In this study, we introduce LUCID (Learning Unified vision-language sparse Codes for Interpretable concept Discovery), a unified vision-language sparse autoencoder that learns a shared latent dictionary for i</p><p><strong>Tags:</strong> SAE, features, vision</p>]]></description>
      <pubDate>Sat, 07 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.07311v1</guid>
    </item>
    <item>
      <title>The Quantum Sieve Tracer: A Hybrid Framework for Layer-Wise Activation Tracing in Large Language Models</title>
      <link>https://arxiv.org/abs/2602.06852v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Jonathan Pan</p><p>Mechanistic interpretability aims to reverse-engineer the internal computations of Large Language Models (LLMs), yet separating sparse semantic signals from high-dimensional polysemantic noise remains a significant challenge. This paper introduces the Quantum Sieve Tracer, a hybrid quantum-classical framework designed to characterize factual recall circuits. We implement a modular pipeline that first localizes critical layers using classical causal tracing, then maps specific attention head acti</p><p><strong>Tags:</strong> circuits, features, attention</p>]]></description>
      <pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.06852v1</guid>
    </item>
    <item>
      <title>Learning a Generative Meta-Model of LLM Activations</title>
      <link>https://arxiv.org/abs/2602.06964v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Grace Luo, Jiahai Feng, Trevor Darrell, Alec Radford, Jacob Steinhardt</p><p>Existing approaches for analyzing neural network activations, such as PCA and sparse autoencoders, rely on strong structural assumptions. Generative models offer an alternative: they can uncover structure without such assumptions and act as priors that improve intervention fidelity. We explore this direction by training diffusion models on one billion residual stream activations, creating &quot;meta-models&quot; that learn the distribution of a network&#x27;s internal states. We find that diffusion loss decrea</p><p><strong>Tags:</strong> SAE</p>]]></description>
      <pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.06964v1</guid>
    </item>
    <item>
      <title>Endogenous Resistance to Activation Steering in Language Models</title>
      <link>https://arxiv.org/abs/2602.06941v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Alex McKenzie, Keenan Pepper, Stijn Servaes, Martin Leitgab, Murat Cubuktepe, et al.</p><p>Large language models can resist task-misaligned activation steering during inference, sometimes recovering mid-generation to produce improved responses even when steering remains active. We term this Endogenous Steering Resistance (ESR). Using sparse autoencoder (SAE) latents to steer model activations, we find that Llama-3.3-70B shows substantial ESR, while smaller models from the Llama-3 and Gemma-2 families exhibit the phenomenon less frequently. We identify 26 SAE latents that activate diff</p><p><strong>Tags:</strong> SAE, steering</p>]]></description>
      <pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.06941v1</guid>
    </item>
    <item>
      <title>Towards Understanding Multimodal Fine-Tuning: Spatial Features</title>
      <link>https://arxiv.org/abs/2602.08713v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Lachin Naghashyar, Hunar Batra, Ashkan Khakzar, Philip Torr, Ronald Clark, et al.</p><p>Contemporary Vision-Language Models (VLMs) achieve strong performance on a wide range of tasks by pairing a vision encoder with a pre-trained language model, fine-tuned for visual-text inputs. Yet despite these gains, it remains unclear how language backbone representations adapt during multimodal training and when vision-specific capabilities emerge. In this work, we present the first mechanistic analysis of VLM adaptation. Using stage-wise model diffing, a technique that isolates representatio</p><p><strong>Tags:</strong> features, vision</p>]]></description>
      <pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.08713v1</guid>
    </item>
    <item>
      <title>CodeCircuit: Toward Inferring LLM-Generated Code Correctness via Attribution Graphs</title>
      <link>https://arxiv.org/abs/2602.07080v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Yicheng He, Zheng Zhao, Zhou Kaiyu, Bryan Dai, Jie Fu, et al.</p><p>Current paradigms for code verification rely heavily on external mechanisms-such as execution-based unit tests or auxiliary LLM judges-which are often labor-intensive or limited by the judging model&#x27;s own capabilities. This raises a fundamental, yet unexplored question: Can an LLM&#x27;s functional correctness be assessed purely from its internal computational structure? Our primary objective is to investigate whether the model&#x27;s neural dynamics encode internally decodable signals that are predictive</p><p><strong>Tags:</strong> circuits</p>]]></description>
      <pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.07080v1</guid>
    </item>
    <item>
      <title>DLM-Scope: Mechanistic Interpretability of Diffusion Language Models via Sparse Autoencoders</title>
      <link>https://arxiv.org/abs/2602.05859v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Xu Wang, Bingqing Jiang, Yu Wan, Baosong Yang, Lingpeng Kong, et al.</p><p>Sparse autoencoders (SAEs) have become a standard tool for mechanistic interpretability in autoregressive large language models (LLMs), enabling researchers to extract sparse, human-interpretable features and intervene on model behavior. Recently, as diffusion language models (DLMs) have become an increasingly promising alternative to the autoregressive LLMs, it is essential to develop tailored mechanistic interpretability tools for this emerging class of models. In this work, we present DLM-Sco</p><p><strong>Tags:</strong> SAE, features</p>]]></description>
      <pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.05859v1</guid>
    </item>
    <item>
      <title>Split Personality Training: Revealing Latent Knowledge Through Alternate Personalities</title>
      <link>https://arxiv.org/abs/2602.05532v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Florian Dietz, William Wale, Oscar Gilg, Robert McCarthy, Felix Michalak, et al.</p><p>Detecting misalignment in large language models is challenging because models may learn to conceal misbehavior during training. Standard auditing techniques fall short: black-box methods often cannot distinguish misaligned outputs from benign ones, and mechanistic interpretability does not scale with model capabilities. We introduce Split Personality Training (SPT), which fine-tunes a second ``honest persona&#x27;&#x27; into LoRA parameters that remain inactive during normal operation. After the main mode</p><p><strong>Tags:</strong> safety</p>]]></description>
      <pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.05532v1</guid>
    </item>
    <item>
      <title>Causal Front-Door Adjustment for Robust Jailbreak Attacks on LLMs</title>
      <link>https://arxiv.org/abs/2602.05444v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Yao Zhou, Zeen Song, Wenwen Qiang, Fengge Wu, Shuyi Zhou, et al.</p><p>Safety alignment mechanisms in Large Language Models (LLMs) often operate as latent internal states, obscuring the model&#x27;s inherent capabilities. Building on this observation, we model the safety mechanism as an unobserved confounder from a causal perspective. Then, we propose the \textbf{C}ausal \textbf{F}ront-Door \textbf{A}djustment \textbf{A}ttack ({\textbf{CFA}}$^2$) to jailbreak LLM, which is a framework that leverages Pearl&#x27;s Front-Door Criterion to sever the confounding associations for </p><p><strong>Tags:</strong> safety</p>]]></description>
      <pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.05444v1</guid>
    </item>
    <item>
      <title>Advancing Opinion Dynamics Modeling with Neural Diffusion-Convection-Reaction Equation</title>
      <link>https://arxiv.org/abs/2602.05403v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Chenghua Gong, Yihang Jiang, Hao Li, Rui Sun, Juyuan Zhang, et al.</p><p>Advanced opinion dynamics modeling is vital for deciphering social behavior, emphasizing its role in mitigating polarization and securing cyberspace. To synergize mechanistic interpretability with data-driven flexibility, recent studies have explored the integration of Physics-Informed Neural Networks (PINNs) for opinion modeling. Despite this promise, existing methods are tailored to incomplete priors, lacking a comprehensive physical system to integrate dynamics from local, global, and endogen</p>]]></description>
      <pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.05403v1</guid>
    </item>
  </channel>
</rss>
