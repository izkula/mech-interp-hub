<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Mechanistic Interpretability Hub</title>
    <link>https://izkula.github.io/cc</link>
    <description>Latest research in mechanistic interpretability - understanding how neural networks work internally</description>
    <language>en-us</language>
    <lastBuildDate>Wed, 04 Feb 2026 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://izkula.github.io/cc/feed.xml" rel="self" type="application/rss+xml"/>

    <item>
      <title>Explaining the Explainer: Understanding the Inner Workings of Transformer-based Symbolic Regression Models</title>
      <link>https://arxiv.org/abs/2602.03506v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Arco van Breda, Erman Acar</p><p>Following their success across many domains, transformers have also proven effective for symbolic regression (SR); however, the internal mechanisms underlying their generation of mathematical operators remain largely unexplored. Although mechanistic interpretability has successfully identified circuits in language and vision models, it has not yet been applied to SR. In this article, we introduce PATCHES, an evolutionary circuit discovery algorithm that identifies compact and correct circuits fo</p><p><strong>Tags:</strong> circuits, vision</p>]]></description>
      <pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.03506v1</guid>
    </item>
    <item>
      <title>CSR-Bench: A Benchmark for Evaluating the Cross-modal Safety and Reliability of MLLMs</title>
      <link>https://arxiv.org/abs/2602.03263v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Yuxuan Liu, Yuntian Shi, Kun Wang, Haoting Shen, Kun Yang</p><p>Multimodal large language models (MLLMs) enable interaction over both text and images, but their safety behavior can be driven by unimodal shortcuts instead of true joint intent understanding. We introduce CSR-Bench, a benchmark for evaluating cross-modal reliability through four stress-testing interaction patterns spanning Safety, Over-rejection, Bias, and Hallucination, covering 61 fine-grained types. Each instance is constructed to require integrated image-text interpretation, and we addition</p><p><strong>Tags:</strong> safety, vision</p>]]></description>
      <pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.03263v1</guid>
    </item>
    <item>
      <title>Universal Redundancies in Time Series Foundation Models</title>
      <link>https://arxiv.org/abs/2602.01605v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Anthony Bao, Venkata Hasith Vattikuti, Jeffrey Lai, William Gilpin</p><p>Time Series Foundation Models (TSFMs) leverage extensive pretraining to accurately predict unseen time series during inference, without the need for task-specific fine-tuning. Through large-scale evaluations on standard benchmarks, we find that leading transformer-based TSFMs exhibit redundant components in their intermediate layers. We introduce a set of tools for mechanistic interpretability of TSFMs, including ablations of specific components and direct logit attribution on the residual strea</p>]]></description>
      <pubDate>Mon, 02 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.01605v1</guid>
    </item>
    <item>
      <title>Mechanistic Indicators of Steering Effectiveness in Large Language Models</title>
      <link>https://arxiv.org/abs/2602.01716v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Mehdi Jafari, Hao Xue, Flora Salim</p><p>Activation-based steering enables Large Language Models (LLMs) to exhibit targeted behaviors by intervening on intermediate activations without retraining. Despite its widespread use, the mechanistic factors that govern when steering succeeds or fails remain poorly understood, as prior work has relied primarily on black-box outputs or LLM-based judges. In this study, we investigate whether the reliability of steering can be diagnosed using internal model signals. We focus on two information-theo</p><p><strong>Tags:</strong> steering</p>]]></description>
      <pubDate>Mon, 02 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.01716v1</guid>
    </item>
    <item>
      <title>Spectral Superposition: A Theory of Feature Geometry</title>
      <link>https://arxiv.org/abs/2602.02224v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Georgi Ivanov, Narmeen Oozeer, Shivam Raval, Tasana Pejovic, Shriyash Upadhyay, et al.</p><p>Neural networks represent more features than they have dimensions via superposition, forcing features to share representational space. Current methods decompose activations into sparse linear features but discard geometric structure. We develop a theory for studying the geometric structre of features by analyzing the spectra (eigenvalues, eigenspaces, etc.) of weight derived matrices. In particular, we introduce the frame operator $F = WW^\top$, which gives us a spectral measure that describes h</p><p><strong>Tags:</strong> superposition, features, theory</p>]]></description>
      <pubDate>Mon, 02 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.02224v1</guid>
    </item>
    <item>
      <title>From Latent Signals to Reflection Behavior: Tracing Meta-Cognitive Activation Trajectory in R1-Style LLMs</title>
      <link>https://arxiv.org/abs/2602.01999v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Yanrui Du, Yibo Gao, Sendong Zhao, Jiayun Li, Haochun Wang, et al.</p><p>R1-style LLMs have attracted growing attention for their capacity for self-reflection, yet the internal mechanisms underlying such behavior remain unclear. To bridge this gap, we anchor on the onset of reflection behavior and trace its layer-wise activation trajectory. Using the logit lens to read out token-level semantics, we uncover a structured progression: (i) Latent-control layers, where an approximate linear direction encodes the semantics of thinking budget; (ii) Semantic-pivot layers, wh</p>]]></description>
      <pubDate>Mon, 02 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.01999v1</guid>
    </item>
    <item>
      <title>Preserving Localized Patch Semantics in VLMs</title>
      <link>https://arxiv.org/abs/2602.01530v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Parsa Esmaeilkhani, Longin Jan Latecki</p><p>Logit Lens has been proposed for visualizing tokens that contribute most to LLM answers. Recently, Logit Lens was also shown to be applicable in autoregressive Vision-Language Models (VLMs), where it illustrates the conceptual content of image tokens in the form of heatmaps, e.g., which image tokens are likely to depict the concept of cat in a given image. However, the visual content of image tokens often gets diffused to language tokens, and consequently, the locality of visual information gets</p><p><strong>Tags:</strong> vision</p>]]></description>
      <pubDate>Mon, 02 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.01530v1</guid>
    </item>
    <item>
      <title>Beyond Dense States: Elevating Sparse Transcoders to Active Operators for Latent Reasoning</title>
      <link>https://arxiv.org/abs/2602.01695v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Yadong Wang, Haodong Chen, Yu Tian, Chuanxing Geng, Dong Liang, et al.</p><p>Latent reasoning compresses the chain-of-thought (CoT) into continuous hidden states, yet existing methods rely on dense latent transitions that remain difficult to interpret and control. Meanwhile, sparse representation models uncover human-interpretable semantic features but remain largely confined to post-hoc analysis. We reconcile this tension by proposing LSTR (Latent Sparse Transcoder Reasoning), a latent reasoning framework that elevates functional sparse transcoders into active reasoning</p><p><strong>Tags:</strong> features, reasoning</p>]]></description>
      <pubDate>Mon, 02 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.01695v1</guid>
    </item>
    <item>
      <title>Research on the instability mechanism of transformer winding short-circuit faults based on magneto-mechanical coupling analysis</title>
      <link>https://www.semanticscholar.org/paper/48681d875dd4a7fd38cd0b9f05262cccabefe846</link>
      <description><![CDATA[<p><strong>Authors:</strong> Zhihao Gao, Jing Zhou, Li Zhu, Yuefeng Hao, Zhanlong Zhang, et al.</p><p><strong>Tags:</strong> circuits</p>]]></description>
      <pubDate>Sun, 01 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://www.semanticscholar.org/paper/48681d875dd4a7fd38cd0b9f05262cccabefe846</guid>
    </item>
    <item>
      <title>Mechanistic Interpretability of Brain-to-Speech Models Across Speech Modes</title>
      <link>https://arxiv.org/abs/2602.01247v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Maryam Maghsoudi, Ayushi Mishra</p><p>Brain-to-speech decoding models demonstrate robust performance in vocalized, mimed, and imagined speech; yet, the fundamental mechanisms via which these models capture and transmit information across different speech modalities are less explored. In this work, we use mechanistic interpretability to causally investigate the internal representations of a neural speech decoder. We perform cross-mode activation patching of internal activations across speech modes, and use tri-modal interpolation to </p>]]></description>
      <pubDate>Sun, 01 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.01247v1</guid>
    </item>
    <item>
      <title>PolySAE: Modeling Feature Interactions in Sparse Autoencoders via Polynomial Decoding</title>
      <link>https://arxiv.org/abs/2602.01322v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Panagiotis Koromilas, Andreas D. Demou, James Oldfield, Yannis Panagakis, Mihalis Nicolaou</p><p>Sparse autoencoders (SAEs) have emerged as a promising method for interpreting neural network representations by decomposing activations into sparse combinations of dictionary atoms. However, SAEs assume that features combine additively through linear reconstruction, an assumption that cannot capture compositional structure: linear models cannot distinguish whether &quot;Starbucks&quot; arises from the composition of &quot;star&quot; and &quot;coffee&quot; features or merely their co-occurrence. This forces SAEs to allocate </p><p><strong>Tags:</strong> SAE, features</p>]]></description>
      <pubDate>Sun, 01 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.01322v1</guid>
    </item>
    <item>
      <title>Neural FOXP2 -- Language Specific Neuron Steering for Targeted Language Improvement in LLMs</title>
      <link>https://arxiv.org/abs/2602.00945v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Anusa Saha, Tanmay Joshi, Vinija Jain, Aman Chadha, Amitava Das</p><p>LLMs are multilingual by training, yet their lingua franca is often English, reflecting English language dominance in pretraining. Other languages remain in parametric memory but are systematically suppressed. We argue that language defaultness is governed by a sparse, low-rank control circuit, language neurons, that can be mechanistically isolated and safely steered.   We introduce Neural FOXP2, that makes a chosen language (Hindi or Spanish) primary in a model by steering language-specific neu</p><p><strong>Tags:</strong> circuits, steering</p>]]></description>
      <pubDate>Sun, 01 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.00945v1</guid>
    </item>
    <item>
      <title>Supervised sparse auto-encoders as unconstrained feature models for semantic composition</title>
      <link>https://arxiv.org/abs/2602.00924v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Ouns El Harzli, Hugo Wallner, Yoonsoo Nam, Haixuan Xavier Tao</p><p>Sparse auto-encoders (SAEs) have re-emerged as a prominent method for mechanistic interpretability, yet they face two significant challenges: the non-smoothness of the $L_1$ penalty, which hinders reconstruction and scalability, and a lack of alignment between learned features and human semantics. In this paper, we address these limitations by adapting unconstrained feature models-a mathematical framework from neural collapse theory-and by supervising the task. We supervise (decoder-only) SAEs t</p><p><strong>Tags:</strong> SAE, features, safety, theory</p>]]></description>
      <pubDate>Sat, 31 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.00924v1</guid>
    </item>
    <item>
      <title>Towards Interpretable Hallucination Analysis and Mitigation in LVLMs via Contrastive Neuron Steering</title>
      <link>https://arxiv.org/abs/2602.00621v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Guangtao Lyu, Xinyi Cheng, Qi Liu, Chenghao Xu, Jiexi Yan, et al.</p><p>LVLMs achieve remarkable multimodal understanding and generation but remain susceptible to hallucinations. Existing mitigation methods predominantly focus on output-level adjustments, leaving the internal mechanisms that give rise to these hallucinations largely unexplored. To gain a deeper understanding, we adopt a representation-level perspective by introducing sparse autoencoders (SAEs) to decompose dense visual embeddings into sparse, interpretable neurons. Through neuron-level analysis, we </p><p><strong>Tags:</strong> SAE, steering, vision</p>]]></description>
      <pubDate>Sat, 31 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.00621v1</guid>
    </item>
    <item>
      <title>Do Latent-CoT Models Think Step-by-Step? A Mechanistic Study on Sequential Reasoning Tasks</title>
      <link>https://arxiv.org/abs/2602.00449v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Jia Liang, Liangming Pan</p><p>Latent Chain-of-Thought (Latent-CoT) aims to enable step-by-step computation without emitting long rationales, yet its mechanisms remain unclear. We study CODI, a continuous-thought teacher-student distillation model, on strictly sequential polynomial-iteration tasks. Using logit-lens decoding, linear probes, attention analysis, and activation patching, we localize intermediate-state representations and trace their routing to the final readout. On two- and three-hop tasks, CODI forms the full se</p><p><strong>Tags:</strong> probing, reasoning</p>]]></description>
      <pubDate>Sat, 31 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.00449v1</guid>
    </item>
    <item>
      <title>Safety-Efficacy Trade Off: Robustness against Data-Poisoning</title>
      <link>https://arxiv.org/abs/2602.00822v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Diego Granziol</p><p>Backdoor and data poisoning attacks can achieve high attack success while evading existing spectral and optimisation based defences. We show that this behaviour is not incidental, but arises from a fundamental geometric mechanism in input space. Using kernel ridge regression as an exact model of wide neural networks, we prove that clustered dirty label poisons induce a rank one spike in the input Hessian whose magnitude scales quadratically with attack efficacy. Crucially, for nonlinear kernels </p><p><strong>Tags:</strong> safety</p>]]></description>
      <pubDate>Sat, 31 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.00822v1</guid>
    </item>
    <item>
      <title>Sparse or Dense? A Mechanistic Estimation of Computation Density in Transformer-based LLMs</title>
      <link>https://arxiv.org/abs/2601.22795v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Corentin Kervadec, Iuliia Lysova, Marco Baroni, Gemma Boleda</p><p>Transformer-based large language models (LLMs) are comprised of billions of parameters arranged in deep and wide computational graphs. Several studies on LLM efficiency optimization argue that it is possible to prune a significant portion of the parameters, while only marginally impacting performance. This suggests that the computation is not uniformly distributed across the parameters. We introduce here a technique to systematically quantify computation density in LLMs. In particular, we design</p><p><strong>Tags:</strong> circuits</p>]]></description>
      <pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.22795v1</guid>
    </item>
    <item>
      <title>Language Model Circuits Are Sparse in the Neuron Basis</title>
      <link>https://arxiv.org/abs/2601.22594v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Aryaman Arora, Zhengxuan Wu, Jacob Steinhardt, Sarah Schwettmann</p><p>The high-level concepts that a neural network uses to perform computation need not be aligned to individual neurons (Smolensky, 1986). Language model interpretability research has thus turned to techniques such as \textit{sparse autoencoders} (SAEs) to decompose the neuron basis into more interpretable units of model computation, for tasks such as \textit{circuit tracing}. However, not all neuron-based representations are uninterpretable. For the first time, we empirically show that \textbf{MLP </p><p><strong>Tags:</strong> SAE, circuits</p>]]></description>
      <pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.22594v1</guid>
    </item>
    <item>
      <title>Beyond Activation Patterns: A Weight-Based Out-of-Context Explanation of Sparse Autoencoder Features</title>
      <link>https://arxiv.org/abs/2601.22447v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Yiting Liu, Zhi-Hong Deng</p><p>Sparse autoencoders (SAEs) have emerged as a powerful technique for decomposing language model representations into interpretable features. Current interpretation methods infer feature semantics from activation patterns, but overlook that features are trained to reconstruct activations that serve computational roles in the forward pass. We introduce a novel weight-based interpretation framework that measures functional effects through direct weight interactions, requiring no activation data. Thr</p><p><strong>Tags:</strong> SAE, features</p>]]></description>
      <pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.22447v1</guid>
    </item>
    <item>
      <title>LLMs Explain&#x27;t: A Post-Mortem on Semantic Interpretability in Transformer Models</title>
      <link>https://arxiv.org/abs/2601.22928v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Alhassan Abdelhalim, Janick Edinger, SÃ¶ren Laue, Michaela Regneri</p><p>Large Language Models (LLMs) are becoming increasingly popular in pervasive computing due to their versatility and strong performance. However, despite their ubiquitous use, the exact mechanisms underlying their outstanding performance remain unclear. Different methods for LLM explainability exist, and many are, as a method, not fully understood themselves. We started with the question of how linguistic abstraction emerges in LLMs, aiming to detect it across different LLM modules (attention head</p><p><strong>Tags:</strong> attention</p>]]></description>
      <pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.22928v1</guid>
    </item>
    <item>
      <title>Putting a Face to Forgetting: Continual Learning meets Mechanistic Interpretability</title>
      <link>https://arxiv.org/abs/2601.22012v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Sergi Masip, Gido M. van de Ven, Javier Ferrando, Tinne Tuytelaars</p><p>Catastrophic forgetting in continual learning is often measured at the performance or last-layer representation level, overlooking the underlying mechanisms. We introduce a mechanistic framework that offers a geometric interpretation of catastrophic forgetting as the result of transformations to the encoding of individual features. These transformations can lead to forgetting by reducing the allocated capacity of features (worse representation) and disrupting their readout by downstream computat</p><p><strong>Tags:</strong> features</p>]]></description>
      <pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.22012v1</guid>
    </item>
    <item>
      <title>Mechanistic Data Attribution: Tracing the Training Origins of Interpretable LLM Units</title>
      <link>https://arxiv.org/abs/2601.21996v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Jianhui Chen, Yuzhang Luo, Liangming Pan</p><p>While Mechanistic Interpretability has identified interpretable circuits in LLMs, their causal origins in training data remain elusive. We introduce Mechanistic Data Attribution (MDA), a scalable framework that employs Influence Functions to trace interpretable units back to specific training samples. Through extensive experiments on the Pythia family, we causally validate that targeted intervention--removing or augmenting a small fraction of high-influence samples--significantly modulates the e</p><p><strong>Tags:</strong> circuits</p>]]></description>
      <pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.21996v1</guid>
    </item>
    <item>
      <title>Symmetry Breaking in Transformers for Efficient and Interpretable Training</title>
      <link>https://arxiv.org/abs/2601.22257v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Eva Silverstein, Daniel Kunin, Vasudev Shyam</p><p>The attention mechanism in its standard implementation contains extraneous rotational degrees of freedom that are carried through computation but do not affect model activations or outputs. We introduce a simple symmetry-breaking protocol that inserts a preferred direction into this rotational space through batchwise-sampled, unlearned query and value biases. This modification has two theoretically motivated and empirically validated consequences. First, it can substantially improve the performa</p><p><strong>Tags:</strong> theory</p>]]></description>
      <pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.22257v1</guid>
    </item>
    <item>
      <title>Concept Component Analysis: A Principled Approach for Concept Extraction in LLMs</title>
      <link>https://arxiv.org/abs/2601.20420v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Yuhang Liu, Erdun Gao, Dong Gong, Anton van den Hengel, Javen Qinfeng Shi</p><p>Developing human understandable interpretation of large language models (LLMs) becomes increasingly critical for their deployment in essential domains. Mechanistic interpretability seeks to mitigate the issues through extracts human-interpretable process and concepts from LLMs&#x27; activations. Sparse autoencoders (SAEs) have emerged as a popular approach for extracting interpretable and monosemantic concepts by decomposing the LLM internal representations into a dictionary. Despite their empirical </p><p><strong>Tags:</strong> SAE, features</p>]]></description>
      <pubDate>Wed, 28 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.20420v1</guid>
    </item>
    <item>
      <title>How Do Transformers Learn to Associate Tokens: Gradient Leading Terms Bring Mechanistic Interpretability</title>
      <link>https://arxiv.org/abs/2601.19208v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Shawn Im, Changdae Oh, Zhen Fang, Sharon Li</p><p>Semantic associations such as the link between &quot;bird&quot; and &quot;flew&quot; are foundational for language modeling as they enable models to go beyond memorization and instead generalize and generate coherent text. Understanding how these associations are learned and represented in language models is essential for connecting deep learning with linguistic theory and developing a mechanistic foundation for large language models. In this work, we analyze how these associations emerge from natural language data</p><p><strong>Tags:</strong> theory</p>]]></description>
      <pubDate>Tue, 27 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.19208v1</guid>
    </item>
    <item>
      <title>Sparse CLIP: Co-Optimizing Interpretability and Performance in Contrastive Learning</title>
      <link>https://arxiv.org/abs/2601.20075v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Chuan Qin, Constantin Venhoff, Sonia Joseph, Fanyi Xiao, Stefan Scherer</p><p>Contrastive Language-Image Pre-training (CLIP) has become a cornerstone in vision-language representation learning, powering diverse downstream tasks and serving as the default vision backbone in multimodal large language models (MLLMs). Despite its success, CLIP&#x27;s dense and opaque latent representations pose significant interpretability challenges. A common assumption is that interpretability and performance are in tension: enforcing sparsity during training degrades accuracy, motivating recent</p><p><strong>Tags:</strong> vision</p>]]></description>
      <pubDate>Tue, 27 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.20075v1</guid>
    </item>
    <item>
      <title>Decomposing multimodal embedding spaces with group-sparse autoencoders</title>
      <link>https://arxiv.org/abs/2601.20028v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Chiraag Kaushik, Davis Barch, Andrea Fanelli</p><p>The Linear Representation Hypothesis asserts that the embeddings learned by neural networks can be understood as linear combinations of features corresponding to high-level concepts. Based on this ansatz, sparse autoencoders (SAEs) have recently become a popular method for decomposing embeddings into a sparse combination of linear directions, which have been shown empirically to often correspond to human-interpretable semantics. However, recent attempts to apply SAEs to multimodal embedding spac</p><p><strong>Tags:</strong> SAE, features, vision</p>]]></description>
      <pubDate>Tue, 27 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.20028v1</guid>
    </item>
    <item>
      <title>Funny or Persuasive, but Not Both: Evaluating Fine-Grained Multi-Concept Control in LLMs</title>
      <link>https://arxiv.org/abs/2601.18483v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Arya Labroo, Ivaxi Sheth, Vyas Raina, Amaani Ahmed, Mario Fritz</p><p>Large Language Models (LLMs) offer strong generative capabilities, but many applications require explicit and \textit{fine-grained} control over specific textual concepts, such as humor, persuasiveness, or formality. Prior approaches in prompting and representation engineering can provide coarse or single-attribute control, but systematic evaluation of multi-attribute settings remains limited. We introduce an evaluation framework for fine-grained controllability for both single- and dual-concept</p>]]></description>
      <pubDate>Mon, 26 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.18483v1</guid>
    </item>
    <item>
      <title>A Few Bad Neurons: Isolating and Surgically Correcting Sycophancy</title>
      <link>https://arxiv.org/abs/2601.18939v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Claire O&#x27;Brien, Jessica Seto, Dristi Roy, Aditya Dwivedi, Sunishchal Dev, et al.</p><p>Behavioral alignment in large language models (LLMs) is often achieved through broad fine-tuning, which can result in undesired side effects like distributional shift and low interpretability. We propose a method for alignment that identifies and updates only the neurons most responsible for a given behavior, a targeted approach that allows for fine-tuning with significantly less data. Using sparse autoencoders (SAEs) and linear probes, we isolate the 3% of MLP neurons most predictive of a targe</p><p><strong>Tags:</strong> SAE, safety, probing</p>]]></description>
      <pubDate>Mon, 26 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.18939v1</guid>
    </item>
    <item>
      <title>A Monosemantic Attribution Framework for Stable Interpretability in Clinical Neuroscience Large Language Models</title>
      <link>https://arxiv.org/abs/2601.17952v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Michail Mamalakis, Tiago Azevedo, Cristian Cosentino, Chiara D&#x27;Ercoli, Subati Abulikemu, et al.</p><p>Interpretability remains a key challenge for deploying large language models (LLMs) in clinical settings such as Alzheimer&#x27;s disease progression diagnosis, where early and trustworthy predictions are essential. Existing attribution methods exhibit high inter-method variability and unstable explanations due to the polysemantic nature of LLM representations, while mechanistic interpretability approaches lack direct alignment with model inputs and outputs and do not provide explicit importance scor</p><p><strong>Tags:</strong> features, safety</p>]]></description>
      <pubDate>Sun, 25 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.17952v1</guid>
    </item>
  </channel>
</rss>
