<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Mechanistic Interpretability Hub</title>
    <link>https://izkula.github.io/cc</link>
    <description>Latest research in mechanistic interpretability - understanding how neural networks work internally</description>
    <language>en-us</language>
    <lastBuildDate>Thu, 08 Jan 2026 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://izkula.github.io/cc/feed.xml" rel="self" type="application/rss+xml"/>

    <item>
      <title>Controllable LLM Reasoning via Sparse Autoencoder-Based Steering</title>
      <link>https://arxiv.org/abs/2601.03595v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Yi Fang, Wenjie Wang, Mingfeng Xue, Boyi Deng, Fengli Xu, et al.</p><p>Large Reasoning Models (LRMs) exhibit human-like cognitive reasoning strategies (e.g. backtracking, cross-verification) during reasoning process, which improves their performance on complex tasks. Currently, reasoning strategies are autonomously selected by LRMs themselves. However, such autonomous selection often produces inefficient or even erroneous reasoning paths. To make reasoning more reliable and flexible, it is important to develop methods for controlling reasoning strategies. Existing </p><p><strong>Tags:</strong> SAE, steering, reasoning</p>]]></description>
      <pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.03595v1</guid>
    </item>
    <item>
      <title>NeuronScope: A Multi-Agent Framework for Explaining Polysemantic Neurons in Language Models</title>
      <link>https://arxiv.org/abs/2601.03671v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Weiqi Liu, Yongliang Miao, Haiyan Zhao, Yanguang Liu, Mengnan Du</p><p>Neuron-level interpretation in large language models (LLMs) is fundamentally challenged by widespread polysemanticity, where individual neurons respond to multiple distinct semantic concepts. Existing single-pass interpretation methods struggle to faithfully capture such multi-concept behavior. In this work, we propose NeuronScope, a multi-agent framework that reformulates neuron interpretation as an iterative, activation-guided process. NeuronScope explicitly deconstructs neuron activations int</p><p><strong>Tags:</strong> features</p>]]></description>
      <pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.03671v1</guid>
    </item>
    <item>
      <title>Where meaning lives: Layer-wise accessibility of psycholinguistic features in encoder and decoder language models</title>
      <link>https://arxiv.org/abs/2601.03798v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Taisiia Tikhomirova, Dirk U. Wulff</p><p>Understanding where transformer language models encode psychologically meaningful aspects of meaning is essential for both theory and practice. We conduct a systematic layer-wise probing study of 58 psycholinguistic features across 10 transformer models, spanning encoder-only and decoder-only architectures, and compare three embedding extraction methods. We find that apparent localization of meaning is strongly method-dependent: contextualized embeddings yield higher feature-specific selectivity</p><p><strong>Tags:</strong> features, probing, theory</p>]]></description>
      <pubDate>Wed, 07 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.03798v1</guid>
    </item>
    <item>
      <title>When the Coffee Feature Activates on Coffins: An Analysis of Feature Extraction and Steering for Mechanistic Interpretability</title>
      <link>https://arxiv.org/abs/2601.03047v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Raphael Ronge, Markus Maier, Frederick Eberhardt</p><p>Recent work by Anthropic on Mechanistic interpretability claims to understand and control Large Language Models by extracting human-interpretable features from their neural activation patterns using sparse autoencoders (SAEs). If successful, this approach offers one of the most promising routes for human oversight in AI safety. We conduct an initial stress-test of these claims by replicating their main results with open-source SAEs for Llama 3.1. While we successfully reproduce basic feature ext</p><p><strong>Tags:</strong> SAE, features, safety, steering</p>]]></description>
      <pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.03047v1</guid>
    </item>
    <item>
      <title>Mechanistic Interpretability of Large-Scale Counting in LLMs through a System-2 Strategy</title>
      <link>https://arxiv.org/abs/2601.02989v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Hosein Hasani, Mohammadali Banayeeanzade, Ali Nafisi, Sadegh Mohammadian, Fatemeh Askari, et al.</p><p>Large language models (LLMs), despite strong performance on complex mathematical problems, exhibit systematic limitations in counting tasks. This issue arises from architectural limits of transformers, where counting is performed across layers, leading to degraded precision for larger counting problems due to depth constraints. To address this limitation, we propose a simple test-time strategy inspired by System-2 cognitive processes that decomposes large counting tasks into smaller, independent</p>]]></description>
      <pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.02989v1</guid>
    </item>
    <item>
      <title>Mechanistic Knobs in LLMs: Retrieving and Steering High-Order Semantic Features via Sparse Autoencoders</title>
      <link>https://arxiv.org/abs/2601.02978v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Ruikang Zhang, Shuo Wang, Qi Su</p><p>Recent work in Mechanistic Interpretability (MI) has enabled the identification and intervention of internal features in Large Language Models (LLMs). However, a persistent challenge lies in linking such internal features to the reliable control of complex, behavior-level semantic attributes in language generation. In this paper, we propose a Sparse Autoencoder-based framework for retrieving and steering semantically interpretable internal features associated with high-level linguistic behaviors</p><p><strong>Tags:</strong> SAE, features, steering</p>]]></description>
      <pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.02978v1</guid>
    </item>
    <item>
      <title>Bridging Mechanistic Interpretability and Prompt Engineering with Gradient Ascent for Interpretable Persona Control</title>
      <link>https://arxiv.org/abs/2601.02896v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Harshvardhan Saini, Yiming Tang, Dianbo Liu</p><p>Controlling emergent behavioral personas (e.g., sycophancy, hallucination) in Large Language Models (LLMs) is critical for AI safety, yet remains a persistent challenge. Existing solutions face a dilemma: manual prompt engineering is intuitive but unscalable and imprecise, while automatic optimization methods are effective but operate as &quot;black boxes&quot; with no interpretable connection to model internals. We propose a novel framework that adapts gradient ascent to LLMs, enabling targeted prompt di</p><p><strong>Tags:</strong> safety</p>]]></description>
      <pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.02896v1</guid>
    </item>
    <item>
      <title>MAFS: Multi-head Attention Feature Selection for High-Dimensional Data via Deep Fusion of Filter Methods</title>
      <link>https://arxiv.org/abs/2601.02668v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Xiaoyan Sun, Qingyu Meng, Yalu Wen</p><p>Feature selection is essential for high-dimensional biomedical data, enabling stronger predictive performance, reduced computational cost, and improved interpretability in precision medicine applications. Existing approaches face notable challenges. Filter methods are highly scalable but cannot capture complex relationships or eliminate redundancy. Deep learning-based approaches can model nonlinear patterns but often lack stability, interpretability, and efficiency at scale. Single-head attentio</p><p><strong>Tags:</strong> features</p>]]></description>
      <pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.02668v1</guid>
    </item>
    <item>
      <title>TRYLOCK: Defense-in-Depth Against LLM Jailbreaks via Layered Preference and Representation Engineering</title>
      <link>https://arxiv.org/abs/2601.03300v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Scott Thornton</p><p>Large language models remain vulnerable to jailbreak attacks, and single-layer defenses often trade security for usability. We present TRYLOCK, the first defense-in-depth architecture that combines four heterogeneous mechanisms across the inference stack: weight-level safety alignment via DPO, activation-level control via Representation Engineering (RepE) steering, adaptive steering strength selected by a lightweight sidecar classifier, and input canonicalization to neutralize encoding-based byp</p><p><strong>Tags:</strong> safety, probing, steering</p>]]></description>
      <pubDate>Tue, 06 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.03300v1</guid>
    </item>
    <item>
      <title>Central Dogma Transformer: Towards Mechanism-Oriented AI for Cellular Understanding</title>
      <link>https://arxiv.org/abs/2601.01089v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Nobuyuki Ota</p><p>Understanding cellular mechanisms requires integrating information across DNA, RNA, and protein - the three molecular systems linked by the Central Dogma of molecular biology. While domain-specific foundation models have achieved success for each modality individually, they remain isolated, limiting our ability to model integrated cellular processes. Here we present the Central Dogma Transformer (CDT), an architecture that integrates pre-trained language models for DNA, RNA, and protein followin</p><p><strong>Tags:</strong> biology</p>]]></description>
      <pubDate>Sat, 03 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.01089v1</guid>
    </item>
    <item>
      <title>Explainability-Guided Defense: Attribution-Aware Model Refinement Against Adversarial Data Attacks</title>
      <link>https://arxiv.org/abs/2601.00968v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Longwei Wang, Mohammad Navid Nayyem, Abdullah Al Rakin, KC Santosh, Chaowei Zhang, et al.</p><p>The growing reliance on deep learning models in safety-critical domains such as healthcare and autonomous navigation underscores the need for defenses that are both robust to adversarial perturbations and transparent in their decision-making. In this paper, we identify a connection between interpretability and robustness that can be directly leveraged during training. Specifically, we observe that spurious, unstable, or semantically irrelevant features identified through Local Interpretable Mode</p><p><strong>Tags:</strong> features, safety</p>]]></description>
      <pubDate>Fri, 02 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.00968v1</guid>
    </item>
    <item>
      <title>ActErase: A Training-Free Paradigm for Precise Concept Erasure via Activation Patching</title>
      <link>https://arxiv.org/abs/2601.00267v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Yi Sun, Xinhao Zhong, Hongyan Li, Yimin Zhou, Junhao Li, et al.</p><p>Recent advances in text-to-image diffusion models have demonstrated remarkable generation capabilities, yet they raise significant concerns regarding safety, copyright, and ethical implications. Existing concept erasure methods address these risks by removing sensitive concepts from pre-trained models, but most of them rely on data-intensive and computationally expensive fine-tuning, which poses a critical limitation. To overcome these challenges, inspired by the observation that the model&#x27;s act</p><p><strong>Tags:</strong> safety, vision</p>]]></description>
      <pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.00267v1</guid>
    </item>
    <item>
      <title>Triangulation as an Acceptance Rule for Multilingual Mechanistic Interpretability</title>
      <link>https://arxiv.org/abs/2512.24842v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Yanan Long</p><p>Multilingual language models achieve strong aggregate performance yet often behave unpredictably across languages, scripts, and cultures. We argue that mechanistic explanations for such models should satisfy a \emph{causal} standard: claims must survive causal interventions and must \emph{cross-reference} across environments that perturb surface form while preserving meaning. We formalize \emph{reference families} as predicate-preserving variants and introduce \emph{triangulation}, an acceptance</p>]]></description>
      <pubDate>Wed, 31 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2512.24842v1</guid>
    </item>
    <item>
      <title>Attribution-Guided Distillation of Matryoshka Sparse Autoencoders</title>
      <link>https://arxiv.org/abs/2512.24975v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Cristina P. Martin-Linares, Jonathan P. Ling</p><p>Sparse autoencoders (SAEs) aim to disentangle model activations into monosemantic, human-interpretable features. In practice, learned features are often redundant and vary across training runs and sparsity levels, which makes interpretations difficult to transfer and reuse. We introduce Distilled Matryoshka Sparse Autoencoders (DMSAEs), a training pipeline that distills a compact core of consistently useful features and reuses it to train new SAEs. DMSAEs run an iterative distillation cycle: tra</p><p><strong>Tags:</strong> SAE, features</p>]]></description>
      <pubDate>Wed, 31 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2512.24975v1</guid>
    </item>
    <item>
      <title>MEIC-DT: Memory-Efficient Incremental Clustering for Long-Text Coreference Resolution with Dual-Threshold Constraints</title>
      <link>https://arxiv.org/abs/2512.24711v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Kangyang Luo, Shuzheng Si, Yuzhuo Bai, Cheng Gao, Zhitong Wang, et al.</p><p>In the era of large language models (LLMs), supervised neural methods remain the state-of-the-art (SOTA) for Coreference Resolution. Yet, their full potential is underexplored, particularly in incremental clustering, which faces the critical challenge of balancing efficiency with performance for long texts. To address the limitation, we propose \textbf{MEIC-DT}, a novel dual-threshold, memory-efficient incremental clustering approach based on a lightweight Transformer. MEIC-DT features a dual-th</p><p><strong>Tags:</strong> features</p>]]></description>
      <pubDate>Wed, 31 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2512.24711v1</guid>
    </item>
    <item>
      <title>Fantastic Reasoning Behaviors and Where to Find Them: Unsupervised Discovery of the Reasoning Process</title>
      <link>https://arxiv.org/abs/2512.23988v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Zhenyu Zhang, Shujian Zhang, John Lambert, Wenxuan Zhou, Zhangyang Wang, et al.</p><p>Despite the growing reasoning capabilities of recent large language models (LLMs), their internal mechanisms during the reasoning process remain underexplored. Prior approaches often rely on human-defined concepts (e.g., overthinking, reflection) at the word level to analyze reasoning in a supervised manner. However, such methods are limited, as it is infeasible to capture the full spectrum of potential reasoning behaviors, many of which are difficult to define in token space. In this work, we p</p><p><strong>Tags:</strong> reasoning</p>]]></description>
      <pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2512.23988v1</guid>
    </item>
    <item>
      <title>Adversarial Lens: Exploiting Attention Layers to Generate Adversarial Examples for Evaluation</title>
      <link>https://arxiv.org/abs/2512.23837v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Kaustubh Dhole</p><p>Recent advances in mechanistic interpretability suggest that intermediate attention layers encode token-level hypotheses that are iteratively refined toward the final output. In this work, we exploit this property to generate adversarial examples directly from attention-layer token distributions. Unlike prompt-based or gradient-based attacks, our approach leverages model-internal token predictions, producing perturbations that are both plausible and internally consistent with the model&#x27;s own gen</p>]]></description>
      <pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2512.23837v1</guid>
    </item>
    <item>
      <title>Interpretable Safety Alignment via SAE-Constructed Low-Rank Subspace Adaptation</title>
      <link>https://arxiv.org/abs/2512.23260v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Dianyun Wang, Qingsen Ma, Yuhu Shang, Zhifeng Lu, Lechen Ning, et al.</p><p>Parameter-efficient fine-tuning has become the dominant paradigm for adapting large language models to downstream tasks. Low-rank adaptation methods such as LoRA operate under the assumption that task-relevant weight updates reside in a low-rank subspace, yet this subspace is learned implicitly from data in a black-box manner, offering no interpretability or direct control. We hypothesize that this difficulty stems from polysemanticity--individual dimensions encoding multiple entangled concepts.</p><p><strong>Tags:</strong> features, safety</p>]]></description>
      <pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2512.23260v1</guid>
    </item>
    <item>
      <title>Mechanistic Analysis of Circuit Preservation in Federated Learning</title>
      <link>https://arxiv.org/abs/2512.23043v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Muhammad Haseeb, Salaar Masood, Muhammad Abdullah Sohail</p><p>Federated Learning (FL) enables collaborative training of models on decentralized data, but its performance degrades significantly under Non-IID (non-independent and identically distributed) data conditions. While this accuracy loss is well-documented, the internal mechanistic causes remain a black box. This paper investigates the canonical FedAvg algorithm through the lens of Mechanistic Interpretability (MI) to diagnose this failure mode. We hypothesize that the aggregation of conflicting clie</p><p><strong>Tags:</strong> circuits</p>]]></description>
      <pubDate>Sun, 28 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2512.23043v1</guid>
    </item>
    <item>
      <title>Decomposing Task Vectors for Refined Model Editing</title>
      <link>https://arxiv.org/abs/2512.22511v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Hamed Damirchi, Ehsan Abbasnejad, Zhen Zhang, Javen Shi</p><p>Large pre-trained models have transformed machine learning, yet adapting these models effectively to exhibit precise, concept-specific behaviors remains a significant challenge. Task vectors, defined as the difference between fine-tuned and pre-trained model parameters, provide a mechanism for steering neural networks toward desired behaviors. This has given rise to large repositories dedicated to task vectors tailored for specific behaviors. The arithmetic operation of these task vectors allows</p><p><strong>Tags:</strong> steering, editing</p>]]></description>
      <pubDate>Sat, 27 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2512.22511v1</guid>
    </item>
    <item>
      <title>The Deepfake Detective: Interpreting Neural Forensics Through Sparse Features and Manifolds</title>
      <link>https://arxiv.org/abs/2512.21670v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Subramanyam Sahoo, Jared Junkin</p><p>Deepfake detection models have achieved high accuracy in identifying synthetic media, but their decision processes remain largely opaque. In this paper we present a mechanistic interpretability framework for deepfake detection applied to a vision-language model. Our approach combines a sparse autoencoder (SAE) analysis of internal network representations with a novel forensic manifold analysis that probes how the model&#x27;s features respond to controlled forensic artifact manipulations. We demonstr</p><p><strong>Tags:</strong> SAE, features, probing, vision</p>]]></description>
      <pubDate>Thu, 25 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2512.21670v1</guid>
    </item>
    <item>
      <title>Omni-Weather: Unified Multimodal Foundation Model for Weather Generation and Understanding</title>
      <link>https://arxiv.org/abs/2512.21643v2</link>
      <description><![CDATA[<p><strong>Authors:</strong> Zhiwang Zhou, Yuandong Pu, Xuming He, Yidi Liu, Yixin Chen, et al.</p><p>Weather modeling requires both accurate prediction and mechanistic interpretation, yet existing methods treat these goals in isolation, separating generation from understanding. To address this gap, we present Omni-Weather, the first multimodal foundation model that unifies weather generation and understanding within a single architecture. Omni-Weather integrates a radar encoder for weather generation tasks, followed by unified processing using a shared self-attention mechanism. Moreover, we con</p><p><strong>Tags:</strong> vision</p>]]></description>
      <pubDate>Thu, 25 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2512.21643v2</guid>
    </item>
    <item>
      <title>Learning from Negative Examples: Why Warning-Framed Training Data Teaches What It Warns Against</title>
      <link>https://arxiv.org/abs/2512.22293v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Tsogt-Ochir Enkhbayar</p><p>Warning-framed content in training data (e.g., &quot;DO NOT USE - this code is vulnerable&quot;) does not, it turns out, teach language models to avoid the warned-against behavior. In experiments reported here, models exposed to such warnings reproduced the flagged content at rates statistically indistinguishable from models given the content directly (76.7% vs. 83.3%). Why? Sparse autoencoder analysis points to a failure of orthogonalization: &quot;describing X&quot; and &quot;performing X&quot; activate overlapping latent </p><p><strong>Tags:</strong> SAE</p>]]></description>
      <pubDate>Thu, 25 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2512.22293v1</guid>
    </item>
    <item>
      <title>Mechanistic Interpretability of Fine-Tuned Protein Language Models for Nanobody Thermostability Prediction</title>
      <link>https://www.semanticscholar.org/paper/0544168cf2012e31794a69547a7537994417b47d</link>
      <description><![CDATA[<p><strong>Authors:</strong> Taihei Murakami, Yuki Hashidate, Yasuhiro Matsunaga</p><p><strong>Tags:</strong> biology</p>]]></description>
      <pubDate>Wed, 24 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://www.semanticscholar.org/paper/0544168cf2012e31794a69547a7537994417b47d</guid>
    </item>
    <item>
      <title>Measuring Mechanistic Independence: Can Bias Be Removed Without Erasing Demographics?</title>
      <link>https://arxiv.org/abs/2512.20796v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Zhengyang Shan, Aaron Mueller</p><p>We investigate how independent demographic bias mechanisms are from general demographic recognition in language models. Using a multi-task evaluation setup where demographics are associated with names, professions, and education levels, we measure whether models can be debiased while preserving demographic detection capabilities. We compare attribution-based and correlation-based methods for locating bias features. We find that targeted sparse autoencoder feature ablations in Gemma-2-9B reduce b</p><p><strong>Tags:</strong> SAE, features</p>]]></description>
      <pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2512.20796v1</guid>
    </item>
    <item>
      <title>Hierarchical Geometry of Cognitive States in Transformer Embedding Spaces</title>
      <link>https://arxiv.org/abs/2512.22227v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Sophie Zhao</p><p>Recent work has shown that transformer-based language models learn rich geometric structure in their embedding spaces, yet the presence of higher-level cognitive organization within these representations remains underexplored. In this work, we investigate whether sentence embeddings encode a graded, hierarchical structure aligned with human-interpretable cognitive or psychological attributes. We construct a dataset of 480 natural-language sentences annotated with continuous ordinal energy scores</p>]]></description>
      <pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2512.22227v1</guid>
    </item>
    <item>
      <title>Toward Explaining Large Language Models in Software Engineering Tasks</title>
      <link>https://arxiv.org/abs/2512.20328v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Antonio Vitale, Khai-Nguyen Nguyen, Denys Poshyvanyk, Rocco Oliveto, Simone Scalabrino, et al.</p><p>Recent progress in Large Language Models (LLMs) has substantially advanced the automation of software engineering (SE) tasks, enabling complex activities such as code generation and code summarization. However, the black-box nature of LLMs remains a major barrier to their adoption in high-stakes and safety-critical domains, where explainability and transparency are vital for trust, accountability, and effective human supervision. Despite increasing interest in explainable AI for software enginee</p><p><strong>Tags:</strong> safety, vision</p>]]></description>
      <pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2512.20328v1</guid>
    </item>
    <item>
      <title>Generative Giants, Retrieval Weaklings: Why do Multimodal Large Language Models Fail at Multimodal Retrieval?</title>
      <link>https://arxiv.org/abs/2512.19115v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Hengyi Feng, Zeang Sheng, Meiyi Qiang, Wentao Zhang</p><p>Despite the remarkable success of multimodal large language models (MLLMs) in generative tasks, we observe that they exhibit a counterintuitive deficiency in the zero-shot multimodal retrieval task. In this work, we investigate the underlying mechanisms that hinder MLLMs from serving as effective retrievers. With the help of sparse autoencoders (SAEs), we decompose MLLM output representations into interpretable semantic concepts to probe their intrinsic behavior. Our analysis reveals that the re</p><p><strong>Tags:</strong> SAE, probing, vision</p>]]></description>
      <pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2512.19115v1</guid>
    </item>
    <item>
      <title>LouvreSAE: Sparse Autoencoders for Interpretable and Controllable Style Transfer</title>
      <link>https://arxiv.org/abs/2512.18930v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Raina Panda, Daniel Fein, Arpita Singhal, Mark Fiore, Maneesh Agrawala, et al.</p><p>Artistic style transfer in generative models remains a significant challenge, as existing methods often introduce style only via model fine-tuning, additional adapters, or prompt engineering, all of which can be computationally expensive and may still entangle style with subject matter. In this paper, we introduce a training- and inference-light, interpretable method for representing and transferring artistic style. Our approach leverages an art-specific Sparse Autoencoder (SAE) on top of latent</p><p><strong>Tags:</strong> SAE</p>]]></description>
      <pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2512.18930v1</guid>
    </item>
    <item>
      <title>SAP: Syntactic Attention Pruning for Transformer-based Language Models</title>
      <link>https://arxiv.org/abs/2512.19125v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Tzu-Yun Lee, Ding-Yong Hong, Jan-Jan Wu</p><p>This paper introduces Syntactic Attention Pruning (SAP), a novel method for effectively pruning attention heads in Transformer models. Unlike conventional approaches that rely solely on mathematical analysis of model weights and activations, SAP incorporates both the syntactic structure and attention patterns of sentences to guide the pruning process. By leveraging these linguistic features, SAP not only achieves performance comparable to state-of-the-art methods but also enhances the interpreta</p><p><strong>Tags:</strong> features, attention</p>]]></description>
      <pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2512.19125v1</guid>
    </item>
  </channel>
</rss>
