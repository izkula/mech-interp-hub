<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Mechanistic Interpretability Hub</title>
    <link>https://izkula.github.io/cc</link>
    <description>Latest research in mechanistic interpretability - understanding how neural networks work internally</description>
    <language>en-us</language>
    <lastBuildDate>Wed, 28 Jan 2026 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://izkula.github.io/cc/feed.xml" rel="self" type="application/rss+xml"/>

    <item>
      <title>Research on the instability mechanism of transformer winding short-circuit faults based on magneto-mechanical coupling analysis</title>
      <link>https://www.semanticscholar.org/paper/48681d875dd4a7fd38cd0b9f05262cccabefe846</link>
      <description><![CDATA[<p><strong>Authors:</strong> Zhihao Gao, Jing Zhou, Li Zhu, Yuefeng Hao, Zhanlong Zhang, et al.</p><p><strong>Tags:</strong> circuits</p>]]></description>
      <pubDate>Sun, 01 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://www.semanticscholar.org/paper/48681d875dd4a7fd38cd0b9f05262cccabefe846</guid>
    </item>
    <item>
      <title>How Do Transformers Learn to Associate Tokens: Gradient Leading Terms Bring Mechanistic Interpretability</title>
      <link>https://arxiv.org/abs/2601.19208v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Shawn Im, Changdae Oh, Zhen Fang, Sharon Li</p><p>Semantic associations such as the link between &quot;bird&quot; and &quot;flew&quot; are foundational for language modeling as they enable models to go beyond memorization and instead generalize and generate coherent text. Understanding how these associations are learned and represented in language models is essential for connecting deep learning with linguistic theory and developing a mechanistic foundation for large language models. In this work, we analyze how these associations emerge from natural language data</p><p><strong>Tags:</strong> theory</p>]]></description>
      <pubDate>Tue, 27 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.19208v1</guid>
    </item>
    <item>
      <title>Funny or Persuasive, but Not Both: Evaluating Fine-Grained Multi-Concept Control in LLMs</title>
      <link>https://arxiv.org/abs/2601.18483v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Arya Labroo, Ivaxi Sheth, Vyas Raina, Amaani Ahmed, Mario Fritz</p><p>Large Language Models (LLMs) offer strong generative capabilities, but many applications require explicit and \textit{fine-grained} control over specific textual concepts, such as humor, persuasiveness, or formality. Prior approaches in prompting and representation engineering can provide coarse or single-attribute control, but systematic evaluation of multi-attribute settings remains limited. We introduce an evaluation framework for fine-grained controllability for both single- and dual-concept</p>]]></description>
      <pubDate>Mon, 26 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.18483v1</guid>
    </item>
    <item>
      <title>A Few Bad Neurons: Isolating and Surgically Correcting Sycophancy</title>
      <link>https://arxiv.org/abs/2601.18939v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Claire O&#x27;Brien, Jessica Seto, Dristi Roy, Aditya Dwivedi, Sunishchal Dev, et al.</p><p>Behavioral alignment in large language models (LLMs) is often achieved through broad fine-tuning, which can result in undesired side effects like distributional shift and low interpretability. We propose a method for alignment that identifies and updates only the neurons most responsible for a given behavior, a targeted approach that allows for fine-tuning with significantly less data. Using sparse autoencoders (SAEs) and linear probes, we isolate the 3% of MLP neurons most predictive of a targe</p><p><strong>Tags:</strong> SAE, safety, probing</p>]]></description>
      <pubDate>Mon, 26 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.18939v1</guid>
    </item>
    <item>
      <title>A Monosemantic Attribution Framework for Stable Interpretability in Clinical Neuroscience Large Language Models</title>
      <link>https://arxiv.org/abs/2601.17952v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Michail Mamalakis, Tiago Azevedo, Cristian Cosentino, Chiara D&#x27;Ercoli, Subati Abulikemu, et al.</p><p>Interpretability remains a key challenge for deploying large language models (LLMs) in clinical settings such as Alzheimer&#x27;s disease progression diagnosis, where early and trustworthy predictions are essential. Existing attribution methods exhibit high inter-method variability and unstable explanations due to the polysemantic nature of LLM representations, while mechanistic interpretability approaches lack direct alignment with model inputs and outputs and do not provide explicit importance scor</p><p><strong>Tags:</strong> features, safety</p>]]></description>
      <pubDate>Sun, 25 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.17952v1</guid>
    </item>
    <item>
      <title>TensorLens: End-to-End Transformer Analysis via High-Order Attention Tensors</title>
      <link>https://arxiv.org/abs/2601.17958v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Ido Andrew Atad, Itamar Zimerman, Shahar Katz, Lior Wolf</p><p>Attention matrices are fundamental to transformer research, supporting a broad range of applications including interpretability, visualization, manipulation, and distillation. Yet, most existing analyses focus on individual attention heads or layers, failing to account for the model&#x27;s global behavior. While prior efforts have extended attention formulations across multiple heads via averaging and matrix multiplications or incorporated components such as normalization and FFNs, a unified and comp</p><p><strong>Tags:</strong> attention, vision</p>]]></description>
      <pubDate>Sun, 25 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.17958v1</guid>
    </item>
    <item>
      <title>Implementing Tensor Logic: Unifying Datalog and Neural Reasoning via Tensor Contraction</title>
      <link>https://arxiv.org/abs/2601.17188v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Swapn Shah, Wlodek Zadrozny</p><p>The unification of symbolic reasoning and neural networks remains a central challenge in artificial intelligence. Symbolic systems offer reliability and interpretability but lack scalability, while neural networks provide learning capabilities but sacrifice transparency. Tensor Logic, proposed by Domingos, suggests that logical rules and Einstein summation are mathematically equivalent, offering a principled path toward unification. This paper provides empirical validation of this framework thro</p><p><strong>Tags:</strong> reasoning</p>]]></description>
      <pubDate>Fri, 23 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.17188v1</guid>
    </item>
    <item>
      <title>Attributing and Exploiting Safety Vectors through Global Optimization in Large Language Models</title>
      <link>https://arxiv.org/abs/2601.15801v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Fengheng Chu, Jiahao Chen, Yuhong Wang, Jun Wang, Zhihui Fu, et al.</p><p>While Large Language Models (LLMs) are aligned to mitigate risks, their safety guardrails remain fragile against jailbreak attacks. This reveals limited understanding of components governing safety. Existing methods rely on local, greedy attribution that assumes independent component contributions. However, they overlook the cooperative interactions between different components in LLMs, such as attention heads, which jointly contribute to safety mechanisms. We propose \textbf{G}lobal \textbf{O}p</p><p><strong>Tags:</strong> safety, attention</p>]]></description>
      <pubDate>Thu, 22 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.15801v1</guid>
    </item>
    <item>
      <title>From Insight to Intervention: Interpretable Neuron Steering for Controlling Popularity Bias in Recommender Systems</title>
      <link>https://arxiv.org/abs/2601.15122v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Parviz Ahmadov, Masoud Mansoury</p><p>Popularity bias is a pervasive challenge in recommender systems, where a few popular items dominate attention while the majority of less popular items remain underexposed. This imbalance can reduce recommendation quality and lead to unfair item exposure. Although existing mitigation methods address this issue to some extent, they often lack transparency in how they operate. In this paper, we propose a post-hoc approach, PopSteer, that leverages a Sparse Autoencoder (SAE) to both interpret and mi</p><p><strong>Tags:</strong> SAE, steering</p>]]></description>
      <pubDate>Wed, 21 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.15122v1</guid>
    </item>
    <item>
      <title>Mechanism Shift During Post-training from Autoregressive to Masked Diffusion Language Models</title>
      <link>https://arxiv.org/abs/2601.14758v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Injin Kong, Hyoungjoon Lee, Yohan Jo</p><p>Post-training pretrained Autoregressive models (ARMs) into Masked Diffusion models (MDMs) has emerged as a cost-effective strategy to overcome the limitations of sequential generation. However, the internal algorithmic transformations induced by this paradigm shift remain unexplored, leaving it unclear whether post-trained MDMs acquire genuine bidirectional reasoning capabilities or merely repackage autoregressive heuristics. In this work, we address this question by conducting a comparative cir</p><p><strong>Tags:</strong> reasoning</p>]]></description>
      <pubDate>Wed, 21 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.14758v1</guid>
    </item>
    <item>
      <title>CASL: Concept-Aligned Sparse Latents for Interpreting Diffusion Models</title>
      <link>https://arxiv.org/abs/2601.15441v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Zhenghao He, Guangzhi Xiong, Boyang Wang, Sanchit Sinha, Aidong Zhang</p><p>Internal activations of diffusion models encode rich semantic information, but interpreting such representations remains challenging. While Sparse Autoencoders (SAEs) have shown promise in disentangling latent representations, existing SAE-based methods for diffusion model understanding rely on unsupervised approaches that fail to align sparse features with human-understandable concepts. This limits their ability to provide reliable semantic control over generated images. We introduce CASL (Conc</p><p><strong>Tags:</strong> SAE, features, vision</p>]]></description>
      <pubDate>Wed, 21 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.15441v1</guid>
    </item>
    <item>
      <title>PRISM: Deriving the Transformer as a Signal-Denoising Operator via Maximum Coding Rate Reduction</title>
      <link>https://arxiv.org/abs/2601.15540v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Dongchen Huang</p><p>Deep learning models, particularly Transformers, are often criticized as &quot;black boxes&quot; and lack interpretability. We propose Prism, a white-box attention-based architecture derived from the principles of Maximizing Coding Rate Reduction ($\text{MCR}^2$). By modeling the attention mechanism as a gradient ascent process on a distinct signal-noise manifold, we introduce two physical constraints: an overcomplete dictionary to expand the representational phase space, and an irrational frequency separ</p>]]></description>
      <pubDate>Wed, 21 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.15540v1</guid>
    </item>
    <item>
      <title>Locate, Steer, and Improve: A Practical Survey of Actionable Mechanistic Interpretability in Large Language Models</title>
      <link>https://arxiv.org/abs/2601.14004v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Hengyuan Zhang, Zhihao Zhang, Mingyang Wang, Zunhai Su, Yiwei Wang, et al.</p><p>Mechanistic Interpretability (MI) has emerged as a vital approach to demystify the opaque decision-making of Large Language Models (LLMs). However, existing reviews primarily treat MI as an observational science, summarizing analytical insights while lacking a systematic framework for actionable intervention. To bridge this gap, we present a practical survey structured around the pipeline: &quot;Locate, Steer, and Improve.&quot; We formally categorize Localizing (diagnosis) and Steering (intervention) met</p><p><strong>Tags:</strong> steering, survey</p>]]></description>
      <pubDate>Tue, 20 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.14004v1</guid>
    </item>
    <item>
      <title>Patterning: The Dual of Interpretability</title>
      <link>https://arxiv.org/abs/2601.13548v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> George Wang, Daniel Murfet</p><p>Mechanistic interpretability aims to understand how neural networks generalize beyond their training data by reverse-engineering their internal structures. We introduce patterning as the dual problem: given a desired form of generalization, determine what training data produces it. Our approach is based on susceptibilities, which measure how posterior expectation values of observables respond to infinitesimal shifts in the data distribution. Inverting this linear response relationship yields the</p>]]></description>
      <pubDate>Tue, 20 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.13548v1</guid>
    </item>
    <item>
      <title>Hierarchical Sparse Circuit Extraction from Billion-Parameter Language Models through Scalable Attribution Graph Decomposition</title>
      <link>https://arxiv.org/abs/2601.12879v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Mohammed Mudassir Uddin, Shahnawaz Alam, Mohammed Kaif Pasha</p><p>Mechanistic interpretability seeks to reverse-engineer neural network computations into human-understandable algorithms, yet extracting sparse computational circuits from billion-parameter language models remains challenging due to exponential search complexity and pervasive polysemanticity. The proposed Hierarchical Attribution Graph Decomposition (HAGD) framework reduces circuit discovery complexity from O(2^n) exhaustive enumeration to O(n^2 log n) through multi-resolution abstraction hierarc</p><p><strong>Tags:</strong> circuits, features</p>]]></description>
      <pubDate>Mon, 19 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.12879v1</guid>
    </item>
    <item>
      <title>Conversational Context Classification: A Representation Engineering Approach</title>
      <link>https://arxiv.org/abs/2601.12286v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Jonathan Pan</p><p>The increasing prevalence of Large Language Models (LLMs) demands effective safeguards for their operation, particularly concerning their tendency to generate out-of-context responses. A key challenge is accurately detecting when LLMs stray from expected conversational norms, manifesting as topic shifts, factual inaccuracies, or outright hallucinations. Traditional anomaly detection struggles to directly apply within contextual semantics. This paper outlines our experiment in exploring the use o</p>]]></description>
      <pubDate>Sun, 18 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.12286v1</guid>
    </item>
    <item>
      <title>From Interpretability to Performance: Optimizing Retrieval Heads for Long-Context Language Models</title>
      <link>https://arxiv.org/abs/2601.11020v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Youmi Ma, Naoaki Okazaki</p><p>Advances in mechanistic interpretability have identified special attention heads, known as retrieval heads, that are responsible for retrieving information from the context. However, the role of these retrieval heads in improving model performance remains unexplored. This work investigates whether retrieval heads can be leveraged to enhance the long-context capabilities of LLMs. Specifically, we propose RetMask, a method that generates training signals by contrasting normal model outputs with th</p><p><strong>Tags:</strong> attention</p>]]></description>
      <pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.11020v1</guid>
    </item>
    <item>
      <title>From Knots to Knobs: Towards Steerable Collaborative Filtering Using Sparse Autoencoders</title>
      <link>https://arxiv.org/abs/2601.11182v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Martin Spišák, Ladislav Peška, Petr Škoda, Vojtěch Vančura, Rodrigo Alves</p><p>Sparse autoencoders (SAEs) have recently emerged as pivotal tools for introspection into large language models. SAEs can uncover high-quality, interpretable features at different levels of granularity and enable targeted steering of the generation process by selectively activating specific neurons in their latent activations. Our paper is the first to apply this approach to collaborative filtering, aiming to extract similarly interpretable features from representations learned purely from intera</p><p><strong>Tags:</strong> SAE, features, steering</p>]]></description>
      <pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.11182v1</guid>
    </item>
    <item>
      <title>Finding the Translation Switch: Discovering and Exploiting the Task-Initiation Features in LLMs</title>
      <link>https://arxiv.org/abs/2601.11019v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Xinwei Wu, Heng Liu, Xiaohu Zhao, Yuqi Ren, Linlong Xu, et al.</p><p>Large Language Models (LLMs) frequently exhibit strong translation abilities, even without task-specific fine-tuning. However, the internal mechanisms governing this innate capability remain largely opaque. To demystify this process, we leverage Sparse Autoencoders (SAEs) and introduce a novel framework for identifying task-specific features. Our method first recalls features that are frequently co-activated on translation inputs and then filters them for functional coherence using a PCA-based c</p><p><strong>Tags:</strong> SAE, features</p>]]></description>
      <pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.11019v1</guid>
    </item>
    <item>
      <title>Spurious Rewards Paradox: Mechanistically Understanding How RLVR Activates Memorization Shortcuts in LLMs</title>
      <link>https://arxiv.org/abs/2601.11061v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Lecheng Yan, Ruizhe Li, Guanhua Chen, Qing Li, Jiahui Geng, et al.</p><p>Reinforcement Learning with Verifiable Rewards (RLVR) is highly effective for enhancing LLM reasoning, yet recent evidence shows models like Qwen 2.5 achieve significant gains even with spurious or incorrect rewards. We investigate this phenomenon and identify a &quot;Perplexity Paradox&quot;: spurious RLVR triggers a divergence where answer-token perplexity drops while prompt-side coherence degrades, suggesting the model is bypassing reasoning in favor of memorization. Using Path Patching, Logit Lens, JS</p><p><strong>Tags:</strong> reasoning</p>]]></description>
      <pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.11061v1</guid>
    </item>
    <item>
      <title>TANDEM: Temporal-Aware Neural Detection for Multimodal Hate Speech</title>
      <link>https://arxiv.org/abs/2601.11178v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Girish A. Koushik, Helen Treharne, Diptesh Kanojia</p><p>Social media platforms are increasingly dominated by long-form multimodal content, where harmful narratives are constructed through a complex interplay of audio, visual, and textual cues. While automated systems can flag hate speech with high accuracy, they often function as &quot;black boxes&quot; that fail to provide the granular, interpretable evidence, such as precise timestamps and target identities, required for effective human-in-the-loop moderation. In this work, we introduce TANDEM, a unified fra</p><p><strong>Tags:</strong> safety, vision</p>]]></description>
      <pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.11178v1</guid>
    </item>
    <item>
      <title>Attesting Model Lineage by Consisted Knowledge Evolution with Fine-Tuning Trajectory</title>
      <link>https://arxiv.org/abs/2601.11683v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Zhuoyi Shang, Jiasen Li, Pengzhen Chen, Yanwei Liu, Xiaoyan Gu, et al.</p><p>The fine-tuning technique in deep learning gives rise to an emerging lineage relationship among models. This lineage provides a promising perspective for addressing security concerns such as unauthorized model redistribution and false claim of model provenance, which are particularly pressing in \textcolor{blue}{open-weight model} libraries where robust lineage verification mechanisms are often lacking. Existing approaches to model lineage detection primarily rely on static architectural similar</p>]]></description>
      <pubDate>Fri, 16 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.11683v1</guid>
    </item>
    <item>
      <title>Diagnosing Generalization Failures in Fine-Tuned LLMs: A Cross-Architectural Study on Phishing Detection</title>
      <link>https://arxiv.org/abs/2601.10524v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Frank Bobe, Gregory D. Vetaw, Chase Pavlick, Darshan Bryner, Matthew Cook, et al.</p><p>The practice of fine-tuning Large Language Models (LLMs) has achieved state-of-the-art performance on specialized tasks, yet diagnosing why these models become brittle and fail to generalize remains a critical open problem. To address this, we introduce and apply a multi-layered diagnostic framework to a cross-architectural study. We fine-tune Llama 3.1 8B, Gemma 2 9B, and Mistral models on a high-stakes phishing detection task and use SHAP analysis and mechanistic interpretability to uncover th</p>]]></description>
      <pubDate>Thu, 15 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.10524v1</guid>
    </item>
    <item>
      <title>Measuring Affinity between Attention-Head Weight Subspaces via the Projection Kernel</title>
      <link>https://arxiv.org/abs/2601.10266v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Hiroaki Yamagiwa, Yusuke Takase, Hidetoshi Shimodaira</p><p>Understanding relationships between attention heads is essential for interpreting the internal structure of Transformers, yet existing metrics do not capture this structure well. We focus on the subspaces spanned by attention-head weight matrices and quantify head-to-head relationships using the Projection Kernel (PK), a principal-angle-based measure of subspace similarity. Experiments show that PK reproduces known head-to-head interactions on the IOI task more clearly than prior metrics such as</p><p><strong>Tags:</strong> attention</p>]]></description>
      <pubDate>Thu, 15 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.10266v1</guid>
    </item>
    <item>
      <title>Reasoning Models Generate Societies of Thought</title>
      <link>https://arxiv.org/abs/2601.10825v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Junsol Kim, Shiyang Lai, Nino Scherrer, Blaise Agüera y Arcas, James Evans</p><p>Large language models have achieved remarkable capabilities across domains, yet mechanisms underlying sophisticated reasoning remain elusive. Recent reasoning models outperform comparable instruction-tuned models on complex cognitive tasks, attributed to extended computation through longer chains of thought. Here we show that enhanced reasoning emerges not from extended computation alone, but from simulating multi-agent-like interactions -- a society of thought -- which enables diversification a</p><p><strong>Tags:</strong> reasoning</p>]]></description>
      <pubDate>Thu, 15 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.10825v1</guid>
    </item>
    <item>
      <title>Where Knowledge Collides: A Mechanistic Study of Intra-Memory Knowledge Conflict in Language Models</title>
      <link>https://arxiv.org/abs/2601.09445v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Minh Vu Pham, Hsuvas Borkakoty, Yufang Hou</p><p>In language models (LMs), intra-memory knowledge conflict largely arises when inconsistent information about the same event is encoded within the model&#x27;s parametric knowledge. While prior work has primarily focused on resolving conflicts between a model&#x27;s internal knowledge and external resources through approaches such as fine-tuning or knowledge editing, the problem of localizing conflicts that originate during pre-training within the model&#x27;s internal representations remain unexplored. In this</p><p><strong>Tags:</strong> editing</p>]]></description>
      <pubDate>Wed, 14 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.09445v1</guid>
    </item>
    <item>
      <title>CLaS-Bench: A Cross-Lingual Alignment and Steering Benchmark</title>
      <link>https://arxiv.org/abs/2601.08331v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Daniil Gurgurov, Yusser Al Ghussin, Tanja Baeumel, Cheng-Ting Chou, Patrick Schramowski, et al.</p><p>Understanding and controlling the behavior of large language models (LLMs) is an increasingly important topic in multilingual NLP. Beyond prompting or fine-tuning, , i.e.,~manipulating internal representations during inference, has emerged as a more efficient and interpretable technique for adapting models to a target language. Yet, no dedicated benchmarks or evaluation protocols exist to quantify the effectiveness of steering techniques. We introduce CLaS-Bench, a lightweight parallel-question </p><p><strong>Tags:</strong> safety, steering</p>]]></description>
      <pubDate>Tue, 13 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.08331v1</guid>
    </item>
    <item>
      <title>Reasoning Beyond Chain-of-Thought: A Latent Computational Mode in Large Language Models</title>
      <link>https://arxiv.org/abs/2601.08058v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Zhenghao He, Guangzhi Xiong, Bohan Liu, Sanchit Sinha, Aidong Zhang</p><p>Chain-of-Thought (CoT) prompting has improved the reasoning performance of large language models (LLMs), but it remains unclear why it works and whether it is the unique mechanism for triggering reasoning in large language models. In this work, we study this question by directly analyzing and intervening on the internal representations of LLMs with Sparse Autoencoders (SAEs), identifying a small set of latent features that are causally associated with LLM reasoning behavior. Across multiple mode</p><p><strong>Tags:</strong> SAE, features, reasoning</p>]]></description>
      <pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.08058v1</guid>
    </item>
    <item>
      <title>Semantic Gravity Wells: Why Negative Constraints Backfire</title>
      <link>https://arxiv.org/abs/2601.08070v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Shailesh Rana</p><p>Negative constraints (instructions of the form &quot;do not use word X&quot;) represent a fundamental test of instruction-following capability in large language models. Despite their apparent simplicity, these constraints fail with striking regularity, and the conditions governing failure have remained poorly understood. This paper presents the first comprehensive mechanistic investigation of negative instruction failure. We introduce semantic pressure, a quantitative measure of the model&#x27;s intrinsic prob</p>]]></description>
      <pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.08070v1</guid>
    </item>
    <item>
      <title>Dynamical Systems Analysis Reveals Functional Regimes in Large Language Models</title>
      <link>https://arxiv.org/abs/2601.11622v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Hassan Ugail, Newton Howard</p><p>Large language models perform text generation through high-dimensional internal dynamics, yet the temporal organisation of these dynamics remains poorly understood. Most interpretability approaches emphasise static representations or causal interventions, leaving temporal structure largely unexplored. Drawing on neuroscience, where temporal integration and metastability are core markers of neural organisation, we adapt these concepts to transformer models and discuss a composite dynamical metric</p>]]></description>
      <pubDate>Sun, 11 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.11622v1</guid>
    </item>
  </channel>
</rss>
