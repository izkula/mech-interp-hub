<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Mechanistic Interpretability Hub</title>
    <link>https://izkula.github.io/cc</link>
    <description>Latest research in mechanistic interpretability - understanding how neural networks work internally</description>
    <language>en-us</language>
    <lastBuildDate>Tue, 06 Jan 2026 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://izkula.github.io/cc/feed.xml" rel="self" type="application/rss+xml"/>

    <item>
      <title>Central Dogma Transformer: Towards Mechanism-Oriented AI for Cellular Understanding</title>
      <link>https://arxiv.org/abs/2601.01089v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Nobuyuki Ota</p><p>Understanding cellular mechanisms requires integrating information across DNA, RNA, and protein - the three molecular systems linked by the Central Dogma of molecular biology. While domain-specific foundation models have achieved success for each modality individually, they remain isolated, limiting our ability to model integrated cellular processes. Here we present the Central Dogma Transformer (CDT), an architecture that integrates pre-trained language models for DNA, RNA, and protein followin</p><p><strong>Tags:</strong> biology</p>]]></description>
      <pubDate>Sat, 03 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.01089v1</guid>
    </item>
    <item>
      <title>Explainability-Guided Defense: Attribution-Aware Model Refinement Against Adversarial Data Attacks</title>
      <link>https://arxiv.org/abs/2601.00968v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Longwei Wang, Mohammad Navid Nayyem, Abdullah Al Rakin, KC Santosh, Chaowei Zhang, et al.</p><p>The growing reliance on deep learning models in safety-critical domains such as healthcare and autonomous navigation underscores the need for defenses that are both robust to adversarial perturbations and transparent in their decision-making. In this paper, we identify a connection between interpretability and robustness that can be directly leveraged during training. Specifically, we observe that spurious, unstable, or semantically irrelevant features identified through Local Interpretable Mode</p><p><strong>Tags:</strong> features, safety</p>]]></description>
      <pubDate>Fri, 02 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.00968v1</guid>
    </item>
    <item>
      <title>ActErase: A Training-Free Paradigm for Precise Concept Erasure via Activation Patching</title>
      <link>https://arxiv.org/abs/2601.00267v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Yi Sun, Xinhao Zhong, Hongyan Li, Yimin Zhou, Junhao Li, et al.</p><p>Recent advances in text-to-image diffusion models have demonstrated remarkable generation capabilities, yet they raise significant concerns regarding safety, copyright, and ethical implications. Existing concept erasure methods address these risks by removing sensitive concepts from pre-trained models, but most of them rely on data-intensive and computationally expensive fine-tuning, which poses a critical limitation. To overcome these challenges, inspired by the observation that the model&#x27;s act</p><p><strong>Tags:</strong> safety, vision</p>]]></description>
      <pubDate>Thu, 01 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2601.00267v1</guid>
    </item>
    <item>
      <title>Triangulation as an Acceptance Rule for Multilingual Mechanistic Interpretability</title>
      <link>https://arxiv.org/abs/2512.24842v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Yanan Long</p><p>Multilingual language models achieve strong aggregate performance yet often behave unpredictably across languages, scripts, and cultures. We argue that mechanistic explanations for such models should satisfy a \emph{causal} standard: claims must survive causal interventions and must \emph{cross-reference} across environments that perturb surface form while preserving meaning. We formalize \emph{reference families} as predicate-preserving variants and introduce \emph{triangulation}, an acceptance</p>]]></description>
      <pubDate>Wed, 31 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2512.24842v1</guid>
    </item>
    <item>
      <title>Attribution-Guided Distillation of Matryoshka Sparse Autoencoders</title>
      <link>https://arxiv.org/abs/2512.24975v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Cristina P. Martin-Linares, Jonathan P. Ling</p><p>Sparse autoencoders (SAEs) aim to disentangle model activations into monosemantic, human-interpretable features. In practice, learned features are often redundant and vary across training runs and sparsity levels, which makes interpretations difficult to transfer and reuse. We introduce Distilled Matryoshka Sparse Autoencoders (DMSAEs), a training pipeline that distills a compact core of consistently useful features and reuses it to train new SAEs. DMSAEs run an iterative distillation cycle: tra</p><p><strong>Tags:</strong> SAE, features</p>]]></description>
      <pubDate>Wed, 31 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2512.24975v1</guid>
    </item>
    <item>
      <title>MEIC-DT: Memory-Efficient Incremental Clustering for Long-Text Coreference Resolution with Dual-Threshold Constraints</title>
      <link>https://arxiv.org/abs/2512.24711v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Kangyang Luo, Shuzheng Si, Yuzhuo Bai, Cheng Gao, Zhitong Wang, et al.</p><p>In the era of large language models (LLMs), supervised neural methods remain the state-of-the-art (SOTA) for Coreference Resolution. Yet, their full potential is underexplored, particularly in incremental clustering, which faces the critical challenge of balancing efficiency with performance for long texts. To address the limitation, we propose \textbf{MEIC-DT}, a novel dual-threshold, memory-efficient incremental clustering approach based on a lightweight Transformer. MEIC-DT features a dual-th</p><p><strong>Tags:</strong> features</p>]]></description>
      <pubDate>Wed, 31 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2512.24711v1</guid>
    </item>
    <item>
      <title>Fantastic Reasoning Behaviors and Where to Find Them: Unsupervised Discovery of the Reasoning Process</title>
      <link>https://arxiv.org/abs/2512.23988v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Zhenyu Zhang, Shujian Zhang, John Lambert, Wenxuan Zhou, Zhangyang Wang, et al.</p><p>Despite the growing reasoning capabilities of recent large language models (LLMs), their internal mechanisms during the reasoning process remain underexplored. Prior approaches often rely on human-defined concepts (e.g., overthinking, reflection) at the word level to analyze reasoning in a supervised manner. However, such methods are limited, as it is infeasible to capture the full spectrum of potential reasoning behaviors, many of which are difficult to define in token space. In this work, we p</p><p><strong>Tags:</strong> reasoning</p>]]></description>
      <pubDate>Tue, 30 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2512.23988v1</guid>
    </item>
    <item>
      <title>Adversarial Lens: Exploiting Attention Layers to Generate Adversarial Examples for Evaluation</title>
      <link>https://arxiv.org/abs/2512.23837v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Kaustubh Dhole</p><p>Recent advances in mechanistic interpretability suggest that intermediate attention layers encode token-level hypotheses that are iteratively refined toward the final output. In this work, we exploit this property to generate adversarial examples directly from attention-layer token distributions. Unlike prompt-based or gradient-based attacks, our approach leverages model-internal token predictions, producing perturbations that are both plausible and internally consistent with the model&#x27;s own gen</p>]]></description>
      <pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2512.23837v1</guid>
    </item>
    <item>
      <title>Interpretable Safety Alignment via SAE-Constructed Low-Rank Subspace Adaptation</title>
      <link>https://arxiv.org/abs/2512.23260v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Dianyun Wang, Qingsen Ma, Yuhu Shang, Zhifeng Lu, Lechen Ning, et al.</p><p>Parameter-efficient fine-tuning has become the dominant paradigm for adapting large language models to downstream tasks. Low-rank adaptation methods such as LoRA operate under the assumption that task-relevant weight updates reside in a low-rank subspace, yet this subspace is learned implicitly from data in a black-box manner, offering no interpretability or direct control. We hypothesize that this difficulty stems from polysemanticity--individual dimensions encoding multiple entangled concepts.</p><p><strong>Tags:</strong> features, safety</p>]]></description>
      <pubDate>Mon, 29 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2512.23260v1</guid>
    </item>
    <item>
      <title>Mechanistic Analysis of Circuit Preservation in Federated Learning</title>
      <link>https://arxiv.org/abs/2512.23043v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Muhammad Haseeb, Salaar Masood, Muhammad Abdullah Sohail</p><p>Federated Learning (FL) enables collaborative training of models on decentralized data, but its performance degrades significantly under Non-IID (non-independent and identically distributed) data conditions. While this accuracy loss is well-documented, the internal mechanistic causes remain a black box. This paper investigates the canonical FedAvg algorithm through the lens of Mechanistic Interpretability (MI) to diagnose this failure mode. We hypothesize that the aggregation of conflicting clie</p><p><strong>Tags:</strong> circuits</p>]]></description>
      <pubDate>Sun, 28 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2512.23043v1</guid>
    </item>
    <item>
      <title>Decomposing Task Vectors for Refined Model Editing</title>
      <link>https://arxiv.org/abs/2512.22511v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Hamed Damirchi, Ehsan Abbasnejad, Zhen Zhang, Javen Shi</p><p>Large pre-trained models have transformed machine learning, yet adapting these models effectively to exhibit precise, concept-specific behaviors remains a significant challenge. Task vectors, defined as the difference between fine-tuned and pre-trained model parameters, provide a mechanism for steering neural networks toward desired behaviors. This has given rise to large repositories dedicated to task vectors tailored for specific behaviors. The arithmetic operation of these task vectors allows</p><p><strong>Tags:</strong> steering, editing</p>]]></description>
      <pubDate>Sat, 27 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2512.22511v1</guid>
    </item>
    <item>
      <title>The Deepfake Detective: Interpreting Neural Forensics Through Sparse Features and Manifolds</title>
      <link>https://arxiv.org/abs/2512.21670v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Subramanyam Sahoo, Jared Junkin</p><p>Deepfake detection models have achieved high accuracy in identifying synthetic media, but their decision processes remain largely opaque. In this paper we present a mechanistic interpretability framework for deepfake detection applied to a vision-language model. Our approach combines a sparse autoencoder (SAE) analysis of internal network representations with a novel forensic manifold analysis that probes how the model&#x27;s features respond to controlled forensic artifact manipulations. We demonstr</p><p><strong>Tags:</strong> SAE, features, probing, vision</p>]]></description>
      <pubDate>Thu, 25 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2512.21670v1</guid>
    </item>
    <item>
      <title>Omni-Weather: Unified Multimodal Foundation Model for Weather Generation and Understanding</title>
      <link>https://arxiv.org/abs/2512.21643v2</link>
      <description><![CDATA[<p><strong>Authors:</strong> Zhiwang Zhou, Yuandong Pu, Xuming He, Yidi Liu, Yixin Chen, et al.</p><p>Weather modeling requires both accurate prediction and mechanistic interpretation, yet existing methods treat these goals in isolation, separating generation from understanding. To address this gap, we present Omni-Weather, the first multimodal foundation model that unifies weather generation and understanding within a single architecture. Omni-Weather integrates a radar encoder for weather generation tasks, followed by unified processing using a shared self-attention mechanism. Moreover, we con</p><p><strong>Tags:</strong> vision</p>]]></description>
      <pubDate>Thu, 25 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2512.21643v2</guid>
    </item>
    <item>
      <title>Learning from Negative Examples: Why Warning-Framed Training Data Teaches What It Warns Against</title>
      <link>https://arxiv.org/abs/2512.22293v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Tsogt-Ochir Enkhbayar</p><p>Warning-framed content in training data (e.g., &quot;DO NOT USE - this code is vulnerable&quot;) does not, it turns out, teach language models to avoid the warned-against behavior. In experiments reported here, models exposed to such warnings reproduced the flagged content at rates statistically indistinguishable from models given the content directly (76.7% vs. 83.3%). Why? Sparse autoencoder analysis points to a failure of orthogonalization: &quot;describing X&quot; and &quot;performing X&quot; activate overlapping latent </p><p><strong>Tags:</strong> SAE</p>]]></description>
      <pubDate>Thu, 25 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2512.22293v1</guid>
    </item>
    <item>
      <title>Measuring Mechanistic Independence: Can Bias Be Removed Without Erasing Demographics?</title>
      <link>https://arxiv.org/abs/2512.20796v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Zhengyang Shan, Aaron Mueller</p><p>We investigate how independent demographic bias mechanisms are from general demographic recognition in language models. Using a multi-task evaluation setup where demographics are associated with names, professions, and education levels, we measure whether models can be debiased while preserving demographic detection capabilities. We compare attribution-based and correlation-based methods for locating bias features. We find that targeted sparse autoencoder feature ablations in Gemma-2-9B reduce b</p><p><strong>Tags:</strong> SAE, features</p>]]></description>
      <pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2512.20796v1</guid>
    </item>
    <item>
      <title>Hierarchical Geometry of Cognitive States in Transformer Embedding Spaces</title>
      <link>https://arxiv.org/abs/2512.22227v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Sophie Zhao</p><p>Recent work has shown that transformer-based language models learn rich geometric structure in their embedding spaces, yet the presence of higher-level cognitive organization within these representations remains underexplored. In this work, we investigate whether sentence embeddings encode a graded, hierarchical structure aligned with human-interpretable cognitive or psychological attributes. We construct a dataset of 480 natural-language sentences annotated with continuous ordinal energy scores</p>]]></description>
      <pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2512.22227v1</guid>
    </item>
    <item>
      <title>Toward Explaining Large Language Models in Software Engineering Tasks</title>
      <link>https://arxiv.org/abs/2512.20328v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Antonio Vitale, Khai-Nguyen Nguyen, Denys Poshyvanyk, Rocco Oliveto, Simone Scalabrino, et al.</p><p>Recent progress in Large Language Models (LLMs) has substantially advanced the automation of software engineering (SE) tasks, enabling complex activities such as code generation and code summarization. However, the black-box nature of LLMs remains a major barrier to their adoption in high-stakes and safety-critical domains, where explainability and transparency are vital for trust, accountability, and effective human supervision. Despite increasing interest in explainable AI for software enginee</p><p><strong>Tags:</strong> safety, vision</p>]]></description>
      <pubDate>Tue, 23 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2512.20328v1</guid>
    </item>
    <item>
      <title>Generative Giants, Retrieval Weaklings: Why do Multimodal Large Language Models Fail at Multimodal Retrieval?</title>
      <link>https://arxiv.org/abs/2512.19115v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Hengyi Feng, Zeang Sheng, Meiyi Qiang, Wentao Zhang</p><p>Despite the remarkable success of multimodal large language models (MLLMs) in generative tasks, we observe that they exhibit a counterintuitive deficiency in the zero-shot multimodal retrieval task. In this work, we investigate the underlying mechanisms that hinder MLLMs from serving as effective retrievers. With the help of sparse autoencoders (SAEs), we decompose MLLM output representations into interpretable semantic concepts to probe their intrinsic behavior. Our analysis reveals that the re</p><p><strong>Tags:</strong> SAE, probing, vision</p>]]></description>
      <pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2512.19115v1</guid>
    </item>
    <item>
      <title>LouvreSAE: Sparse Autoencoders for Interpretable and Controllable Style Transfer</title>
      <link>https://arxiv.org/abs/2512.18930v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Raina Panda, Daniel Fein, Arpita Singhal, Mark Fiore, Maneesh Agrawala, et al.</p><p>Artistic style transfer in generative models remains a significant challenge, as existing methods often introduce style only via model fine-tuning, additional adapters, or prompt engineering, all of which can be computationally expensive and may still entangle style with subject matter. In this paper, we introduce a training- and inference-light, interpretable method for representing and transferring artistic style. Our approach leverages an art-specific Sparse Autoencoder (SAE) on top of latent</p><p><strong>Tags:</strong> SAE</p>]]></description>
      <pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2512.18930v1</guid>
    </item>
    <item>
      <title>SAP: Syntactic Attention Pruning for Transformer-based Language Models</title>
      <link>https://arxiv.org/abs/2512.19125v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Tzu-Yun Lee, Ding-Yong Hong, Jan-Jan Wu</p><p>This paper introduces Syntactic Attention Pruning (SAP), a novel method for effectively pruning attention heads in Transformer models. Unlike conventional approaches that rely solely on mathematical analysis of model weights and activations, SAP incorporates both the syntactic structure and attention patterns of sentences to guide the pruning process. By leveraging these linguistic features, SAP not only achieves performance comparable to state-of-the-art methods but also enhances the interpreta</p><p><strong>Tags:</strong> features, attention</p>]]></description>
      <pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2512.19125v1</guid>
    </item>
    <item>
      <title>Faithful and Stable Neuron Explanations for Trustworthy Mechanistic Interpretability</title>
      <link>https://arxiv.org/abs/2512.18092v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Ge Yan, Tuomas Oikarinen,  Tsui-Wei,  Weng</p><p>Neuron identification is a popular tool in mechanistic interpretability, aiming to uncover the human-interpretable concepts represented by individual neurons in deep networks. While algorithms such as Network Dissection and CLIP-Dissect achieve great empirical success, a rigorous theoretical foundation remains absent, which is crucial to enable trustworthy and reliable explanations. In this work, we observe that neuron identification can be viewed as the inverse process of machine learning, whic</p><p><strong>Tags:</strong> theory</p>]]></description>
      <pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2512.18092v1</guid>
    </item>
    <item>
      <title>Task Schema and Binding: A Double Dissociation Study of In-Context Learning</title>
      <link>https://arxiv.org/abs/2512.17325v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Chaeha Kim</p><p>We provide causal mechanistic validation that in-context learning (ICL) decomposes into two separable mechanisms: Task Schema (abstract task type recognition) and Binding (specific input-output associations). Through activation patching experiments across 9 models from 7 Transformer families plus Mamba (370M-13B parameters), we establish three key findings:   1. Double dissociation: Task Schema transfers at 100% via late MLP patching; Binding transfers at 62% via residual stream patching -- prov</p>]]></description>
      <pubDate>Fri, 19 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2512.17325v1</guid>
    </item>
    <item>
      <title>SALVE: Sparse Autoencoder-Latent Vector Editing for Mechanistic Control of Neural Networks</title>
      <link>https://arxiv.org/abs/2512.15938v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Vegard Flovik</p><p>Deep neural networks achieve impressive performance but remain difficult to interpret and control. We present SALVE (Sparse Autoencoder-Latent Vector Editing), a unified &quot;discover, validate, and control&quot; framework that bridges mechanistic interpretability and model editing. Using an $\ell_1$-regularized autoencoder, we learn a sparse, model-native feature basis without supervision. We validate these features with Grad-FAM, a feature-level saliency mapping method that visually grounds latent feat</p><p><strong>Tags:</strong> SAE, features, editing, vision</p>]]></description>
      <pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2512.15938v1</guid>
    </item>
    <item>
      <title>The Deleuzian Representation Hypothesis</title>
      <link>https://arxiv.org/abs/2512.19734v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Clément Cornet, Romaric Besançon, Hervé Le Borgne</p><p>We propose an alternative to sparse autoencoders (SAEs) as a simple and effective unsupervised method for extracting interpretable concepts from neural networks. The core idea is to cluster differences in activations, which we formally justify within a discriminant analysis framework. To enhance the diversity of extracted concepts, we refine the approach by weighting the clustering using the skewness of activations. The method aligns with Deleuze&#x27;s modern view of concepts as differences. We eval</p><p><strong>Tags:</strong> SAE</p>]]></description>
      <pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2512.19734v1</guid>
    </item>
    <item>
      <title>From Isolation to Entanglement: When Do Interpretability Methods Identify and Disentangle Known Concepts?</title>
      <link>https://arxiv.org/abs/2512.15134v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Aaron Mueller, Andrew Lee, Shruti Joshi, Ekdeep Singh Lubana, Dhanya Sridhar, et al.</p><p>A central goal of interpretability is to recover representations of causally relevant concepts from the activations of neural networks. The quality of these concept representations is typically evaluated in isolation, and under implicit independence assumptions that may not hold in practice. Thus, it is unclear whether common featurization methods - including sparse autoencoders (SAEs) and sparse probes - recover disentangled representations of these concepts. This study proposes a multi-concept</p><p><strong>Tags:</strong> SAE, probing</p>]]></description>
      <pubDate>Wed, 17 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2512.15134v1</guid>
    </item>
    <item>
      <title>Task Matrices: Linear Maps for Cross-Model Finetuning Transfer</title>
      <link>https://arxiv.org/abs/2512.14880v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Darrin O&#x27; Brien, Dhikshith Gajulapalli, Eric Xia</p><p>Results in interpretability suggest that large vision and language models learn implicit linear encodings when models are biased by in-context prompting. However, the existence of similar linear representations in more general adaptation regimes has not yet been demonstrated. In this work, we develop the concept of a task matrix, a linear transformation from a base to finetuned embedding state. We demonstrate that for vision and text models and ten different datasets, a base model augmented with</p><p><strong>Tags:</strong> vision</p>]]></description>
      <pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2512.14880v1</guid>
    </item>
    <item>
      <title>ReflCtrl: Controlling LLM Reflection via Representation Engineering</title>
      <link>https://arxiv.org/abs/2512.13979v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Ge Yan, Chung-En Sun,  Tsui-Wei,  Weng</p><p>Large language models (LLMs) with Chain-of-Thought (CoT) reasoning have achieved strong performance across diverse tasks, including mathematics, coding, and general reasoning. A distinctive ability of these reasoning models is self-reflection: the ability to review and revise previous reasoning steps. While self-reflection enhances reasoning performance, it also increases inference cost. In this work, we study self-reflection through the lens of representation engineering. We segment the model&#x27;s</p><p><strong>Tags:</strong> survey, reasoning</p>]]></description>
      <pubDate>Tue, 16 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2512.13979v1</guid>
    </item>
    <item>
      <title>AI Epidemiology: achieving explainable AI through expert oversight patterns</title>
      <link>https://arxiv.org/abs/2512.15783v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Kit Tempest-Walters</p><p>AI Epidemiology is a framework for governing and explaining advanced AI systems by applying population-level surveillance methods to AI outputs. The approach mirrors the way in which epidemiologists enable public health interventions through statistical evidence before molecular mechanisms are understood. This bypasses the problem of model complexity which plagues current interpretability methods (such as SHAP and mechanistic interpretability) at the scale of deployed models.   AI Epidemiology a</p>]]></description>
      <pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2512.15783v1</guid>
    </item>
    <item>
      <title>Superposition as Lossy Compression: Measure with Sparse Autoencoders and Connect to Adversarial Vulnerability</title>
      <link>https://arxiv.org/abs/2512.13568v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Leonard Bereska, Zoe Tzifa-Kratira, Reza Samavi, Efstratios Gavves</p><p>Neural networks achieve remarkable performance through superposition: encoding multiple features as overlapping directions in activation space rather than dedicating individual neurons to each feature. This challenges interpretability, yet we lack principled methods to measure superposition. We present an information-theoretic framework measuring a neural representation&#x27;s effective degrees of freedom. We apply Shannon entropy to sparse autoencoder activations to compute the number of effective f</p><p><strong>Tags:</strong> SAE, superposition, features</p>]]></description>
      <pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2512.13568v1</guid>
    </item>
    <item>
      <title>XNNTab -- Interpretable Neural Networks for Tabular Data using Sparse Autoencoders</title>
      <link>https://arxiv.org/abs/2512.13442v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Khawla Elhadri, Jörg Schlötterer, Christin Seifert</p><p>In data-driven applications relying on tabular data, where interpretability is key, machine learning models such as decision trees and linear regression are applied. Although neural networks can provide higher predictive performance, they are not used because of their blackbox nature. In this work, we present XNNTab, a neural architecture that combines the expressiveness of neural networks and interpretability. XNNTab first learns highly non-linear feature representations, which are decomposed i</p><p><strong>Tags:</strong> SAE, features</p>]]></description>
      <pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2512.13442v1</guid>
    </item>
  </channel>
</rss>
