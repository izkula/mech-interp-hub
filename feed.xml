<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Mechanistic Interpretability Hub</title>
    <link>https://izkula.github.io/cc</link>
    <description>Latest research in mechanistic interpretability - understanding how neural networks work internally</description>
    <language>en-us</language>
    <lastBuildDate>Thu, 12 Feb 2026 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://izkula.github.io/cc/feed.xml" rel="self" type="application/rss+xml"/>

    <item>
      <title>From Circuits to Dynamics: Understanding and Stabilizing Failure in 3D Diffusion Transformers</title>
      <link>https://arxiv.org/abs/2602.11130v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Maximilian Plattner, Fabian Paischer, Johannes Brandstetter, Arturs Berzins</p><p>Reliable surface completion from sparse point clouds underpins many applications spanning content creation and robotics. While 3D diffusion transformers attain state-of-the-art results on this task, we uncover that they exhibit a catastrophic mode of failure: arbitrarily small on-surface perturbations to the input point cloud can fracture the output into multiple disconnected pieces -- a phenomenon we call Meltdown. Using activation-patching from mechanistic interpretability, we localize Meltdow</p><p><strong>Tags:</strong> circuits</p>]]></description>
      <pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.11130v1</guid>
    </item>
    <item>
      <title>Control Reinforcement Learning: Token-Level Mechanistic Analysis via Learned SAE Feature Steering</title>
      <link>https://arxiv.org/abs/2602.10437v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Seonglae Cho, Zekun Wu, Adriano Koshiyama</p><p>Sparse autoencoders (SAEs) decompose language model activations into interpretable features, but existing methods reveal only which features activate, not which change model outputs when amplified. We introduce Control Reinforcement Learning (CRL), which trains a policy to select SAE features for steering at each token, producing interpretable intervention logs: the learned policy identifies features that change model outputs when amplified. Adaptive Feature Masking encourages diverse feature di</p><p><strong>Tags:</strong> SAE, features, steering</p>]]></description>
      <pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.10437v1</guid>
    </item>
    <item>
      <title>Less is Enough: Synthesizing Diverse Data in Feature Space of LLMs</title>
      <link>https://arxiv.org/abs/2602.10388v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Zhongzhi Li, Xuansheng Wu, Yijiang Li, Lijie Hu, Ninghao Liu</p><p>The diversity of post-training data is critical for effective downstream performance in large language models (LLMs). Many existing approaches to constructing post-training data quantify diversity using text-based metrics that capture linguistic variation, but such metrics provide only weak signals for the task-relevant features that determine downstream performance. In this work, we introduce Feature Activation Coverage (FAC) which measures data diversity in an interpretable feature space. Buil</p><p><strong>Tags:</strong> features</p>]]></description>
      <pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.10388v1</guid>
    </item>
    <item>
      <title>Med-SegLens: Latent-Level Model Diffing for Interpretable Medical Image Segmentation</title>
      <link>https://arxiv.org/abs/2602.10508v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Salma J. Ahmed, Emad A. Mohammed, Azam Asilian Bidgoli</p><p>Modern segmentation models achieve strong predictive performance but remain largely opaque, limiting our ability to diagnose failures, understand dataset shift, or intervene in a principled manner. We introduce Med-SegLens, a model-diffing framework that decomposes segmentation model activations into interpretable latent features using sparse autoencoders trained on SegFormer and U-Net. Through cross-architecture and cross-dataset latent alignment across healthy, adult, pediatric, and sub-Sahara</p><p><strong>Tags:</strong> SAE, features, safety, vision</p>]]></description>
      <pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.10508v1</guid>
    </item>
    <item>
      <title>Triggers Hijack Language Circuits: A Mechanistic Analysis of Backdoor Behaviors in Large Language Models</title>
      <link>https://arxiv.org/abs/2602.10382v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Théo Lasnier, Wissam Antoun, Francis Kulumba, Djamé Seddah</p><p>Backdoor attacks pose significant security risks for Large Language Models (LLMs), yet the internal mechanisms by which triggers operate remain poorly understood. We present the first mechanistic analysis of language-switching backdoors, studying the GAPperon model family (1B, 8B, 24B parameters) which contains triggers injected during pretraining that cause output language switching. Using activation patching, we localize trigger formation to early layers (7.5-25% of model depth) and identify w</p><p><strong>Tags:</strong> circuits</p>]]></description>
      <pubDate>Wed, 11 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.10382v1</guid>
    </item>
    <item>
      <title>Simple LLM Baselines are Competitive for Model Diffing</title>
      <link>https://arxiv.org/abs/2602.10371v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Elias Kempf, Simon Schrodi, Bartosz Cywiński, Thomas Brox, Neel Nanda, et al.</p><p>Standard LLM evaluations only test capabilities or dispositions that evaluators designed them for, missing unexpected differences such as behavioral shifts between model revisions or emergent misaligned tendencies. Model diffing addresses this limitation by automatically surfacing systematic behavioral differences. Recent approaches include LLM-based methods that generate natural language descriptions and sparse autoencoder (SAE)-based methods that identify interpretable features. However, no sy</p><p><strong>Tags:</strong> SAE, features, vision</p>]]></description>
      <pubDate>Tue, 10 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.10371v1</guid>
    </item>
    <item>
      <title>Learning Self-Interpretation from Interpretability Artifacts: Training Lightweight Adapters on Vector-Label Pairs</title>
      <link>https://arxiv.org/abs/2602.10352v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Keenan Pepper, Alex McKenzie, Florin Pop, Stijn Servaes, Martin Leitgab, et al.</p><p>Self-interpretation methods prompt language models to describe their own internal states, but remain unreliable due to hyperparameter sensitivity. We show that training lightweight adapters on interpretability artifacts, while keeping the LM entirely frozen, yields reliable self-interpretation across tasks and model families. A scalar affine adapter with just $d_\text{model}+1$ parameters suffices: trained adapters generate sparse autoencoder feature labels that outperform the training labels th</p><p><strong>Tags:</strong> SAE, features</p>]]></description>
      <pubDate>Tue, 10 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.10352v1</guid>
    </item>
    <item>
      <title>Why Linear Interpretability Works: Invariant Subspaces as a Result of Architectural Constraints</title>
      <link>https://arxiv.org/abs/2602.09783v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Andres Saurez, Yousung Lee, Dongsoo Har</p><p>Linear probes and sparse autoencoders consistently recover meaningful structure from transformer representations -- yet why should such simple methods succeed in deep, nonlinear systems? We show this is not merely an empirical regularity but a consequence of architectural necessity: transformers communicate information through linear interfaces (attention OV circuits, unembedding matrices), and any semantic feature decoded through such an interface must occupy a context-invariant linear subspace</p><p><strong>Tags:</strong> SAE, circuits, features, probing</p>]]></description>
      <pubDate>Tue, 10 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.09783v1</guid>
    </item>
    <item>
      <title>Circuit Fingerprints: How Answer Tokens Encode Their Geometrical Path</title>
      <link>https://arxiv.org/abs/2602.09784v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Andres Saurez, Neha Sengar, Dongsoo Har</p><p>Circuit discovery and activation steering in transformers have developed as separate research threads, yet both operate on the same representational space. Are they two views of the same underlying structure? We show they follow a single geometric principle: answer tokens, processed in isolation, encode the directions that would produce them. This Circuit Fingerprint hypothesis enables circuit discovery without gradients or causal intervention -- recovering comparable structure to gradient-based</p><p><strong>Tags:</strong> circuits, steering</p>]]></description>
      <pubDate>Tue, 10 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.09784v1</guid>
    </item>
    <item>
      <title>How Do Language Models Understand Tables? A Mechanistic Analysis of Cell Location</title>
      <link>https://arxiv.org/abs/2602.08548v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Xuanliang Zhang, Dingzirui Wang, Keyan Xu, Qingfu Zhu, Wanxiang Che</p><p>While Large Language Models (LLMs) are increasingly deployed for table-related tasks, the internal mechanisms enabling them to process linearized two-dimensional structured tables remain opaque. In this work, we investigate the process of table understanding by dissecting the atomic task of cell location. Through activation patching and complementary interpretability techniques, we delineate the table understanding mechanism into a sequential three-stage pipeline: Semantic Binding, Coordinate Lo</p>]]></description>
      <pubDate>Mon, 09 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.08548v1</guid>
    </item>
    <item>
      <title>Patches of Nonlinearity: Instruction Vectors in Large Language Models</title>
      <link>https://arxiv.org/abs/2602.07930v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Irina Bigoulaeva, Jonas Rohweder, Subhabrata Dutta, Iryna Gurevych</p><p>Despite the recent success of instruction-tuned language models and their ubiquitous usage, very little is known of how models process instructions internally. In this work, we address this gap from a mechanistic point of view by investigating how instruction-specific representations are constructed and utilized in different stages of post-training: Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO). Via causal mediation, we identify that instruction representation is fairly l</p>]]></description>
      <pubDate>Sun, 08 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.07930v1</guid>
    </item>
    <item>
      <title>LUCID-SAE: Learning Unified Vision-Language Sparse Codes for Interpretable Concept Discovery</title>
      <link>https://arxiv.org/abs/2602.07311v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Difei Gu, Yunhe Gao, Gerasimos Chatzoudis, Zihan Dong, Guoning Zhang, et al.</p><p>Sparse autoencoders (SAEs) offer a natural path toward comparable explanations across different representation spaces. However, current SAEs are trained per modality, producing dictionaries whose features are not directly understandable and whose explanations do not transfer across domains. In this study, we introduce LUCID (Learning Unified vision-language sparse Codes for Interpretable concept Discovery), a unified vision-language sparse autoencoder that learns a shared latent dictionary for i</p><p><strong>Tags:</strong> SAE, features, vision</p>]]></description>
      <pubDate>Sat, 07 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.07311v1</guid>
    </item>
    <item>
      <title>The Quantum Sieve Tracer: A Hybrid Framework for Layer-Wise Activation Tracing in Large Language Models</title>
      <link>https://arxiv.org/abs/2602.06852v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Jonathan Pan</p><p>Mechanistic interpretability aims to reverse-engineer the internal computations of Large Language Models (LLMs), yet separating sparse semantic signals from high-dimensional polysemantic noise remains a significant challenge. This paper introduces the Quantum Sieve Tracer, a hybrid quantum-classical framework designed to characterize factual recall circuits. We implement a modular pipeline that first localizes critical layers using classical causal tracing, then maps specific attention head acti</p><p><strong>Tags:</strong> circuits, features, attention</p>]]></description>
      <pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.06852v1</guid>
    </item>
    <item>
      <title>Learning a Generative Meta-Model of LLM Activations</title>
      <link>https://arxiv.org/abs/2602.06964v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Grace Luo, Jiahai Feng, Trevor Darrell, Alec Radford, Jacob Steinhardt</p><p>Existing approaches for analyzing neural network activations, such as PCA and sparse autoencoders, rely on strong structural assumptions. Generative models offer an alternative: they can uncover structure without such assumptions and act as priors that improve intervention fidelity. We explore this direction by training diffusion models on one billion residual stream activations, creating &quot;meta-models&quot; that learn the distribution of a network&#x27;s internal states. We find that diffusion loss decrea</p><p><strong>Tags:</strong> SAE</p>]]></description>
      <pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.06964v1</guid>
    </item>
    <item>
      <title>Endogenous Resistance to Activation Steering in Language Models</title>
      <link>https://arxiv.org/abs/2602.06941v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Alex McKenzie, Keenan Pepper, Stijn Servaes, Martin Leitgab, Murat Cubuktepe, et al.</p><p>Large language models can resist task-misaligned activation steering during inference, sometimes recovering mid-generation to produce improved responses even when steering remains active. We term this Endogenous Steering Resistance (ESR). Using sparse autoencoder (SAE) latents to steer model activations, we find that Llama-3.3-70B shows substantial ESR, while smaller models from the Llama-3 and Gemma-2 families exhibit the phenomenon less frequently. We identify 26 SAE latents that activate diff</p><p><strong>Tags:</strong> SAE, steering</p>]]></description>
      <pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.06941v1</guid>
    </item>
    <item>
      <title>Towards Understanding Multimodal Fine-Tuning: Spatial Features</title>
      <link>https://arxiv.org/abs/2602.08713v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Lachin Naghashyar, Hunar Batra, Ashkan Khakzar, Philip Torr, Ronald Clark, et al.</p><p>Contemporary Vision-Language Models (VLMs) achieve strong performance on a wide range of tasks by pairing a vision encoder with a pre-trained language model, fine-tuned for visual-text inputs. Yet despite these gains, it remains unclear how language backbone representations adapt during multimodal training and when vision-specific capabilities emerge. In this work, we present the first mechanistic analysis of VLM adaptation. Using stage-wise model diffing, a technique that isolates representatio</p><p><strong>Tags:</strong> features, vision</p>]]></description>
      <pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.08713v1</guid>
    </item>
    <item>
      <title>CodeCircuit: Toward Inferring LLM-Generated Code Correctness via Attribution Graphs</title>
      <link>https://arxiv.org/abs/2602.07080v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Yicheng He, Zheng Zhao, Zhou Kaiyu, Bryan Dai, Jie Fu, et al.</p><p>Current paradigms for code verification rely heavily on external mechanisms-such as execution-based unit tests or auxiliary LLM judges-which are often labor-intensive or limited by the judging model&#x27;s own capabilities. This raises a fundamental, yet unexplored question: Can an LLM&#x27;s functional correctness be assessed purely from its internal computational structure? Our primary objective is to investigate whether the model&#x27;s neural dynamics encode internally decodable signals that are predictive</p><p><strong>Tags:</strong> circuits</p>]]></description>
      <pubDate>Fri, 06 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.07080v1</guid>
    </item>
    <item>
      <title>DLM-Scope: Mechanistic Interpretability of Diffusion Language Models via Sparse Autoencoders</title>
      <link>https://arxiv.org/abs/2602.05859v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Xu Wang, Bingqing Jiang, Yu Wan, Baosong Yang, Lingpeng Kong, et al.</p><p>Sparse autoencoders (SAEs) have become a standard tool for mechanistic interpretability in autoregressive large language models (LLMs), enabling researchers to extract sparse, human-interpretable features and intervene on model behavior. Recently, as diffusion language models (DLMs) have become an increasingly promising alternative to the autoregressive LLMs, it is essential to develop tailored mechanistic interpretability tools for this emerging class of models. In this work, we present DLM-Sco</p><p><strong>Tags:</strong> SAE, features</p>]]></description>
      <pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.05859v1</guid>
    </item>
    <item>
      <title>Split Personality Training: Revealing Latent Knowledge Through Alternate Personalities</title>
      <link>https://arxiv.org/abs/2602.05532v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Florian Dietz, William Wale, Oscar Gilg, Robert McCarthy, Felix Michalak, et al.</p><p>Detecting misalignment in large language models is challenging because models may learn to conceal misbehavior during training. Standard auditing techniques fall short: black-box methods often cannot distinguish misaligned outputs from benign ones, and mechanistic interpretability does not scale with model capabilities. We introduce Split Personality Training (SPT), which fine-tunes a second ``honest persona&#x27;&#x27; into LoRA parameters that remain inactive during normal operation. After the main mode</p><p><strong>Tags:</strong> safety</p>]]></description>
      <pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.05532v1</guid>
    </item>
    <item>
      <title>Causal Front-Door Adjustment for Robust Jailbreak Attacks on LLMs</title>
      <link>https://arxiv.org/abs/2602.05444v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Yao Zhou, Zeen Song, Wenwen Qiang, Fengge Wu, Shuyi Zhou, et al.</p><p>Safety alignment mechanisms in Large Language Models (LLMs) often operate as latent internal states, obscuring the model&#x27;s inherent capabilities. Building on this observation, we model the safety mechanism as an unobserved confounder from a causal perspective. Then, we propose the \textbf{C}ausal \textbf{F}ront-Door \textbf{A}djustment \textbf{A}ttack ({\textbf{CFA}}$^2$) to jailbreak LLM, which is a framework that leverages Pearl&#x27;s Front-Door Criterion to sever the confounding associations for </p><p><strong>Tags:</strong> safety</p>]]></description>
      <pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.05444v1</guid>
    </item>
    <item>
      <title>Advancing Opinion Dynamics Modeling with Neural Diffusion-Convection-Reaction Equation</title>
      <link>https://arxiv.org/abs/2602.05403v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Chenghua Gong, Yihang Jiang, Hao Li, Rui Sun, Juyuan Zhang, et al.</p><p>Advanced opinion dynamics modeling is vital for deciphering social behavior, emphasizing its role in mitigating polarization and securing cyberspace. To synergize mechanistic interpretability with data-driven flexibility, recent studies have explored the integration of Physics-Informed Neural Networks (PINNs) for opinion modeling. Despite this promise, existing methods are tailored to incomplete priors, lacking a comprehensive physical system to integrate dynamics from local, global, and endogen</p>]]></description>
      <pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.05403v1</guid>
    </item>
    <item>
      <title>Data-Centric Interpretability for LLM-based Multi-Agent Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2602.05183v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> John Yan, Michael Yu, Yuqi Sun, Alexander Duffy, Tyler Marques, et al.</p><p>Large language models (LLMs) are increasingly trained in complex Reinforcement Learning, multi-agent environments, making it difficult to understand how behavior changes over training. Sparse Autoencoders (SAEs) have recently shown to be useful for data-centric interpretability. In this work, we analyze large-scale reinforcement learning training runs from the sophisticated environment of Full-Press Diplomacy by applying pretrained SAEs, alongside LLM-summarizer methods. We introduce Meta-Autoin</p><p><strong>Tags:</strong> SAE</p>]]></description>
      <pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.05183v1</guid>
    </item>
    <item>
      <title>Cross-Modal Redundancy and the Geometry of Vision-Language Embeddings</title>
      <link>https://arxiv.org/abs/2602.06218v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Grégoire Dhimoïla, Thomas Fel, Victor Boutin, Agustin Picard</p><p>Vision-language models (VLMs) align images and text with remarkable success, yet the geometry of their shared embedding space remains poorly understood. To probe this geometry, we begin from the Iso-Energy Assumption, which exploits cross-modal redundancy: a concept that is truly shared should exhibit the same average energy across modalities. We operationalize this assumption with an Aligned Sparse Autoencoder (SAE) that encourages energy consistency during training while preserving reconstruct</p><p><strong>Tags:</strong> SAE, probing, vision</p>]]></description>
      <pubDate>Thu, 05 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.06218v1</guid>
    </item>
    <item>
      <title>Disentangling meaning from language in LLM-based machine translation</title>
      <link>https://arxiv.org/abs/2602.04613v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Théo Lasnier, Armel Zebaze, Djamé Seddah, Rachel Bawden, Benoît Sagot</p><p>Mechanistic Interpretability (MI) seeks to explain how neural networks implement their capabilities, but the scale of Large Language Models (LLMs) has limited prior MI work in Machine Translation (MT) to word-level analyses. We study sentence-level MT from a mechanistic perspective by analyzing attention heads to understand how LLMs internally encode and distribute translation functions. We decompose MT into two subtasks: producing text in the target language (i.e. target language identification</p><p><strong>Tags:</strong> attention</p>]]></description>
      <pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.04613v1</guid>
    </item>
    <item>
      <title>Identifying Intervenable and Interpretable Features via Orthogonality Regularization</title>
      <link>https://arxiv.org/abs/2602.04718v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Moritz Miller, Florent Draye, Bernhard Schölkopf</p><p>With recent progress on fine-tuning language models around a fixed sparse autoencoder, we disentangle the decoder matrix into almost orthogonal features. This reduces interference and superposition between the features, while keeping performance on the target dataset essentially unchanged. Our orthogonality penalty leads to identifiable features, ensuring the uniqueness of the decomposition. Further, we find that the distance between embedded feature explanations increases with stricter orthogon</p><p><strong>Tags:</strong> SAE, superposition, features</p>]]></description>
      <pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.04718v1</guid>
    </item>
    <item>
      <title>Decomposing Query-Key Feature Interactions Using Contrastive Covariances</title>
      <link>https://arxiv.org/abs/2602.04752v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Andrew Lee, Yonatan Belinkov, Fernanda Viégas, Martin Wattenberg</p><p>Despite the central role of attention heads in Transformers, we lack tools to understand why a model attends to a particular token. To address this, we study the query-key (QK) space -- the bilinear joint embedding space between queries and keys. We present a contrastive covariance method to decompose the QK space into low-rank, human-interpretable components. It is when features in keys and queries align in these low-rank subspaces that high attention scores are produced. We first study our met</p><p><strong>Tags:</strong> features, attention</p>]]></description>
      <pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.04752v1</guid>
    </item>
    <item>
      <title>AudioSAE: Towards Understanding of Audio-Processing Models with Sparse AutoEncoders</title>
      <link>https://arxiv.org/abs/2602.05027v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Georgii Aparin, Tasnima Sadekova, Alexey Rukhovich, Assel Yermekova, Laida Kushnareva, et al.</p><p>Sparse Autoencoders (SAEs) are powerful tools for interpreting neural representations, yet their use in audio remains underexplored. We train SAEs across all encoder layers of Whisper and HuBERT, provide an extensive evaluation of their stability, interpretability, and show their practical utility. Over 50% of the features remain consistent across random seeds, and reconstruction quality is preserved. SAE features capture general acoustic and semantic information as well as specific events, incl</p><p><strong>Tags:</strong> SAE, features</p>]]></description>
      <pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.05027v1</guid>
    </item>
    <item>
      <title>ASA: Training-Free Representation Engineering for Tool-Calling Agents</title>
      <link>https://arxiv.org/abs/2602.04935v2</link>
      <description><![CDATA[<p><strong>Authors:</strong> Youjin Wang, Run Zhou, Rong Fu, Shuaishuai Cao, Hongwei Zeng, et al.</p><p>Adapting LLM agents to domain-specific tool calling remains notably brittle under evolving interfaces. Prompt and schema engineering is easy to deploy but often fragile under distribution shift and strict parsers, while continual parameter-efficient fine-tuning improves reliability at the cost of training, maintenance, and potential forgetting. We identify a critical Lazy Agent failure mode where tool necessity is nearly perfectly decodable from mid-layer activations, yet the model remains conse</p>]]></description>
      <pubDate>Wed, 04 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.04935v2</guid>
    </item>
    <item>
      <title>Explaining the Explainer: Understanding the Inner Workings of Transformer-based Symbolic Regression Models</title>
      <link>https://arxiv.org/abs/2602.03506v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Arco van Breda, Erman Acar</p><p>Following their success across many domains, transformers have also proven effective for symbolic regression (SR); however, the internal mechanisms underlying their generation of mathematical operators remain largely unexplored. Although mechanistic interpretability has successfully identified circuits in language and vision models, it has not yet been applied to SR. In this article, we introduce PATCHES, an evolutionary circuit discovery algorithm that identifies compact and correct circuits fo</p><p><strong>Tags:</strong> circuits, vision</p>]]></description>
      <pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.03506v1</guid>
    </item>
    <item>
      <title>CSR-Bench: A Benchmark for Evaluating the Cross-modal Safety and Reliability of MLLMs</title>
      <link>https://arxiv.org/abs/2602.03263v1</link>
      <description><![CDATA[<p><strong>Authors:</strong> Yuxuan Liu, Yuntian Shi, Kun Wang, Haoting Shen, Kun Yang</p><p>Multimodal large language models (MLLMs) enable interaction over both text and images, but their safety behavior can be driven by unimodal shortcuts instead of true joint intent understanding. We introduce CSR-Bench, a benchmark for evaluating cross-modal reliability through four stress-testing interaction patterns spanning Safety, Over-rejection, Bias, and Hallucination, covering 61 fine-grained types. Each instance is constructed to require integrated image-text interpretation, and we addition</p><p><strong>Tags:</strong> safety, vision</p>]]></description>
      <pubDate>Tue, 03 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://arxiv.org/abs/2602.03263v1</guid>
    </item>
  </channel>
</rss>
