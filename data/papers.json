{
  "lastUpdated": "2026-02-22",
  "papers": [
    {
      "id": "arxiv-2602-17532v1",
      "title": "Systematic Evaluation of Single-Cell Foundation Model Interpretability Reveals Attention Captures Co-Expression Rather Than Unique Regulatory Signal",
      "authors": "Ihor Kendiukhov",
      "date": "2026-02-19",
      "url": "https://arxiv.org/abs/2602.17532v1",
      "abstract": "We present a systematic evaluation framework - thirty-seven analyses, 153 statistical tests, four cell types, two perturbation modalities - for assessing mechanistic interpretability in single-cell foundation models. Applying this framework to scGPT and Geneformer, we find that attention patterns encode structured biological information with layer-specific organisation - protein-protein interactions in early layers, transcriptional regulation in late layers - but this structure provides no incre",
      "tags": [
        "attention",
        "biology"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2602-17229v1",
      "title": "Mechanistic Interpretability of Cognitive Complexity in LLMs via Linear Probing using Bloom's Taxonomy",
      "authors": "Bianca Raimondi, Maurizio Gabbrielli",
      "date": "2026-02-19",
      "url": "https://arxiv.org/abs/2602.17229v1",
      "abstract": "The black-box nature of Large Language Models necessitates novel evaluation frameworks that transcend surface-level performance metrics. This study investigates the internal neural representations of cognitive complexity using Bloom's Taxonomy as a hierarchical lens. By analyzing high-dimensional activation vectors from different LLMs, we probe whether different cognitive levels, ranging from basic recall (Remember) to abstract synthesis (Create), are linearly separable within the model's residu",
      "tags": [
        "probing"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2602-16980v1",
      "title": "Discovering Universal Activation Directions for PII Leakage in Language Models",
      "authors": "Leo Marchyok, Zachary Coalson, Sungho Keum, Sooel Son, Sanghyun Hong",
      "date": "2026-02-19",
      "url": "https://arxiv.org/abs/2602.16980v1",
      "abstract": "Modern language models exhibit rich internal structure, yet little is known about how privacy-sensitive behaviors, such as personally identifiable information (PII) leakage, are represented and modulated within their hidden states. We present UniLeak, a mechanistic-interpretability framework that identifies universal activation directions: latent directions in a model's residual stream whose linear addition at inference time consistently increases the likelihood of generating PII across prompts.",
      "tags": [],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2602-17598v1",
      "title": "The Cascade Equivalence Hypothesis: When Do Speech LLMs Behave Like ASR$\\rightarrow$LLM Pipelines?",
      "authors": "Jayadev Billa",
      "date": "2026-02-19",
      "url": "https://arxiv.org/abs/2602.17598v1",
      "abstract": "Current speech LLMs largely perform implicit ASR: on tasks solvable from a transcript, they are behaviorally and mechanistically equivalent to simple Whisper$\\to$LLM cascades. We show this through matched-backbone testing across four speech LLMs and six tasks, controlling for the LLM backbone for the first time. Ultravox is statistically indistinguishable from its matched cascade ($\u03ba{=}0.93$); logit lens reveals literal text emerging in hidden states; LEACE concept erasure confirms text represen",
      "tags": [],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2602-17560v1",
      "title": "ODESteer: A Unified ODE-Based Steering Framework for LLM Alignment",
      "authors": "Hongjue Zhao, Haosen Sun, Jiangtao Kong, Xiaochang Li, Qineng Wang, et al.",
      "date": "2026-02-19",
      "url": "https://arxiv.org/abs/2602.17560v1",
      "abstract": "Activation steering, or representation engineering, offers a lightweight approach to align large language models (LLMs) by manipulating their internal activations at inference time. However, current methods suffer from two key limitations: \\textit{(i)} the lack of a unified theoretical framework for guiding the design of steering directions, and \\textit{(ii)} an over-reliance on \\textit{one-step steering} that fail to capture complex patterns of activation distributions. In this work, we propose",
      "tags": [
        "safety",
        "steering",
        "theory"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2602-16698v1",
      "title": "Causality is Key for Interpretability Claims to Generalise",
      "authors": "Shruti Joshi, Aaron Mueller, David Klindt, Wieland Brendel, Patrik Reizinger, et al.",
      "date": "2026-02-18",
      "url": "https://arxiv.org/abs/2602.16698v1",
      "abstract": "Interpretability research on large language models (LLMs) has yielded important insights into model behaviour, yet recurring pitfalls persist: findings that do not generalise, and causal interpretations that outrun the evidence. Our position is that causal inference specifies what constitutes a valid mapping from model activations to invariant high-level structures, the data or assumptions needed to achieve it, and the inferences it can support. Specifically, Pearl's causal hierarchy clarifies w",
      "tags": [],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2602-16849v1",
      "title": "On the Mechanism and Dynamics of Modular Addition: Fourier Features, Lottery Ticket, and Grokking",
      "authors": "Jianliang He, Leda Wang, Siyu Chen, Zhuoran Yang",
      "date": "2026-02-18",
      "url": "https://arxiv.org/abs/2602.16849v1",
      "abstract": "We present a comprehensive analysis of how two-layer neural networks learn features to solve the modular addition task. Our work provides a full mechanistic interpretation of the learned model and a theoretical explanation of its training dynamics. While prior work has identified that individual neurons learn single-frequency Fourier features and phase alignment, it does not fully explain how these features combine into a global solution. We bridge this gap by formalizing a diversification condi",
      "tags": [
        "features",
        "safety",
        "theory"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2602-16823v1",
      "title": "Formal Mechanistic Interpretability: Automated Circuit Discovery with Provable Guarantees",
      "authors": "Itamar Hadad, Guy Katz, Shahaf Bassan",
      "date": "2026-02-18",
      "url": "https://arxiv.org/abs/2602.16823v1",
      "abstract": "*Automated circuit discovery* is a central tool in mechanistic interpretability for identifying the internal components of neural networks responsible for specific behaviors. While prior methods have made significant progress, they typically depend on heuristics or approximations and do not offer provable guarantees over continuous input domains for the resulting circuits. In this work, we leverage recent advances in neural network verification to propose a suite of automated algorithms that yie",
      "tags": [
        "circuits"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2602-15307v1",
      "title": "What Do Neurons Listen To? A Neuron-level Dissection of a General-purpose Audio Model",
      "authors": "Takao Kawamura, Daisuke Niizumi, Nobutaka Ono",
      "date": "2026-02-17",
      "url": "https://arxiv.org/abs/2602.15307v1",
      "abstract": "In this paper, we analyze the internal representations of a general-purpose audio self-supervised learning (SSL) model from a neuron-level perspective. Despite their strong empirical performance as feature extractors, the internal mechanisms underlying the robust generalization of SSL audio models remain unclear. Drawing on the framework of mechanistic interpretability, we identify and examine class-specific neurons by analyzing conditional activation patterns across diverse tasks. Our analysis ",
      "tags": [
        "features"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2602-15730v1",
      "title": "Causal Effect Estimation with Latent Textual Treatments",
      "authors": "Omri Feldman, Amar Venugopal, Jann Spiess, Amir Feder",
      "date": "2026-02-17",
      "url": "https://arxiv.org/abs/2602.15730v1",
      "abstract": "Understanding the causal effects of text on downstream outcomes is a central task in many applications. Estimating such effects requires researchers to run controlled experiments that systematically vary textual features. While large language models (LLMs) hold promise for generating text, producing and evaluating controlled variation requires more careful attention. In this paper, we present an end-to-end pipeline for the generation and causal estimation of latent textual interventions. Our wor",
      "tags": [
        "features"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2602-16740v1",
      "title": "Quantifying LLM Attention-Head Stability: Implications for Circuit Universality",
      "authors": "Karan Bali, Jack Stanley, Praneet Suresh, Danilo Bzdok",
      "date": "2026-02-17",
      "url": "https://arxiv.org/abs/2602.16740v1",
      "abstract": "In mechanistic interpretability, recent work scrutinizes transformer \"circuits\" - sparse, mono or multi layer sub computations, that may reflect human understandable functions. Yet, these network circuits are rarely acid-tested for their stability across different instances of the same deep learning architecture. Without this, it remains unclear whether reported circuits emerge universally across labs or turn out to be idiosyncratic to a particular estimation instance, potentially limiting confi",
      "tags": [
        "circuits"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2602-14687v1",
      "title": "SynthSAEBench: Evaluating Sparse Autoencoders on Scalable Realistic Synthetic Data",
      "authors": "David Chanin, Adri\u00e0 Garriga-Alonso",
      "date": "2026-02-16",
      "url": "https://arxiv.org/abs/2602.14687v1",
      "abstract": "Improving Sparse Autoencoders (SAEs) requires benchmarks that can precisely validate architectural innovations. However, current SAE benchmarks on LLMs are often too noisy to differentiate architectural improvements, and current synthetic data experiments are too small-scale and unrealistic to provide meaningful comparisons. We introduce SynthSAEBench, a toolkit for generating large-scale synthetic data with realistic feature characteristics including correlation, hierarchy, and superposition, a",
      "tags": [
        "SAE",
        "superposition",
        "features"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2602-14869v1",
      "title": "Concept Influence: Leveraging Interpretability to Improve Performance and Efficiency in Training Data Attribution",
      "authors": "Matthew Kowal, Goncalo Paulo, Louis Jaburi, Tom Tseng, Lev E McKinney, et al.",
      "date": "2026-02-16",
      "url": "https://arxiv.org/abs/2602.14869v1",
      "abstract": "As large language models are increasingly trained and fine-tuned, practitioners need methods to identify which training data drive specific behaviors, particularly unintended ones. Training Data Attribution (TDA) methods address this by estimating datapoint influence. Existing approaches like influence functions are both computationally expensive and attribute based on single test examples, which can bias results toward syntactic rather than semantic similarity. To address these issues of scalab",
      "tags": [],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2602-14111v1",
      "title": "Sanity Checks for Sparse Autoencoders: Do SAEs Beat Random Baselines?",
      "authors": "Anton Korznikov, Andrey Galichin, Alexey Dontsov, Oleg Rogov, Ivan Oseledets, et al.",
      "date": "2026-02-15",
      "url": "https://arxiv.org/abs/2602.14111v1",
      "abstract": "Sparse Autoencoders (SAEs) have emerged as a promising tool for interpreting neural networks by decomposing their activations into sparse sets of human-interpretable features. Recent work has introduced multiple SAE variants and successfully scaled them to frontier models. Despite much excitement, a growing number of negative results in downstream tasks casts doubt on whether SAEs recover meaningful features. To directly investigate this, we perform two complementary evaluations. On a synthetic ",
      "tags": [
        "SAE",
        "features"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2602-13567v1",
      "title": "DistillLens: Symmetric Knowledge Distillation Through Logit Lens",
      "authors": "Manish Dhakal, Uthman Jinadu, Anjila Budathoki, Rajshekhar Sunderraman, Yi Ding",
      "date": "2026-02-14",
      "url": "https://arxiv.org/abs/2602.13567v1",
      "abstract": "Standard Knowledge Distillation (KD) compresses Large Language Models (LLMs) by optimizing final outputs, yet it typically treats the teacher's intermediate layer's thought process as a black box. While feature-based distillation attempts to bridge this gap, existing methods (e.g., MSE and asymmetric KL divergence) ignore the rich uncertainty profiles required for the final output. In this paper, we introduce DistillLens, a framework that symmetrically aligns the evolving thought processes of st",
      "tags": [
        "features"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2602-13524v1",
      "title": "Singular Vectors of Attention Heads Align with Features",
      "authors": "Gabriel Franco, Carson Loughridge, Mark Crovella",
      "date": "2026-02-13",
      "url": "https://arxiv.org/abs/2602.13524v1",
      "abstract": "Identifying feature representations in language models is a central task in mechanistic interpretability. Several recent studies have made an implicit assumption that feature representations can be inferred in some cases from singular vectors of attention matrices. However, sound justification for this assumption is lacking. In this paper we address that question, asking: why and when do singular vectors align with features? First, we demonstrate that singular vectors robustly align with feature",
      "tags": [
        "features",
        "attention"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2602-13483v1",
      "title": "Finding Highly Interpretable Prompt-Specific Circuits in Language Models",
      "authors": "Gabriel Franco, Lucas M. Tassis, Azalea Rohr, Mark Crovella",
      "date": "2026-02-13",
      "url": "https://arxiv.org/abs/2602.13483v1",
      "abstract": "Understanding the internal circuits that language models use to solve tasks remains a central challenge in mechanistic interpretability. Most prior work identifies circuits at the task level by averaging across many prompts, implicitly assuming a single stable mechanism per task. We show that this assumption can obscure a crucial source of structure: circuits are prompt-specific, even within a fixed task. Building on attention causal communication (ACC) (Franco & Crovella, 2025), we introduce AC",
      "tags": [
        "circuits"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2602-12026v1",
      "title": "Protein Circuit Tracing via Cross-layer Transcoders",
      "authors": "Darin Tsui, Kunal Talreja, Daniel Saeedi, Amirali Aghazadeh",
      "date": "2026-02-12",
      "url": "https://arxiv.org/abs/2602.12026v1",
      "abstract": "Protein language models (pLMs) have emerged as powerful predictors of protein structure and function. However, the computational circuits underlying their predictions remain poorly understood. Recent mechanistic interpretability methods decompose pLM representations into interpretable features, but they treat each layer independently and thus fail to capture cross-layer computation, limiting their ability to approximate the full model. We introduce ProtoMech, a framework for discovering computat",
      "tags": [
        "circuits",
        "features",
        "biology"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2602-11881v1",
      "title": "From Atoms to Trees: Building a Structured Feature Forest with Hierarchical Sparse Autoencoders",
      "authors": "Yifan Luo, Yang Zhan, Jiedong Jiang, Tianyang Liu, Mingrui Wu, et al.",
      "date": "2026-02-12",
      "url": "https://arxiv.org/abs/2602.11881v1",
      "abstract": "Sparse autoencoders (SAEs) have proven effective for extracting monosemantic features from large language models (LLMs), yet these features are typically identified in isolation. However, broad evidence suggests that LLMs capture the intrinsic structure of natural language, where the phenomenon of \"feature splitting\" in particular indicates that such structure is hierarchical. To capture this, we propose the Hierarchical Sparse Autoencoder (HSAE), which jointly learns a series of SAEs and the pa",
      "tags": [
        "SAE",
        "features"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2602-11461v1",
      "title": "EM-Aware Physical Synthesis: Neural Inductor Modeling and Intelligent Placement & Routing for RF Circuits",
      "authors": "Yilun Huang, Asal Mehradfar, Salman Avestimehr, Hamidreza Aghasi",
      "date": "2026-02-12",
      "url": "https://arxiv.org/abs/2602.11461v1",
      "abstract": "This paper presents an ML-driven framework for automated RF physical synthesis that transforms circuit netlists into manufacturable GDSII layouts. While recent ML approaches demonstrate success in topology selection and parameter optimization, they fail to produce manufacturable layouts due to oversimplified component models and lack of routing capabilities. Our framework addresses these limitations through three key innovations: (1) a neural network framework trained on 18,210 inductor geometri",
      "tags": [
        "circuits"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2602-11910v1",
      "title": "TADA! Tuning Audio Diffusion Models through Activation Steering",
      "authors": "\u0141ukasz Staniszewski, Katarzyna Zaleska, Mateusz Modrzejewski, Kamil Deja",
      "date": "2026-02-12",
      "url": "https://arxiv.org/abs/2602.11910v1",
      "abstract": "Audio diffusion models can synthesize high-fidelity music from text, yet their internal mechanisms for representing high-level concepts remain poorly understood. In this work, we use activation patching to demonstrate that distinct semantic musical concepts, such as the presence of specific instruments, vocals, or genre characteristics, are controlled by a small, shared subset of attention layers in state-of-the-art audio diffusion architectures. Next, we demonstrate that applying Contrastive Ac",
      "tags": [
        "steering"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2602-12158v1",
      "title": "SafeNeuron: Neuron-Level Safety Alignment for Large Language Models",
      "authors": "Zhaoxin Wang, Jiaming Liang, Fengbin Zhu, Weixiang Zhao, Junfeng Fang, et al.",
      "date": "2026-02-12",
      "url": "https://arxiv.org/abs/2602.12158v1",
      "abstract": "Large language models (LLMs) and multimodal LLMs are typically safety-aligned before release to prevent harmful content generation. However, recent studies show that safety behaviors are concentrated in a small subset of parameters, making alignment brittle and easily bypassed through neuron-level attacks. Moreover, most existing alignment methods operate at the behavioral level, offering limited control over the model's internal safety mechanisms. In this work, we propose SafeNeuron, a neuron-l",
      "tags": [
        "safety",
        "vision"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2602-11729v1",
      "title": "Cross-Architecture Model Diffing with Crosscoders: Unsupervised Discovery of Differences Between LLMs",
      "authors": "Thomas Jiralerspong, Trenton Bricken",
      "date": "2026-02-12",
      "url": "https://arxiv.org/abs/2602.11729v1",
      "abstract": "Model diffing, the process of comparing models' internal representations to identify their differences, is a promising approach for uncovering safety-critical behaviors in new models. However, its application has so far been primarily focused on comparing a base model with its finetune. Since new LLM releases are often novel architectures, cross-architecture methods are essential to make model diffing widely applicable. Crosscoders are one solution capable of cross-architecture model diffing but",
      "tags": [
        "safety"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2602-12418v1",
      "title": "Sparse Autoencoders are Capable LLM Jailbreak Mitigators",
      "authors": "Yannick Assogba, Jacopo Cortellazzi, Javier Abad, Pau Rodriguez, Xavier Suau, et al.",
      "date": "2026-02-12",
      "url": "https://arxiv.org/abs/2602.12418v1",
      "abstract": "Jailbreak attacks remain a persistent threat to large language model safety. We propose Context-Conditioned Delta Steering (CC-Delta), an SAE-based defense that identifies jailbreak-relevant sparse features by comparing token-level representations of the same harmful request with and without jailbreak context. Using paired harmful/jailbreak prompts, CC-Delta selects features via statistical testing and applies inference-time mean-shift steering in SAE latent space. Across four aligned instructio",
      "tags": [
        "SAE",
        "features",
        "safety",
        "steering"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2602-12403v1",
      "title": "MonoLoss: A Training Objective for Interpretable Monosemantic Representations",
      "authors": "Ali Nasiri-Sarvi, Anh Tien Nguyen, Hassan Rivaz, Dimitris Samaras, Mahdi S. Hosseini",
      "date": "2026-02-12",
      "url": "https://arxiv.org/abs/2602.12403v1",
      "abstract": "Sparse autoencoders (SAEs) decompose polysemantic neural representations, where neurons respond to multiple unrelated concepts, into monosemantic features that capture single, interpretable concepts. However, standard training objectives only weakly encourage this decomposition, and existing monosemanticity metrics require pairwise comparisons across all dataset samples, making them inefficient during training and evaluation. We study a recent MonoScore metric and derive a single-pass algorithm ",
      "tags": [
        "SAE",
        "features"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2602-11130v1",
      "title": "From Circuits to Dynamics: Understanding and Stabilizing Failure in 3D Diffusion Transformers",
      "authors": "Maximilian Plattner, Fabian Paischer, Johannes Brandstetter, Arturs Berzins",
      "date": "2026-02-11",
      "url": "https://arxiv.org/abs/2602.11130v1",
      "abstract": "Reliable surface completion from sparse point clouds underpins many applications spanning content creation and robotics. While 3D diffusion transformers attain state-of-the-art results on this task, we uncover that they exhibit a catastrophic mode of failure: arbitrarily small on-surface perturbations to the input point cloud can fracture the output into multiple disconnected pieces -- a phenomenon we call Meltdown. Using activation-patching from mechanistic interpretability, we localize Meltdow",
      "tags": [
        "circuits"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2602-10437v1",
      "title": "Control Reinforcement Learning: Token-Level Mechanistic Analysis via Learned SAE Feature Steering",
      "authors": "Seonglae Cho, Zekun Wu, Adriano Koshiyama",
      "date": "2026-02-11",
      "url": "https://arxiv.org/abs/2602.10437v1",
      "abstract": "Sparse autoencoders (SAEs) decompose language model activations into interpretable features, but existing methods reveal only which features activate, not which change model outputs when amplified. We introduce Control Reinforcement Learning (CRL), which trains a policy to select SAE features for steering at each token, producing interpretable intervention logs: the learned policy identifies features that change model outputs when amplified. Adaptive Feature Masking encourages diverse feature di",
      "tags": [
        "SAE",
        "features",
        "steering"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2602-10388v1",
      "title": "Less is Enough: Synthesizing Diverse Data in Feature Space of LLMs",
      "authors": "Zhongzhi Li, Xuansheng Wu, Yijiang Li, Lijie Hu, Ninghao Liu",
      "date": "2026-02-11",
      "url": "https://arxiv.org/abs/2602.10388v1",
      "abstract": "The diversity of post-training data is critical for effective downstream performance in large language models (LLMs). Many existing approaches to constructing post-training data quantify diversity using text-based metrics that capture linguistic variation, but such metrics provide only weak signals for the task-relevant features that determine downstream performance. In this work, we introduce Feature Activation Coverage (FAC) which measures data diversity in an interpretable feature space. Buil",
      "tags": [
        "features"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2602-10508v1",
      "title": "Med-SegLens: Latent-Level Model Diffing for Interpretable Medical Image Segmentation",
      "authors": "Salma J. Ahmed, Emad A. Mohammed, Azam Asilian Bidgoli",
      "date": "2026-02-11",
      "url": "https://arxiv.org/abs/2602.10508v1",
      "abstract": "Modern segmentation models achieve strong predictive performance but remain largely opaque, limiting our ability to diagnose failures, understand dataset shift, or intervene in a principled manner. We introduce Med-SegLens, a model-diffing framework that decomposes segmentation model activations into interpretable latent features using sparse autoencoders trained on SegFormer and U-Net. Through cross-architecture and cross-dataset latent alignment across healthy, adult, pediatric, and sub-Sahara",
      "tags": [
        "SAE",
        "features",
        "safety",
        "vision"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2602-10382v1",
      "title": "Triggers Hijack Language Circuits: A Mechanistic Analysis of Backdoor Behaviors in Large Language Models",
      "authors": "Th\u00e9o Lasnier, Wissam Antoun, Francis Kulumba, Djam\u00e9 Seddah",
      "date": "2026-02-11",
      "url": "https://arxiv.org/abs/2602.10382v1",
      "abstract": "Backdoor attacks pose significant security risks for Large Language Models (LLMs), yet the internal mechanisms by which triggers operate remain poorly understood. We present the first mechanistic analysis of language-switching backdoors, studying the GAPperon model family (1B, 8B, 24B parameters) which contains triggers injected during pretraining that cause output language switching. Using activation patching, we localize trigger formation to early layers (7.5-25% of model depth) and identify w",
      "tags": [
        "circuits"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2602-10437v2",
      "title": "Control Reinforcement Learning: Interpretable Token-Level Steering of LLMs via Sparse Autoencoder Features",
      "authors": "Seonglae Cho, Zekun Wu, Adriano Koshiyama",
      "date": "2026-02-11",
      "url": "https://arxiv.org/abs/2602.10437v2",
      "abstract": "Sparse autoencoders (SAEs) decompose language model activations into interpretable features, but existing methods reveal only which features activate, not which change model outputs when amplified. We introduce Control Reinforcement Learning (CRL), which trains a policy to select SAE features for steering at each token, producing interpretable intervention logs: the learned policy identifies features that change model outputs when amplified. Adaptive Feature Masking encourages diverse feature di",
      "tags": [
        "SAE",
        "features",
        "steering"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2602-10371v1",
      "title": "Simple LLM Baselines are Competitive for Model Diffing",
      "authors": "Elias Kempf, Simon Schrodi, Bartosz Cywi\u0144ski, Thomas Brox, Neel Nanda, et al.",
      "date": "2026-02-10",
      "url": "https://arxiv.org/abs/2602.10371v1",
      "abstract": "Standard LLM evaluations only test capabilities or dispositions that evaluators designed them for, missing unexpected differences such as behavioral shifts between model revisions or emergent misaligned tendencies. Model diffing addresses this limitation by automatically surfacing systematic behavioral differences. Recent approaches include LLM-based methods that generate natural language descriptions and sparse autoencoder (SAE)-based methods that identify interpretable features. However, no sy",
      "tags": [
        "SAE",
        "features",
        "vision"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2602-10352v1",
      "title": "Learning Self-Interpretation from Interpretability Artifacts: Training Lightweight Adapters on Vector-Label Pairs",
      "authors": "Keenan Pepper, Alex McKenzie, Florin Pop, Stijn Servaes, Martin Leitgab, et al.",
      "date": "2026-02-10",
      "url": "https://arxiv.org/abs/2602.10352v1",
      "abstract": "Self-interpretation methods prompt language models to describe their own internal states, but remain unreliable due to hyperparameter sensitivity. We show that training lightweight adapters on interpretability artifacts, while keeping the LM entirely frozen, yields reliable self-interpretation across tasks and model families. A scalar affine adapter with just $d_\\text{model}+1$ parameters suffices: trained adapters generate sparse autoencoder feature labels that outperform the training labels th",
      "tags": [
        "SAE",
        "features"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2602-09783v1",
      "title": "Why Linear Interpretability Works: Invariant Subspaces as a Result of Architectural Constraints",
      "authors": "Andres Saurez, Yousung Lee, Dongsoo Har",
      "date": "2026-02-10",
      "url": "https://arxiv.org/abs/2602.09783v1",
      "abstract": "Linear probes and sparse autoencoders consistently recover meaningful structure from transformer representations -- yet why should such simple methods succeed in deep, nonlinear systems? We show this is not merely an empirical regularity but a consequence of architectural necessity: transformers communicate information through linear interfaces (attention OV circuits, unembedding matrices), and any semantic feature decoded through such an interface must occupy a context-invariant linear subspace",
      "tags": [
        "SAE",
        "circuits",
        "features",
        "probing"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2602-09784v1",
      "title": "Circuit Fingerprints: How Answer Tokens Encode Their Geometrical Path",
      "authors": "Andres Saurez, Neha Sengar, Dongsoo Har",
      "date": "2026-02-10",
      "url": "https://arxiv.org/abs/2602.09784v1",
      "abstract": "Circuit discovery and activation steering in transformers have developed as separate research threads, yet both operate on the same representational space. Are they two views of the same underlying structure? We show they follow a single geometric principle: answer tokens, processed in isolation, encode the directions that would produce them. This Circuit Fingerprint hypothesis enables circuit discovery without gradients or causal intervention -- recovering comparable structure to gradient-based",
      "tags": [
        "circuits",
        "steering"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2602-08548v1",
      "title": "How Do Language Models Understand Tables? A Mechanistic Analysis of Cell Location",
      "authors": "Xuanliang Zhang, Dingzirui Wang, Keyan Xu, Qingfu Zhu, Wanxiang Che",
      "date": "2026-02-09",
      "url": "https://arxiv.org/abs/2602.08548v1",
      "abstract": "While Large Language Models (LLMs) are increasingly deployed for table-related tasks, the internal mechanisms enabling them to process linearized two-dimensional structured tables remain opaque. In this work, we investigate the process of table understanding by dissecting the atomic task of cell location. Through activation patching and complementary interpretability techniques, we delineate the table understanding mechanism into a sequential three-stage pipeline: Semantic Binding, Coordinate Lo",
      "tags": [],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2602-07930v1",
      "title": "Patches of Nonlinearity: Instruction Vectors in Large Language Models",
      "authors": "Irina Bigoulaeva, Jonas Rohweder, Subhabrata Dutta, Iryna Gurevych",
      "date": "2026-02-08",
      "url": "https://arxiv.org/abs/2602.07930v1",
      "abstract": "Despite the recent success of instruction-tuned language models and their ubiquitous usage, very little is known of how models process instructions internally. In this work, we address this gap from a mechanistic point of view by investigating how instruction-specific representations are constructed and utilized in different stages of post-training: Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO). Via causal mediation, we identify that instruction representation is fairly l",
      "tags": [],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2602-07311v1",
      "title": "LUCID-SAE: Learning Unified Vision-Language Sparse Codes for Interpretable Concept Discovery",
      "authors": "Difei Gu, Yunhe Gao, Gerasimos Chatzoudis, Zihan Dong, Guoning Zhang, et al.",
      "date": "2026-02-07",
      "url": "https://arxiv.org/abs/2602.07311v1",
      "abstract": "Sparse autoencoders (SAEs) offer a natural path toward comparable explanations across different representation spaces. However, current SAEs are trained per modality, producing dictionaries whose features are not directly understandable and whose explanations do not transfer across domains. In this study, we introduce LUCID (Learning Unified vision-language sparse Codes for Interpretable concept Discovery), a unified vision-language sparse autoencoder that learns a shared latent dictionary for i",
      "tags": [
        "SAE",
        "features",
        "vision"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2602-06852v1",
      "title": "The Quantum Sieve Tracer: A Hybrid Framework for Layer-Wise Activation Tracing in Large Language Models",
      "authors": "Jonathan Pan",
      "date": "2026-02-06",
      "url": "https://arxiv.org/abs/2602.06852v1",
      "abstract": "Mechanistic interpretability aims to reverse-engineer the internal computations of Large Language Models (LLMs), yet separating sparse semantic signals from high-dimensional polysemantic noise remains a significant challenge. This paper introduces the Quantum Sieve Tracer, a hybrid quantum-classical framework designed to characterize factual recall circuits. We implement a modular pipeline that first localizes critical layers using classical causal tracing, then maps specific attention head acti",
      "tags": [
        "circuits",
        "features",
        "attention"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2602-06964v1",
      "title": "Learning a Generative Meta-Model of LLM Activations",
      "authors": "Grace Luo, Jiahai Feng, Trevor Darrell, Alec Radford, Jacob Steinhardt",
      "date": "2026-02-06",
      "url": "https://arxiv.org/abs/2602.06964v1",
      "abstract": "Existing approaches for analyzing neural network activations, such as PCA and sparse autoencoders, rely on strong structural assumptions. Generative models offer an alternative: they can uncover structure without such assumptions and act as priors that improve intervention fidelity. We explore this direction by training diffusion models on one billion residual stream activations, creating \"meta-models\" that learn the distribution of a network's internal states. We find that diffusion loss decrea",
      "tags": [
        "SAE"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2602-06941v1",
      "title": "Endogenous Resistance to Activation Steering in Language Models",
      "authors": "Alex McKenzie, Keenan Pepper, Stijn Servaes, Martin Leitgab, Murat Cubuktepe, et al.",
      "date": "2026-02-06",
      "url": "https://arxiv.org/abs/2602.06941v1",
      "abstract": "Large language models can resist task-misaligned activation steering during inference, sometimes recovering mid-generation to produce improved responses even when steering remains active. We term this Endogenous Steering Resistance (ESR). Using sparse autoencoder (SAE) latents to steer model activations, we find that Llama-3.3-70B shows substantial ESR, while smaller models from the Llama-3 and Gemma-2 families exhibit the phenomenon less frequently. We identify 26 SAE latents that activate diff",
      "tags": [
        "SAE",
        "steering"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2602-08713v1",
      "title": "Towards Understanding Multimodal Fine-Tuning: Spatial Features",
      "authors": "Lachin Naghashyar, Hunar Batra, Ashkan Khakzar, Philip Torr, Ronald Clark, et al.",
      "date": "2026-02-06",
      "url": "https://arxiv.org/abs/2602.08713v1",
      "abstract": "Contemporary Vision-Language Models (VLMs) achieve strong performance on a wide range of tasks by pairing a vision encoder with a pre-trained language model, fine-tuned for visual-text inputs. Yet despite these gains, it remains unclear how language backbone representations adapt during multimodal training and when vision-specific capabilities emerge. In this work, we present the first mechanistic analysis of VLM adaptation. Using stage-wise model diffing, a technique that isolates representatio",
      "tags": [
        "features",
        "vision"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2602-07080v1",
      "title": "CodeCircuit: Toward Inferring LLM-Generated Code Correctness via Attribution Graphs",
      "authors": "Yicheng He, Zheng Zhao, Zhou Kaiyu, Bryan Dai, Jie Fu, et al.",
      "date": "2026-02-06",
      "url": "https://arxiv.org/abs/2602.07080v1",
      "abstract": "Current paradigms for code verification rely heavily on external mechanisms-such as execution-based unit tests or auxiliary LLM judges-which are often labor-intensive or limited by the judging model's own capabilities. This raises a fundamental, yet unexplored question: Can an LLM's functional correctness be assessed purely from its internal computational structure? Our primary objective is to investigate whether the model's neural dynamics encode internally decodable signals that are predictive",
      "tags": [
        "circuits"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2602-05859v1",
      "title": "DLM-Scope: Mechanistic Interpretability of Diffusion Language Models via Sparse Autoencoders",
      "authors": "Xu Wang, Bingqing Jiang, Yu Wan, Baosong Yang, Lingpeng Kong, et al.",
      "date": "2026-02-05",
      "url": "https://arxiv.org/abs/2602.05859v1",
      "abstract": "Sparse autoencoders (SAEs) have become a standard tool for mechanistic interpretability in autoregressive large language models (LLMs), enabling researchers to extract sparse, human-interpretable features and intervene on model behavior. Recently, as diffusion language models (DLMs) have become an increasingly promising alternative to the autoregressive LLMs, it is essential to develop tailored mechanistic interpretability tools for this emerging class of models. In this work, we present DLM-Sco",
      "tags": [
        "SAE",
        "features"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2602-05532v1",
      "title": "Split Personality Training: Revealing Latent Knowledge Through Alternate Personalities",
      "authors": "Florian Dietz, William Wale, Oscar Gilg, Robert McCarthy, Felix Michalak, et al.",
      "date": "2026-02-05",
      "url": "https://arxiv.org/abs/2602.05532v1",
      "abstract": "Detecting misalignment in large language models is challenging because models may learn to conceal misbehavior during training. Standard auditing techniques fall short: black-box methods often cannot distinguish misaligned outputs from benign ones, and mechanistic interpretability does not scale with model capabilities. We introduce Split Personality Training (SPT), which fine-tunes a second ``honest persona'' into LoRA parameters that remain inactive during normal operation. After the main mode",
      "tags": [
        "safety"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2602-05444v1",
      "title": "Causal Front-Door Adjustment for Robust Jailbreak Attacks on LLMs",
      "authors": "Yao Zhou, Zeen Song, Wenwen Qiang, Fengge Wu, Shuyi Zhou, et al.",
      "date": "2026-02-05",
      "url": "https://arxiv.org/abs/2602.05444v1",
      "abstract": "Safety alignment mechanisms in Large Language Models (LLMs) often operate as latent internal states, obscuring the model's inherent capabilities. Building on this observation, we model the safety mechanism as an unobserved confounder from a causal perspective. Then, we propose the \\textbf{C}ausal \\textbf{F}ront-Door \\textbf{A}djustment \\textbf{A}ttack ({\\textbf{CFA}}$^2$) to jailbreak LLM, which is a framework that leverages Pearl's Front-Door Criterion to sever the confounding associations for ",
      "tags": [
        "safety"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2602-05403v1",
      "title": "Advancing Opinion Dynamics Modeling with Neural Diffusion-Convection-Reaction Equation",
      "authors": "Chenghua Gong, Yihang Jiang, Hao Li, Rui Sun, Juyuan Zhang, et al.",
      "date": "2026-02-05",
      "url": "https://arxiv.org/abs/2602.05403v1",
      "abstract": "Advanced opinion dynamics modeling is vital for deciphering social behavior, emphasizing its role in mitigating polarization and securing cyberspace. To synergize mechanistic interpretability with data-driven flexibility, recent studies have explored the integration of Physics-Informed Neural Networks (PINNs) for opinion modeling. Despite this promise, existing methods are tailored to incomplete priors, lacking a comprehensive physical system to integrate dynamics from local, global, and endogen",
      "tags": [],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2602-05183v1",
      "title": "Data-Centric Interpretability for LLM-based Multi-Agent Reinforcement Learning",
      "authors": "John Yan, Michael Yu, Yuqi Sun, Alexander Duffy, Tyler Marques, et al.",
      "date": "2026-02-05",
      "url": "https://arxiv.org/abs/2602.05183v1",
      "abstract": "Large language models (LLMs) are increasingly trained in complex Reinforcement Learning, multi-agent environments, making it difficult to understand how behavior changes over training. Sparse Autoencoders (SAEs) have recently shown to be useful for data-centric interpretability. In this work, we analyze large-scale reinforcement learning training runs from the sophisticated environment of Full-Press Diplomacy by applying pretrained SAEs, alongside LLM-summarizer methods. We introduce Meta-Autoin",
      "tags": [
        "SAE"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2602-06218v1",
      "title": "Cross-Modal Redundancy and the Geometry of Vision-Language Embeddings",
      "authors": "Gr\u00e9goire Dhimo\u00efla, Thomas Fel, Victor Boutin, Agustin Picard",
      "date": "2026-02-05",
      "url": "https://arxiv.org/abs/2602.06218v1",
      "abstract": "Vision-language models (VLMs) align images and text with remarkable success, yet the geometry of their shared embedding space remains poorly understood. To probe this geometry, we begin from the Iso-Energy Assumption, which exploits cross-modal redundancy: a concept that is truly shared should exhibit the same average energy across modalities. We operationalize this assumption with an Aligned Sparse Autoencoder (SAE) that encourages energy consistency during training while preserving reconstruct",
      "tags": [
        "SAE",
        "probing",
        "vision"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2602-04613v1",
      "title": "Disentangling meaning from language in LLM-based machine translation",
      "authors": "Th\u00e9o Lasnier, Armel Zebaze, Djam\u00e9 Seddah, Rachel Bawden, Beno\u00eet Sagot",
      "date": "2026-02-04",
      "url": "https://arxiv.org/abs/2602.04613v1",
      "abstract": "Mechanistic Interpretability (MI) seeks to explain how neural networks implement their capabilities, but the scale of Large Language Models (LLMs) has limited prior MI work in Machine Translation (MT) to word-level analyses. We study sentence-level MT from a mechanistic perspective by analyzing attention heads to understand how LLMs internally encode and distribute translation functions. We decompose MT into two subtasks: producing text in the target language (i.e. target language identification",
      "tags": [
        "attention"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2602-04718v1",
      "title": "Identifying Intervenable and Interpretable Features via Orthogonality Regularization",
      "authors": "Moritz Miller, Florent Draye, Bernhard Sch\u00f6lkopf",
      "date": "2026-02-04",
      "url": "https://arxiv.org/abs/2602.04718v1",
      "abstract": "With recent progress on fine-tuning language models around a fixed sparse autoencoder, we disentangle the decoder matrix into almost orthogonal features. This reduces interference and superposition between the features, while keeping performance on the target dataset essentially unchanged. Our orthogonality penalty leads to identifiable features, ensuring the uniqueness of the decomposition. Further, we find that the distance between embedded feature explanations increases with stricter orthogon",
      "tags": [
        "SAE",
        "superposition",
        "features"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2602-04752v1",
      "title": "Decomposing Query-Key Feature Interactions Using Contrastive Covariances",
      "authors": "Andrew Lee, Yonatan Belinkov, Fernanda Vi\u00e9gas, Martin Wattenberg",
      "date": "2026-02-04",
      "url": "https://arxiv.org/abs/2602.04752v1",
      "abstract": "Despite the central role of attention heads in Transformers, we lack tools to understand why a model attends to a particular token. To address this, we study the query-key (QK) space -- the bilinear joint embedding space between queries and keys. We present a contrastive covariance method to decompose the QK space into low-rank, human-interpretable components. It is when features in keys and queries align in these low-rank subspaces that high attention scores are produced. We first study our met",
      "tags": [
        "features",
        "attention"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2602-05027v1",
      "title": "AudioSAE: Towards Understanding of Audio-Processing Models with Sparse AutoEncoders",
      "authors": "Georgii Aparin, Tasnima Sadekova, Alexey Rukhovich, Assel Yermekova, Laida Kushnareva, et al.",
      "date": "2026-02-04",
      "url": "https://arxiv.org/abs/2602.05027v1",
      "abstract": "Sparse Autoencoders (SAEs) are powerful tools for interpreting neural representations, yet their use in audio remains underexplored. We train SAEs across all encoder layers of Whisper and HuBERT, provide an extensive evaluation of their stability, interpretability, and show their practical utility. Over 50% of the features remain consistent across random seeds, and reconstruction quality is preserved. SAE features capture general acoustic and semantic information as well as specific events, incl",
      "tags": [
        "SAE",
        "features"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2602-04935v2",
      "title": "ASA: Training-Free Representation Engineering for Tool-Calling Agents",
      "authors": "Youjin Wang, Run Zhou, Rong Fu, Shuaishuai Cao, Hongwei Zeng, et al.",
      "date": "2026-02-04",
      "url": "https://arxiv.org/abs/2602.04935v2",
      "abstract": "Adapting LLM agents to domain-specific tool calling remains notably brittle under evolving interfaces. Prompt and schema engineering is easy to deploy but often fragile under distribution shift and strict parsers, while continual parameter-efficient fine-tuning improves reliability at the cost of training, maintenance, and potential forgetting. We identify a critical Lazy Agent failure mode where tool necessity is nearly perfectly decodable from mid-layer activations, yet the model remains conse",
      "tags": [],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2602-03506v1",
      "title": "Explaining the Explainer: Understanding the Inner Workings of Transformer-based Symbolic Regression Models",
      "authors": "Arco van Breda, Erman Acar",
      "date": "2026-02-03",
      "url": "https://arxiv.org/abs/2602.03506v1",
      "abstract": "Following their success across many domains, transformers have also proven effective for symbolic regression (SR); however, the internal mechanisms underlying their generation of mathematical operators remain largely unexplored. Although mechanistic interpretability has successfully identified circuits in language and vision models, it has not yet been applied to SR. In this article, we introduce PATCHES, an evolutionary circuit discovery algorithm that identifies compact and correct circuits fo",
      "tags": [
        "circuits",
        "vision"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2602-03263v1",
      "title": "CSR-Bench: A Benchmark for Evaluating the Cross-modal Safety and Reliability of MLLMs",
      "authors": "Yuxuan Liu, Yuntian Shi, Kun Wang, Haoting Shen, Kun Yang",
      "date": "2026-02-03",
      "url": "https://arxiv.org/abs/2602.03263v1",
      "abstract": "Multimodal large language models (MLLMs) enable interaction over both text and images, but their safety behavior can be driven by unimodal shortcuts instead of true joint intent understanding. We introduce CSR-Bench, a benchmark for evaluating cross-modal reliability through four stress-testing interaction patterns spanning Safety, Over-rejection, Bias, and Hallucination, covering 61 fine-grained types. Each instance is constructed to require integrated image-text interpretation, and we addition",
      "tags": [
        "safety",
        "vision"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2602-04902v1",
      "title": "Momentum Attention: The Physics of In-Context Learning and Spectral Forensics for Mechanistic Interpretability",
      "authors": "Kingsuk Maitra",
      "date": "2026-02-03",
      "url": "https://arxiv.org/abs/2602.04902v1",
      "abstract": "The Mechanistic Interpretability (MI) program has mapped the Transformer as a precise computational graph. We extend this graph with a conservation law and time-varying AC dynamics, viewing it as a physical circuit. We introduce Momentum Attention, a symplectic augmentation embedding physical priors via the kinematic difference operator $p_t = q_t - q_{t-1}$, implementing the symplectic shear $\\hat{q}_t = q_t + \u03b3p_t$ on queries and keys. We identify a fundamental Symplectic-Filter Duality: the p",
      "tags": [
        "circuits"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2602-04903v1",
      "title": "Mind the Performance Gap: Capability-Behavior Trade-offs in Feature Steering",
      "authors": "Eitan Sprejer, Oscar Agust\u00edn Stanchi, Mar\u00eda Victoria Carro, Denise Alejandra Mester, Iv\u00e1n Arcuschin",
      "date": "2026-02-03",
      "url": "https://arxiv.org/abs/2602.04903v1",
      "abstract": "Feature steering has emerged as a promising approach for controlling LLM behavior through direct manipulation of internal representations, offering advantages over prompt engineering. However, its practical effectiveness in real-world applications remains poorly understood, particularly regarding potential trade-offs with output quality. We show that feature steering methods substantially degrade model performance even when successfully controlling target behaviors, a critical trade-off. Specifi",
      "tags": [
        "features",
        "steering"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2602-03994v2",
      "title": "Bypassing the Rationale: Causal Auditing of Implicit Reasoning in Language Models",
      "authors": "Anish Sathyanarayanan, Aditya Nagarsekar, Aarush Rathore",
      "date": "2026-02-03",
      "url": "https://arxiv.org/abs/2602.03994v2",
      "abstract": "Chain-of-thought (CoT) prompting is widely used as a reasoning aid and is often treated as a transparency mechanism. Yet behavioral gains under CoT do not imply that the model's internal computation causally depends on the emitted reasoning text, i.e., models may produce fluent rationales while routing decision-critical computation through latent pathways. We introduce a causal, layerwise audit of CoT faithfulness based on activation patching. Our key metric, the CoT Mediation Index (CMI), isola",
      "tags": [
        "reasoning"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2602-01605v1",
      "title": "Universal Redundancies in Time Series Foundation Models",
      "authors": "Anthony Bao, Venkata Hasith Vattikuti, Jeffrey Lai, William Gilpin",
      "date": "2026-02-02",
      "url": "https://arxiv.org/abs/2602.01605v1",
      "abstract": "Time Series Foundation Models (TSFMs) leverage extensive pretraining to accurately predict unseen time series during inference, without the need for task-specific fine-tuning. Through large-scale evaluations on standard benchmarks, we find that leading transformer-based TSFMs exhibit redundant components in their intermediate layers. We introduce a set of tools for mechanistic interpretability of TSFMs, including ablations of specific components and direct logit attribution on the residual strea",
      "tags": [],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2602-01716v1",
      "title": "Mechanistic Indicators of Steering Effectiveness in Large Language Models",
      "authors": "Mehdi Jafari, Hao Xue, Flora Salim",
      "date": "2026-02-02",
      "url": "https://arxiv.org/abs/2602.01716v1",
      "abstract": "Activation-based steering enables Large Language Models (LLMs) to exhibit targeted behaviors by intervening on intermediate activations without retraining. Despite its widespread use, the mechanistic factors that govern when steering succeeds or fails remain poorly understood, as prior work has relied primarily on black-box outputs or LLM-based judges. In this study, we investigate whether the reliability of steering can be diagnosed using internal model signals. We focus on two information-theo",
      "tags": [
        "steering"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2602-02224v1",
      "title": "Spectral Superposition: A Theory of Feature Geometry",
      "authors": "Georgi Ivanov, Narmeen Oozeer, Shivam Raval, Tasana Pejovic, Shriyash Upadhyay, et al.",
      "date": "2026-02-02",
      "url": "https://arxiv.org/abs/2602.02224v1",
      "abstract": "Neural networks represent more features than they have dimensions via superposition, forcing features to share representational space. Current methods decompose activations into sparse linear features but discard geometric structure. We develop a theory for studying the geometric structre of features by analyzing the spectra (eigenvalues, eigenspaces, etc.) of weight derived matrices. In particular, we introduce the frame operator $F = WW^\\top$, which gives us a spectral measure that describes h",
      "tags": [
        "superposition",
        "features",
        "theory"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2602-01999v1",
      "title": "From Latent Signals to Reflection Behavior: Tracing Meta-Cognitive Activation Trajectory in R1-Style LLMs",
      "authors": "Yanrui Du, Yibo Gao, Sendong Zhao, Jiayun Li, Haochun Wang, et al.",
      "date": "2026-02-02",
      "url": "https://arxiv.org/abs/2602.01999v1",
      "abstract": "R1-style LLMs have attracted growing attention for their capacity for self-reflection, yet the internal mechanisms underlying such behavior remain unclear. To bridge this gap, we anchor on the onset of reflection behavior and trace its layer-wise activation trajectory. Using the logit lens to read out token-level semantics, we uncover a structured progression: (i) Latent-control layers, where an approximate linear direction encodes the semantics of thinking budget; (ii) Semantic-pivot layers, wh",
      "tags": [],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2602-01530v1",
      "title": "Preserving Localized Patch Semantics in VLMs",
      "authors": "Parsa Esmaeilkhani, Longin Jan Latecki",
      "date": "2026-02-02",
      "url": "https://arxiv.org/abs/2602.01530v1",
      "abstract": "Logit Lens has been proposed for visualizing tokens that contribute most to LLM answers. Recently, Logit Lens was also shown to be applicable in autoregressive Vision-Language Models (VLMs), where it illustrates the conceptual content of image tokens in the form of heatmaps, e.g., which image tokens are likely to depict the concept of cat in a given image. However, the visual content of image tokens often gets diffused to language tokens, and consequently, the locality of visual information gets",
      "tags": [
        "vision"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2602-01695v1",
      "title": "Beyond Dense States: Elevating Sparse Transcoders to Active Operators for Latent Reasoning",
      "authors": "Yadong Wang, Haodong Chen, Yu Tian, Chuanxing Geng, Dong Liang, et al.",
      "date": "2026-02-02",
      "url": "https://arxiv.org/abs/2602.01695v1",
      "abstract": "Latent reasoning compresses the chain-of-thought (CoT) into continuous hidden states, yet existing methods rely on dense latent transitions that remain difficult to interpret and control. Meanwhile, sparse representation models uncover human-interpretable semantic features but remain largely confined to post-hoc analysis. We reconcile this tension by proposing LSTR (Latent Sparse Transcoder Reasoning), a latent reasoning framework that elevates functional sparse transcoders into active reasoning",
      "tags": [
        "features",
        "reasoning"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "ss-48681d875dd4a7fd38cd",
      "title": "Research on the instability mechanism of transformer winding short-circuit faults based on magneto-mechanical coupling analysis",
      "authors": "Zhihao Gao, Jing Zhou, Li Zhu, Yuefeng Hao, Zhanlong Zhang, et al.",
      "date": "2026-02-01",
      "url": "https://www.semanticscholar.org/paper/48681d875dd4a7fd38cd0b9f05262cccabefe846",
      "abstract": "",
      "tags": [
        "circuits"
      ],
      "source": "Semantic Scholar",
      "featured": false
    },
    {
      "id": "arxiv-2602-01247v1",
      "title": "Mechanistic Interpretability of Brain-to-Speech Models Across Speech Modes",
      "authors": "Maryam Maghsoudi, Ayushi Mishra",
      "date": "2026-02-01",
      "url": "https://arxiv.org/abs/2602.01247v1",
      "abstract": "Brain-to-speech decoding models demonstrate robust performance in vocalized, mimed, and imagined speech; yet, the fundamental mechanisms via which these models capture and transmit information across different speech modalities are less explored. In this work, we use mechanistic interpretability to causally investigate the internal representations of a neural speech decoder. We perform cross-mode activation patching of internal activations across speech modes, and use tri-modal interpolation to ",
      "tags": [],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2602-01322v1",
      "title": "PolySAE: Modeling Feature Interactions in Sparse Autoencoders via Polynomial Decoding",
      "authors": "Panagiotis Koromilas, Andreas D. Demou, James Oldfield, Yannis Panagakis, Mihalis Nicolaou",
      "date": "2026-02-01",
      "url": "https://arxiv.org/abs/2602.01322v1",
      "abstract": "Sparse autoencoders (SAEs) have emerged as a promising method for interpreting neural network representations by decomposing activations into sparse combinations of dictionary atoms. However, SAEs assume that features combine additively through linear reconstruction, an assumption that cannot capture compositional structure: linear models cannot distinguish whether \"Starbucks\" arises from the composition of \"star\" and \"coffee\" features or merely their co-occurrence. This forces SAEs to allocate ",
      "tags": [
        "SAE",
        "features"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2602-00945v1",
      "title": "Neural FOXP2 -- Language Specific Neuron Steering for Targeted Language Improvement in LLMs",
      "authors": "Anusa Saha, Tanmay Joshi, Vinija Jain, Aman Chadha, Amitava Das",
      "date": "2026-02-01",
      "url": "https://arxiv.org/abs/2602.00945v1",
      "abstract": "LLMs are multilingual by training, yet their lingua franca is often English, reflecting English language dominance in pretraining. Other languages remain in parametric memory but are systematically suppressed. We argue that language defaultness is governed by a sparse, low-rank control circuit, language neurons, that can be mechanistically isolated and safely steered.   We introduce Neural FOXP2, that makes a chosen language (Hindi or Spanish) primary in a model by steering language-specific neu",
      "tags": [
        "circuits",
        "steering"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2602-00924v1",
      "title": "Supervised sparse auto-encoders as unconstrained feature models for semantic composition",
      "authors": "Ouns El Harzli, Hugo Wallner, Yoonsoo Nam, Haixuan Xavier Tao",
      "date": "2026-01-31",
      "url": "https://arxiv.org/abs/2602.00924v1",
      "abstract": "Sparse auto-encoders (SAEs) have re-emerged as a prominent method for mechanistic interpretability, yet they face two significant challenges: the non-smoothness of the $L_1$ penalty, which hinders reconstruction and scalability, and a lack of alignment between learned features and human semantics. In this paper, we address these limitations by adapting unconstrained feature models-a mathematical framework from neural collapse theory-and by supervising the task. We supervise (decoder-only) SAEs t",
      "tags": [
        "SAE",
        "features",
        "safety",
        "theory"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2602-00621v1",
      "title": "Towards Interpretable Hallucination Analysis and Mitigation in LVLMs via Contrastive Neuron Steering",
      "authors": "Guangtao Lyu, Xinyi Cheng, Qi Liu, Chenghao Xu, Jiexi Yan, et al.",
      "date": "2026-01-31",
      "url": "https://arxiv.org/abs/2602.00621v1",
      "abstract": "LVLMs achieve remarkable multimodal understanding and generation but remain susceptible to hallucinations. Existing mitigation methods predominantly focus on output-level adjustments, leaving the internal mechanisms that give rise to these hallucinations largely unexplored. To gain a deeper understanding, we adopt a representation-level perspective by introducing sparse autoencoders (SAEs) to decompose dense visual embeddings into sparse, interpretable neurons. Through neuron-level analysis, we ",
      "tags": [
        "SAE",
        "steering",
        "vision"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2602-00449v1",
      "title": "Do Latent-CoT Models Think Step-by-Step? A Mechanistic Study on Sequential Reasoning Tasks",
      "authors": "Jia Liang, Liangming Pan",
      "date": "2026-01-31",
      "url": "https://arxiv.org/abs/2602.00449v1",
      "abstract": "Latent Chain-of-Thought (Latent-CoT) aims to enable step-by-step computation without emitting long rationales, yet its mechanisms remain unclear. We study CODI, a continuous-thought teacher-student distillation model, on strictly sequential polynomial-iteration tasks. Using logit-lens decoding, linear probes, attention analysis, and activation patching, we localize intermediate-state representations and trace their routing to the final readout. On two- and three-hop tasks, CODI forms the full se",
      "tags": [
        "probing",
        "reasoning"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2602-00822v1",
      "title": "Safety-Efficacy Trade Off: Robustness against Data-Poisoning",
      "authors": "Diego Granziol",
      "date": "2026-01-31",
      "url": "https://arxiv.org/abs/2602.00822v1",
      "abstract": "Backdoor and data poisoning attacks can achieve high attack success while evading existing spectral and optimisation based defences. We show that this behaviour is not incidental, but arises from a fundamental geometric mechanism in input space. Using kernel ridge regression as an exact model of wide neural networks, we prove that clustered dirty label poisons induce a rank one spike in the input Hessian whose magnitude scales quadratically with attack efficacy. Crucially, for nonlinear kernels ",
      "tags": [
        "safety"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2601-22795v1",
      "title": "Sparse or Dense? A Mechanistic Estimation of Computation Density in Transformer-based LLMs",
      "authors": "Corentin Kervadec, Iuliia Lysova, Marco Baroni, Gemma Boleda",
      "date": "2026-01-30",
      "url": "https://arxiv.org/abs/2601.22795v1",
      "abstract": "Transformer-based large language models (LLMs) are comprised of billions of parameters arranged in deep and wide computational graphs. Several studies on LLM efficiency optimization argue that it is possible to prune a significant portion of the parameters, while only marginally impacting performance. This suggests that the computation is not uniformly distributed across the parameters. We introduce here a technique to systematically quantify computation density in LLMs. In particular, we design",
      "tags": [
        "circuits"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2601-22594v1",
      "title": "Language Model Circuits Are Sparse in the Neuron Basis",
      "authors": "Aryaman Arora, Zhengxuan Wu, Jacob Steinhardt, Sarah Schwettmann",
      "date": "2026-01-30",
      "url": "https://arxiv.org/abs/2601.22594v1",
      "abstract": "The high-level concepts that a neural network uses to perform computation need not be aligned to individual neurons (Smolensky, 1986). Language model interpretability research has thus turned to techniques such as \\textit{sparse autoencoders} (SAEs) to decompose the neuron basis into more interpretable units of model computation, for tasks such as \\textit{circuit tracing}. However, not all neuron-based representations are uninterpretable. For the first time, we empirically show that \\textbf{MLP ",
      "tags": [
        "SAE",
        "circuits"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2601-22447v1",
      "title": "Beyond Activation Patterns: A Weight-Based Out-of-Context Explanation of Sparse Autoencoder Features",
      "authors": "Yiting Liu, Zhi-Hong Deng",
      "date": "2026-01-30",
      "url": "https://arxiv.org/abs/2601.22447v1",
      "abstract": "Sparse autoencoders (SAEs) have emerged as a powerful technique for decomposing language model representations into interpretable features. Current interpretation methods infer feature semantics from activation patterns, but overlook that features are trained to reconstruct activations that serve computational roles in the forward pass. We introduce a novel weight-based interpretation framework that measures functional effects through direct weight interactions, requiring no activation data. Thr",
      "tags": [
        "SAE",
        "features"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2601-22928v1",
      "title": "LLMs Explain't: A Post-Mortem on Semantic Interpretability in Transformer Models",
      "authors": "Alhassan Abdelhalim, Janick Edinger, S\u00f6ren Laue, Michaela Regneri",
      "date": "2026-01-30",
      "url": "https://arxiv.org/abs/2601.22928v1",
      "abstract": "Large Language Models (LLMs) are becoming increasingly popular in pervasive computing due to their versatility and strong performance. However, despite their ubiquitous use, the exact mechanisms underlying their outstanding performance remain unclear. Different methods for LLM explainability exist, and many are, as a method, not fully understood themselves. We started with the question of how linguistic abstraction emerges in LLMs, aiming to detect it across different LLM modules (attention head",
      "tags": [
        "attention"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2601-22012v1",
      "title": "Putting a Face to Forgetting: Continual Learning meets Mechanistic Interpretability",
      "authors": "Sergi Masip, Gido M. van de Ven, Javier Ferrando, Tinne Tuytelaars",
      "date": "2026-01-29",
      "url": "https://arxiv.org/abs/2601.22012v1",
      "abstract": "Catastrophic forgetting in continual learning is often measured at the performance or last-layer representation level, overlooking the underlying mechanisms. We introduce a mechanistic framework that offers a geometric interpretation of catastrophic forgetting as the result of transformations to the encoding of individual features. These transformations can lead to forgetting by reducing the allocated capacity of features (worse representation) and disrupting their readout by downstream computat",
      "tags": [
        "features"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2601-21996v1",
      "title": "Mechanistic Data Attribution: Tracing the Training Origins of Interpretable LLM Units",
      "authors": "Jianhui Chen, Yuzhang Luo, Liangming Pan",
      "date": "2026-01-29",
      "url": "https://arxiv.org/abs/2601.21996v1",
      "abstract": "While Mechanistic Interpretability has identified interpretable circuits in LLMs, their causal origins in training data remain elusive. We introduce Mechanistic Data Attribution (MDA), a scalable framework that employs Influence Functions to trace interpretable units back to specific training samples. Through extensive experiments on the Pythia family, we causally validate that targeted intervention--removing or augmenting a small fraction of high-influence samples--significantly modulates the e",
      "tags": [
        "circuits"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2601-22257v1",
      "title": "Symmetry Breaking in Transformers for Efficient and Interpretable Training",
      "authors": "Eva Silverstein, Daniel Kunin, Vasudev Shyam",
      "date": "2026-01-29",
      "url": "https://arxiv.org/abs/2601.22257v1",
      "abstract": "The attention mechanism in its standard implementation contains extraneous rotational degrees of freedom that are carried through computation but do not affect model activations or outputs. We introduce a simple symmetry-breaking protocol that inserts a preferred direction into this rotational space through batchwise-sampled, unlearned query and value biases. This modification has two theoretically motivated and empirically validated consequences. First, it can substantially improve the performa",
      "tags": [
        "theory"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2602-15038v1",
      "title": "Indic-TunedLens: Interpreting Multilingual Models in Indian Languages",
      "authors": "Mihir Panchal, Deeksha Varshney,  Mamta, Asif Ekbal",
      "date": "2026-01-29",
      "url": "https://arxiv.org/abs/2602.15038v1",
      "abstract": "Multilingual large language models (LLMs) are increasingly deployed in linguistically diverse regions like India, yet most interpretability tools remain tailored to English. Prior work reveals that LLMs often operate in English centric representation spaces, making cross lingual interpretability a pressing concern. We introduce Indic-TunedLens, a novel interpretability framework specifically for Indian languages that learns shared affine transformations. Unlike the standard Logit Lens, which dir",
      "tags": [],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2601-20420v1",
      "title": "Concept Component Analysis: A Principled Approach for Concept Extraction in LLMs",
      "authors": "Yuhang Liu, Erdun Gao, Dong Gong, Anton van den Hengel, Javen Qinfeng Shi",
      "date": "2026-01-28",
      "url": "https://arxiv.org/abs/2601.20420v1",
      "abstract": "Developing human understandable interpretation of large language models (LLMs) becomes increasingly critical for their deployment in essential domains. Mechanistic interpretability seeks to mitigate the issues through extracts human-interpretable process and concepts from LLMs' activations. Sparse autoencoders (SAEs) have emerged as a popular approach for extracting interpretable and monosemantic concepts by decomposing the LLM internal representations into a dictionary. Despite their empirical ",
      "tags": [
        "SAE",
        "features"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2601-19208v1",
      "title": "How Do Transformers Learn to Associate Tokens: Gradient Leading Terms Bring Mechanistic Interpretability",
      "authors": "Shawn Im, Changdae Oh, Zhen Fang, Sharon Li",
      "date": "2026-01-27",
      "url": "https://arxiv.org/abs/2601.19208v1",
      "abstract": "Semantic associations such as the link between \"bird\" and \"flew\" are foundational for language modeling as they enable models to go beyond memorization and instead generalize and generate coherent text. Understanding how these associations are learned and represented in language models is essential for connecting deep learning with linguistic theory and developing a mechanistic foundation for large language models. In this work, we analyze how these associations emerge from natural language data",
      "tags": [
        "theory"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2601-20075v1",
      "title": "Sparse CLIP: Co-Optimizing Interpretability and Performance in Contrastive Learning",
      "authors": "Chuan Qin, Constantin Venhoff, Sonia Joseph, Fanyi Xiao, Stefan Scherer",
      "date": "2026-01-27",
      "url": "https://arxiv.org/abs/2601.20075v1",
      "abstract": "Contrastive Language-Image Pre-training (CLIP) has become a cornerstone in vision-language representation learning, powering diverse downstream tasks and serving as the default vision backbone in multimodal large language models (MLLMs). Despite its success, CLIP's dense and opaque latent representations pose significant interpretability challenges. A common assumption is that interpretability and performance are in tension: enforcing sparsity during training degrades accuracy, motivating recent",
      "tags": [
        "vision"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2601-20028v1",
      "title": "Decomposing multimodal embedding spaces with group-sparse autoencoders",
      "authors": "Chiraag Kaushik, Davis Barch, Andrea Fanelli",
      "date": "2026-01-27",
      "url": "https://arxiv.org/abs/2601.20028v1",
      "abstract": "The Linear Representation Hypothesis asserts that the embeddings learned by neural networks can be understood as linear combinations of features corresponding to high-level concepts. Based on this ansatz, sparse autoencoders (SAEs) have recently become a popular method for decomposing embeddings into a sparse combination of linear directions, which have been shown empirically to often correspond to human-interpretable semantics. However, recent attempts to apply SAEs to multimodal embedding spac",
      "tags": [
        "SAE",
        "features",
        "vision"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2601-18483v1",
      "title": "Funny or Persuasive, but Not Both: Evaluating Fine-Grained Multi-Concept Control in LLMs",
      "authors": "Arya Labroo, Ivaxi Sheth, Vyas Raina, Amaani Ahmed, Mario Fritz",
      "date": "2026-01-26",
      "url": "https://arxiv.org/abs/2601.18483v1",
      "abstract": "Large Language Models (LLMs) offer strong generative capabilities, but many applications require explicit and \\textit{fine-grained} control over specific textual concepts, such as humor, persuasiveness, or formality. Prior approaches in prompting and representation engineering can provide coarse or single-attribute control, but systematic evaluation of multi-attribute settings remains limited. We introduce an evaluation framework for fine-grained controllability for both single- and dual-concept",
      "tags": [],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2601-18939v1",
      "title": "A Few Bad Neurons: Isolating and Surgically Correcting Sycophancy",
      "authors": "Claire O'Brien, Jessica Seto, Dristi Roy, Aditya Dwivedi, Sunishchal Dev, et al.",
      "date": "2026-01-26",
      "url": "https://arxiv.org/abs/2601.18939v1",
      "abstract": "Behavioral alignment in large language models (LLMs) is often achieved through broad fine-tuning, which can result in undesired side effects like distributional shift and low interpretability. We propose a method for alignment that identifies and updates only the neurons most responsible for a given behavior, a targeted approach that allows for fine-tuning with significantly less data. Using sparse autoencoders (SAEs) and linear probes, we isolate the 3% of MLP neurons most predictive of a targe",
      "tags": [
        "SAE",
        "safety",
        "probing"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2601-17952v1",
      "title": "A Monosemantic Attribution Framework for Stable Interpretability in Clinical Neuroscience Large Language Models",
      "authors": "Michail Mamalakis, Tiago Azevedo, Cristian Cosentino, Chiara D'Ercoli, Subati Abulikemu, et al.",
      "date": "2026-01-25",
      "url": "https://arxiv.org/abs/2601.17952v1",
      "abstract": "Interpretability remains a key challenge for deploying large language models (LLMs) in clinical settings such as Alzheimer's disease progression diagnosis, where early and trustworthy predictions are essential. Existing attribution methods exhibit high inter-method variability and unstable explanations due to the polysemantic nature of LLM representations, while mechanistic interpretability approaches lack direct alignment with model inputs and outputs and do not provide explicit importance scor",
      "tags": [
        "features",
        "safety"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2601-17958v1",
      "title": "TensorLens: End-to-End Transformer Analysis via High-Order Attention Tensors",
      "authors": "Ido Andrew Atad, Itamar Zimerman, Shahar Katz, Lior Wolf",
      "date": "2026-01-25",
      "url": "https://arxiv.org/abs/2601.17958v1",
      "abstract": "Attention matrices are fundamental to transformer research, supporting a broad range of applications including interpretability, visualization, manipulation, and distillation. Yet, most existing analyses focus on individual attention heads or layers, failing to account for the model's global behavior. While prior efforts have extended attention formulations across multiple heads via averaging and matrix multiplications or incorporated components such as normalization and FFNs, a unified and comp",
      "tags": [
        "attention",
        "vision"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2601-17188v1",
      "title": "Implementing Tensor Logic: Unifying Datalog and Neural Reasoning via Tensor Contraction",
      "authors": "Swapn Shah, Wlodek Zadrozny",
      "date": "2026-01-23",
      "url": "https://arxiv.org/abs/2601.17188v1",
      "abstract": "The unification of symbolic reasoning and neural networks remains a central challenge in artificial intelligence. Symbolic systems offer reliability and interpretability but lack scalability, while neural networks provide learning capabilities but sacrifice transparency. Tensor Logic, proposed by Domingos, suggests that logical rules and Einstein summation are mathematically equivalent, offering a principled path toward unification. This paper provides empirical validation of this framework thro",
      "tags": [
        "reasoning"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2601-15801v1",
      "title": "Attributing and Exploiting Safety Vectors through Global Optimization in Large Language Models",
      "authors": "Fengheng Chu, Jiahao Chen, Yuhong Wang, Jun Wang, Zhihui Fu, et al.",
      "date": "2026-01-22",
      "url": "https://arxiv.org/abs/2601.15801v1",
      "abstract": "While Large Language Models (LLMs) are aligned to mitigate risks, their safety guardrails remain fragile against jailbreak attacks. This reveals limited understanding of components governing safety. Existing methods rely on local, greedy attribution that assumes independent component contributions. However, they overlook the cooperative interactions between different components in LLMs, such as attention heads, which jointly contribute to safety mechanisms. We propose \\textbf{G}lobal \\textbf{O}p",
      "tags": [
        "safety",
        "attention"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2601-15122v1",
      "title": "From Insight to Intervention: Interpretable Neuron Steering for Controlling Popularity Bias in Recommender Systems",
      "authors": "Parviz Ahmadov, Masoud Mansoury",
      "date": "2026-01-21",
      "url": "https://arxiv.org/abs/2601.15122v1",
      "abstract": "Popularity bias is a pervasive challenge in recommender systems, where a few popular items dominate attention while the majority of less popular items remain underexposed. This imbalance can reduce recommendation quality and lead to unfair item exposure. Although existing mitigation methods address this issue to some extent, they often lack transparency in how they operate. In this paper, we propose a post-hoc approach, PopSteer, that leverages a Sparse Autoencoder (SAE) to both interpret and mi",
      "tags": [
        "SAE",
        "steering"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2601-14758v1",
      "title": "Mechanism Shift During Post-training from Autoregressive to Masked Diffusion Language Models",
      "authors": "Injin Kong, Hyoungjoon Lee, Yohan Jo",
      "date": "2026-01-21",
      "url": "https://arxiv.org/abs/2601.14758v1",
      "abstract": "Post-training pretrained Autoregressive models (ARMs) into Masked Diffusion models (MDMs) has emerged as a cost-effective strategy to overcome the limitations of sequential generation. However, the internal algorithmic transformations induced by this paradigm shift remain unexplored, leaving it unclear whether post-trained MDMs acquire genuine bidirectional reasoning capabilities or merely repackage autoregressive heuristics. In this work, we address this question by conducting a comparative cir",
      "tags": [
        "reasoning"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2601-15441v1",
      "title": "CASL: Concept-Aligned Sparse Latents for Interpreting Diffusion Models",
      "authors": "Zhenghao He, Guangzhi Xiong, Boyang Wang, Sanchit Sinha, Aidong Zhang",
      "date": "2026-01-21",
      "url": "https://arxiv.org/abs/2601.15441v1",
      "abstract": "Internal activations of diffusion models encode rich semantic information, but interpreting such representations remains challenging. While Sparse Autoencoders (SAEs) have shown promise in disentangling latent representations, existing SAE-based methods for diffusion model understanding rely on unsupervised approaches that fail to align sparse features with human-understandable concepts. This limits their ability to provide reliable semantic control over generated images. We introduce CASL (Conc",
      "tags": [
        "SAE",
        "features",
        "vision"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2601-15540v1",
      "title": "PRISM: Deriving the Transformer as a Signal-Denoising Operator via Maximum Coding Rate Reduction",
      "authors": "Dongchen Huang",
      "date": "2026-01-21",
      "url": "https://arxiv.org/abs/2601.15540v1",
      "abstract": "Deep learning models, particularly Transformers, are often criticized as \"black boxes\" and lack interpretability. We propose Prism, a white-box attention-based architecture derived from the principles of Maximizing Coding Rate Reduction ($\\text{MCR}^2$). By modeling the attention mechanism as a gradient ascent process on a distinct signal-noise manifold, we introduce two physical constraints: an overcomplete dictionary to expand the representational phase space, and an irrational frequency separ",
      "tags": [],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2601-15540v2",
      "title": "PRISM: Deriving a White-Box Transformer as a Signal-Noise Decomposition Operator via Maximum Coding Rate Reduction",
      "authors": "Dongchen Huang",
      "date": "2026-01-21",
      "url": "https://arxiv.org/abs/2601.15540v2",
      "abstract": "Deep learning models, particularly Transformers, are often criticized as \"black boxes\" and lack interpretability. We propose Prism, a white-box attention-based architecture derived from the principles of Maximizing Coding Rate Reduction ($\\text{MCR}^2$). By modeling the attention mechanism as a gradient ascent process on a distinct signal-noise manifold, we introduce a specific irrational frequency separation ($\u03c0$-RoPE) to enforce incoherence between signal (semantic) and noise (syntactic) subsp",
      "tags": [],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2602-11180v1",
      "title": "Mechanistic Interpretability for Large Language Model Alignment: Progress, Challenges, and Future Directions",
      "authors": "Usman Naseem",
      "date": "2026-01-21",
      "url": "https://arxiv.org/abs/2602.11180v1",
      "abstract": "Large language models (LLMs) have achieved remarkable capabilities across diverse tasks, yet their internal decision-making processes remain largely opaque. Mechanistic interpretability (i.e., the systematic study of how neural networks implement algorithms through their learned representations and computational structures) has emerged as a critical research direction for understanding and aligning these models. This paper surveys recent progress in mechanistic interpretability techniques applie",
      "tags": [
        "safety",
        "survey"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2601-14004v1",
      "title": "Locate, Steer, and Improve: A Practical Survey of Actionable Mechanistic Interpretability in Large Language Models",
      "authors": "Hengyuan Zhang, Zhihao Zhang, Mingyang Wang, Zunhai Su, Yiwei Wang, et al.",
      "date": "2026-01-20",
      "url": "https://arxiv.org/abs/2601.14004v1",
      "abstract": "Mechanistic Interpretability (MI) has emerged as a vital approach to demystify the opaque decision-making of Large Language Models (LLMs). However, existing reviews primarily treat MI as an observational science, summarizing analytical insights while lacking a systematic framework for actionable intervention. To bridge this gap, we present a practical survey structured around the pipeline: \"Locate, Steer, and Improve.\" We formally categorize Localizing (diagnosis) and Steering (intervention) met",
      "tags": [
        "steering",
        "survey"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2601-13548v1",
      "title": "Patterning: The Dual of Interpretability",
      "authors": "George Wang, Daniel Murfet",
      "date": "2026-01-20",
      "url": "https://arxiv.org/abs/2601.13548v1",
      "abstract": "Mechanistic interpretability aims to understand how neural networks generalize beyond their training data by reverse-engineering their internal structures. We introduce patterning as the dual problem: given a desired form of generalization, determine what training data produces it. Our approach is based on susceptibilities, which measure how posterior expectation values of observables respond to infinitesimal shifts in the data distribution. Inverting this linear response relationship yields the",
      "tags": [],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2601-12879v1",
      "title": "Hierarchical Sparse Circuit Extraction from Billion-Parameter Language Models through Scalable Attribution Graph Decomposition",
      "authors": "Mohammed Mudassir Uddin, Shahnawaz Alam, Mohammed Kaif Pasha",
      "date": "2026-01-19",
      "url": "https://arxiv.org/abs/2601.12879v1",
      "abstract": "Mechanistic interpretability seeks to reverse-engineer neural network computations into human-understandable algorithms, yet extracting sparse computational circuits from billion-parameter language models remains challenging due to exponential search complexity and pervasive polysemanticity. The proposed Hierarchical Attribution Graph Decomposition (HAGD) framework reduces circuit discovery complexity from O(2^n) exhaustive enumeration to O(n^2 log n) through multi-resolution abstraction hierarc",
      "tags": [
        "circuits",
        "features"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2601-12286v1",
      "title": "Conversational Context Classification: A Representation Engineering Approach",
      "authors": "Jonathan Pan",
      "date": "2026-01-18",
      "url": "https://arxiv.org/abs/2601.12286v1",
      "abstract": "The increasing prevalence of Large Language Models (LLMs) demands effective safeguards for their operation, particularly concerning their tendency to generate out-of-context responses. A key challenge is accurately detecting when LLMs stray from expected conversational norms, manifesting as topic shifts, factual inaccuracies, or outright hallucinations. Traditional anomaly detection struggles to directly apply within contextual semantics. This paper outlines our experiment in exploring the use o",
      "tags": [],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2601-11020v1",
      "title": "From Interpretability to Performance: Optimizing Retrieval Heads for Long-Context Language Models",
      "authors": "Youmi Ma, Naoaki Okazaki",
      "date": "2026-01-16",
      "url": "https://arxiv.org/abs/2601.11020v1",
      "abstract": "Advances in mechanistic interpretability have identified special attention heads, known as retrieval heads, that are responsible for retrieving information from the context. However, the role of these retrieval heads in improving model performance remains unexplored. This work investigates whether retrieval heads can be leveraged to enhance the long-context capabilities of LLMs. Specifically, we propose RetMask, a method that generates training signals by contrasting normal model outputs with th",
      "tags": [
        "attention"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2601-11182v1",
      "title": "From Knots to Knobs: Towards Steerable Collaborative Filtering Using Sparse Autoencoders",
      "authors": "Martin Spi\u0161\u00e1k, Ladislav Pe\u0161ka, Petr \u0160koda, Vojt\u011bch Van\u010dura, Rodrigo Alves",
      "date": "2026-01-16",
      "url": "https://arxiv.org/abs/2601.11182v1",
      "abstract": "Sparse autoencoders (SAEs) have recently emerged as pivotal tools for introspection into large language models. SAEs can uncover high-quality, interpretable features at different levels of granularity and enable targeted steering of the generation process by selectively activating specific neurons in their latent activations. Our paper is the first to apply this approach to collaborative filtering, aiming to extract similarly interpretable features from representations learned purely from intera",
      "tags": [
        "SAE",
        "features",
        "steering"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2601-11019v1",
      "title": "Finding the Translation Switch: Discovering and Exploiting the Task-Initiation Features in LLMs",
      "authors": "Xinwei Wu, Heng Liu, Xiaohu Zhao, Yuqi Ren, Linlong Xu, et al.",
      "date": "2026-01-16",
      "url": "https://arxiv.org/abs/2601.11019v1",
      "abstract": "Large Language Models (LLMs) frequently exhibit strong translation abilities, even without task-specific fine-tuning. However, the internal mechanisms governing this innate capability remain largely opaque. To demystify this process, we leverage Sparse Autoencoders (SAEs) and introduce a novel framework for identifying task-specific features. Our method first recalls features that are frequently co-activated on translation inputs and then filters them for functional coherence using a PCA-based c",
      "tags": [
        "SAE",
        "features"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2601-11061v1",
      "title": "Spurious Rewards Paradox: Mechanistically Understanding How RLVR Activates Memorization Shortcuts in LLMs",
      "authors": "Lecheng Yan, Ruizhe Li, Guanhua Chen, Qing Li, Jiahui Geng, et al.",
      "date": "2026-01-16",
      "url": "https://arxiv.org/abs/2601.11061v1",
      "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) is highly effective for enhancing LLM reasoning, yet recent evidence shows models like Qwen 2.5 achieve significant gains even with spurious or incorrect rewards. We investigate this phenomenon and identify a \"Perplexity Paradox\": spurious RLVR triggers a divergence where answer-token perplexity drops while prompt-side coherence degrades, suggesting the model is bypassing reasoning in favor of memorization. Using Path Patching, Logit Lens, JS",
      "tags": [
        "reasoning"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2601-11178v1",
      "title": "TANDEM: Temporal-Aware Neural Detection for Multimodal Hate Speech",
      "authors": "Girish A. Koushik, Helen Treharne, Diptesh Kanojia",
      "date": "2026-01-16",
      "url": "https://arxiv.org/abs/2601.11178v1",
      "abstract": "Social media platforms are increasingly dominated by long-form multimodal content, where harmful narratives are constructed through a complex interplay of audio, visual, and textual cues. While automated systems can flag hate speech with high accuracy, they often function as \"black boxes\" that fail to provide the granular, interpretable evidence, such as precise timestamps and target identities, required for effective human-in-the-loop moderation. In this work, we introduce TANDEM, a unified fra",
      "tags": [
        "safety",
        "vision"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2601-11683v1",
      "title": "Attesting Model Lineage by Consisted Knowledge Evolution with Fine-Tuning Trajectory",
      "authors": "Zhuoyi Shang, Jiasen Li, Pengzhen Chen, Yanwei Liu, Xiaoyan Gu, et al.",
      "date": "2026-01-16",
      "url": "https://arxiv.org/abs/2601.11683v1",
      "abstract": "The fine-tuning technique in deep learning gives rise to an emerging lineage relationship among models. This lineage provides a promising perspective for addressing security concerns such as unauthorized model redistribution and false claim of model provenance, which are particularly pressing in \\textcolor{blue}{open-weight model} libraries where robust lineage verification mechanisms are often lacking. Existing approaches to model lineage detection primarily rely on static architectural similar",
      "tags": [],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2601-10524v1",
      "title": "Diagnosing Generalization Failures in Fine-Tuned LLMs: A Cross-Architectural Study on Phishing Detection",
      "authors": "Frank Bobe, Gregory D. Vetaw, Chase Pavlick, Darshan Bryner, Matthew Cook, et al.",
      "date": "2026-01-15",
      "url": "https://arxiv.org/abs/2601.10524v1",
      "abstract": "The practice of fine-tuning Large Language Models (LLMs) has achieved state-of-the-art performance on specialized tasks, yet diagnosing why these models become brittle and fail to generalize remains a critical open problem. To address this, we introduce and apply a multi-layered diagnostic framework to a cross-architectural study. We fine-tune Llama 3.1 8B, Gemma 2 9B, and Mistral models on a high-stakes phishing detection task and use SHAP analysis and mechanistic interpretability to uncover th",
      "tags": [],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2601-10266v1",
      "title": "Measuring Affinity between Attention-Head Weight Subspaces via the Projection Kernel",
      "authors": "Hiroaki Yamagiwa, Yusuke Takase, Hidetoshi Shimodaira",
      "date": "2026-01-15",
      "url": "https://arxiv.org/abs/2601.10266v1",
      "abstract": "Understanding relationships between attention heads is essential for interpreting the internal structure of Transformers, yet existing metrics do not capture this structure well. We focus on the subspaces spanned by attention-head weight matrices and quantify head-to-head relationships using the Projection Kernel (PK), a principal-angle-based measure of subspace similarity. Experiments show that PK reproduces known head-to-head interactions on the IOI task more clearly than prior metrics such as",
      "tags": [
        "attention"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2601-10825v1",
      "title": "Reasoning Models Generate Societies of Thought",
      "authors": "Junsol Kim, Shiyang Lai, Nino Scherrer, Blaise Ag\u00fcera y Arcas, James Evans",
      "date": "2026-01-15",
      "url": "https://arxiv.org/abs/2601.10825v1",
      "abstract": "Large language models have achieved remarkable capabilities across domains, yet mechanisms underlying sophisticated reasoning remain elusive. Recent reasoning models outperform comparable instruction-tuned models on complex cognitive tasks, attributed to extended computation through longer chains of thought. Here we show that enhanced reasoning emerges not from extended computation alone, but from simulating multi-agent-like interactions -- a society of thought -- which enables diversification a",
      "tags": [
        "reasoning"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2601-09445v1",
      "title": "Where Knowledge Collides: A Mechanistic Study of Intra-Memory Knowledge Conflict in Language Models",
      "authors": "Minh Vu Pham, Hsuvas Borkakoty, Yufang Hou",
      "date": "2026-01-14",
      "url": "https://arxiv.org/abs/2601.09445v1",
      "abstract": "In language models (LMs), intra-memory knowledge conflict largely arises when inconsistent information about the same event is encoded within the model's parametric knowledge. While prior work has primarily focused on resolving conflicts between a model's internal knowledge and external resources through approaches such as fine-tuning or knowledge editing, the problem of localizing conflicts that originate during pre-training within the model's internal representations remain unexplored. In this",
      "tags": [
        "editing"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2602-02496v1",
      "title": "The Hypocrisy Gap: Quantifying Divergence Between Internal Belief and Chain-of-Thought Explanation via Sparse Autoencoders",
      "authors": "Shikhar Shiromani, Archie Chaudhury, Sri Pranav Kunda",
      "date": "2026-01-14",
      "url": "https://arxiv.org/abs/2602.02496v1",
      "abstract": "Large Language Models (LLMs) frequently exhibit unfaithful behavior, producing a final answer that differs significantly from their internal chain of thought (CoT) reasoning in order to appease the user they are conversing with. In order to better detect this behavior, we introduce the Hypocrisy Gap, a mechanistic metric utilizing Sparse Autoencoders (SAEs) to quantify the divergence between a model's internal reasoning and its final generation. By mathematically comparing an internal truth beli",
      "tags": [
        "SAE",
        "reasoning"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2601-08331v1",
      "title": "CLaS-Bench: A Cross-Lingual Alignment and Steering Benchmark",
      "authors": "Daniil Gurgurov, Yusser Al Ghussin, Tanja Baeumel, Cheng-Ting Chou, Patrick Schramowski, et al.",
      "date": "2026-01-13",
      "url": "https://arxiv.org/abs/2601.08331v1",
      "abstract": "Understanding and controlling the behavior of large language models (LLMs) is an increasingly important topic in multilingual NLP. Beyond prompting or fine-tuning, , i.e.,~manipulating internal representations during inference, has emerged as a more efficient and interpretable technique for adapting models to a target language. Yet, no dedicated benchmarks or evaluation protocols exist to quantify the effectiveness of steering techniques. We introduce CLaS-Bench, a lightweight parallel-question ",
      "tags": [
        "safety",
        "steering"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2601-08058v1",
      "title": "Reasoning Beyond Chain-of-Thought: A Latent Computational Mode in Large Language Models",
      "authors": "Zhenghao He, Guangzhi Xiong, Bohan Liu, Sanchit Sinha, Aidong Zhang",
      "date": "2026-01-12",
      "url": "https://arxiv.org/abs/2601.08058v1",
      "abstract": "Chain-of-Thought (CoT) prompting has improved the reasoning performance of large language models (LLMs), but it remains unclear why it works and whether it is the unique mechanism for triggering reasoning in large language models. In this work, we study this question by directly analyzing and intervening on the internal representations of LLMs with Sparse Autoencoders (SAEs), identifying a small set of latent features that are causally associated with LLM reasoning behavior. Across multiple mode",
      "tags": [
        "SAE",
        "features",
        "reasoning"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2601-08070v1",
      "title": "Semantic Gravity Wells: Why Negative Constraints Backfire",
      "authors": "Shailesh Rana",
      "date": "2026-01-12",
      "url": "https://arxiv.org/abs/2601.08070v1",
      "abstract": "Negative constraints (instructions of the form \"do not use word X\") represent a fundamental test of instruction-following capability in large language models. Despite their apparent simplicity, these constraints fail with striking regularity, and the conditions governing failure have remained poorly understood. This paper presents the first comprehensive mechanistic investigation of negative instruction failure. We introduce semantic pressure, a quantitative measure of the model's intrinsic prob",
      "tags": [],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2601-11622v1",
      "title": "Dynamical Systems Analysis Reveals Functional Regimes in Large Language Models",
      "authors": "Hassan Ugail, Newton Howard",
      "date": "2026-01-11",
      "url": "https://arxiv.org/abs/2601.11622v1",
      "abstract": "Large language models perform text generation through high-dimensional internal dynamics, yet the temporal organisation of these dynamics remains poorly understood. Most interpretability approaches emphasise static representations or causal interventions, leaving temporal structure largely unexplored. Drawing on neuroscience, where temporal integration and metastability are core markers of neural organisation, we adapt these concepts to transformer models and discuss a composite dynamical metric",
      "tags": [],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2601-06437v1",
      "title": "Time Travel Engine: A Shared Latent Chronological Manifold Enables Historical Navigation in Large Language Models",
      "authors": "Jingmin An, Wei Liu, Qian Wang, Fang Fang",
      "date": "2026-01-10",
      "url": "https://arxiv.org/abs/2601.06437v1",
      "abstract": "Time functions as a fundamental dimension of human cognition, yet the mechanisms by which Large Language Models (LLMs) encode chronological progression remain opaque. We demonstrate that temporal information in their latent space is organized not as discrete clusters but as a continuous, traversable geometry. We introduce the Time Travel Engine (TTE), an interpretability-driven framework that projects diachronic linguistic patterns onto a shared chronological manifold. Unlike surface-level promp",
      "tags": [],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2601-06478v1",
      "title": "Deriving Decoder-Free Sparse Autoencoders from First Principles",
      "authors": "Alan Oursland",
      "date": "2026-01-10",
      "url": "https://arxiv.org/abs/2601.06478v1",
      "abstract": "Gradient descent on log-sum-exp (LSE) objectives performs implicit expectation--maximization (EM): the gradient with respect to each component output equals its responsibility. The same theory predicts collapse without volume control analogous to the log-determinant in Gaussian mixture models. We instantiate the theory in a single-layer encoder with an LSE objective and InfoMax regularization for volume control. Experiments confirm the theory's predictions. The gradient--responsibility identity ",
      "tags": [
        "SAE",
        "theory"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2601-05679v1",
      "title": "Do Sparse Autoencoders Identify Reasoning Features in Language Models?",
      "authors": "George Ma, Zhongyuan Liang, Irene Y. Chen, Somayeh Sojoudi",
      "date": "2026-01-09",
      "url": "https://arxiv.org/abs/2601.05679v1",
      "abstract": "We investigate whether sparse autoencoders (SAEs) identify genuine reasoning features in large language models (LLMs). Starting from features selected using standard contrastive activation methods, we introduce a falsification-oriented framework that combines causal token injection experiments and LLM-guided falsification to test whether feature activation reflects reasoning processes or superficial linguistic correlates. Across 20 configurations spanning multiple model families, layers, and rea",
      "tags": [
        "SAE",
        "features",
        "reasoning"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2601-05939v1",
      "title": "Context-Aware Decoding for Faithful Vision-Language Generation",
      "authors": "Mehrdad Fazli, Bowen Wei, Ziwei Zhu",
      "date": "2026-01-09",
      "url": "https://arxiv.org/abs/2601.05939v1",
      "abstract": "Hallucinations, generating responses inconsistent with the visual input, remain a critical limitation of large vision-language models (LVLMs), especially in open-ended tasks such as image captioning and visual reasoning. In this work, we probe the layer-wise generation dynamics that drive hallucinations and propose a training-free mitigation strategy. Employing the Logit Lens, we examine how LVLMs construct next-token distributions across decoder layers, uncovering a pronounced commitment-depth ",
      "tags": [
        "probing",
        "vision",
        "reasoning"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2601-06338v1",
      "title": "Circuit Mechanisms for Spatial Relation Generation in Diffusion Transformers",
      "authors": "Binxu Wang, Jingxuan Fan, Xu Pan",
      "date": "2026-01-09",
      "url": "https://arxiv.org/abs/2601.06338v1",
      "abstract": "Diffusion Transformers (DiTs) have greatly advanced text-to-image generation, but models still struggle to generate the correct spatial relations between objects as specified in the text prompt. In this study, we adopt a mechanistic interpretability approach to investigate how a DiT can generate correct spatial relations between objects. We train, from scratch, DiTs of different sizes with different text encoders to learn to generate images containing two objects whose attributes and spatial rel",
      "tags": [
        "circuits",
        "vision"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2601-05679v5",
      "title": "Falsifying Sparse Autoencoder Reasoning Features in Language Models",
      "authors": "George Ma, Zhongyuan Liang, Irene Y. Chen, Somayeh Sojoudi",
      "date": "2026-01-09",
      "url": "https://arxiv.org/abs/2601.05679v5",
      "abstract": "We study how reliably sparse autoencoders (SAEs) support claims about reasoning-related internal features in large language models. We first give a stylized analysis showing that sparsity-regularized decoding can preferentially retain stable low-dimensional correlates while suppressing high-dimensional within-behavior variation, motivating the possibility that contrastively selected \"reasoning\" features may concentrate on cue-like structure when such cues are coupled with reasoning traces. Build",
      "tags": [
        "SAE",
        "features",
        "reasoning"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2601-04768v1",
      "title": "LANGSAE EDITING: Improving Multilingual Information Retrieval via Post-hoc Language Identity Removal",
      "authors": "Dongjun Kim, Jeongho Yoon, Chanjun Park, Heuiseok Lim",
      "date": "2026-01-08",
      "url": "https://arxiv.org/abs/2601.04768v1",
      "abstract": "Dense retrieval in multilingual settings often searches over mixed-language collections, yet multilingual embeddings encode language identity alongside semantics. This language signal can inflate similarity for same-language pairs and crowd out relevant evidence written in other languages. We propose LANGSAE EDITING, a post-hoc sparse autoencoder trained on pooled embeddings that enables controllable removal of language-identity signal directly in vector space. The method identifies language-ass",
      "tags": [
        "SAE"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2601-04670v1",
      "title": "Learning Dynamics in RL Post-Training for Language Models",
      "authors": "Akiyoshi Tomihari",
      "date": "2026-01-08",
      "url": "https://arxiv.org/abs/2601.04670v1",
      "abstract": "Reinforcement learning (RL) post-training is a critical stage in modern language model development, playing a key role in improving alignment and reasoning ability. However, several phenomena remain poorly understood, including the reduction in output diversity. To gain a broader understanding of RL post-training, we analyze the learning dynamics of RL post-training from a perspective that has been studied in supervised learning but remains underexplored in RL. We adopt an empirical neural tange",
      "tags": [
        "safety",
        "reasoning"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2601-04480v1",
      "title": "When Models Manipulate Manifolds: The Geometry of a Counting Task",
      "authors": "Wes Gurnee, Emmanuel Ameisen, Isaac Kauvar, Julius Tarng, Adam Pearce, et al.",
      "date": "2026-01-08",
      "url": "https://arxiv.org/abs/2601.04480v1",
      "abstract": "Language models can perceive visual properties of text despite receiving only sequences of tokens-we mechanistically investigate how Claude 3.5 Haiku accomplishes one such task: linebreaking in fixed-width text. We find that character counts are represented on low-dimensional curved manifolds discretized by sparse feature families, analogous to biological place cells. Accurate predictions emerge from a sequence of geometric transformations: token lengths are accumulated into character count mani",
      "tags": [
        "features",
        "vision",
        "biology"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2601-03595v1",
      "title": "Controllable LLM Reasoning via Sparse Autoencoder-Based Steering",
      "authors": "Yi Fang, Wenjie Wang, Mingfeng Xue, Boyi Deng, Fengli Xu, et al.",
      "date": "2026-01-07",
      "url": "https://arxiv.org/abs/2601.03595v1",
      "abstract": "Large Reasoning Models (LRMs) exhibit human-like cognitive reasoning strategies (e.g. backtracking, cross-verification) during reasoning process, which improves their performance on complex tasks. Currently, reasoning strategies are autonomously selected by LRMs themselves. However, such autonomous selection often produces inefficient or even erroneous reasoning paths. To make reasoning more reliable and flexible, it is important to develop methods for controlling reasoning strategies. Existing ",
      "tags": [
        "SAE",
        "steering",
        "reasoning"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2601-03671v1",
      "title": "NeuronScope: A Multi-Agent Framework for Explaining Polysemantic Neurons in Language Models",
      "authors": "Weiqi Liu, Yongliang Miao, Haiyan Zhao, Yanguang Liu, Mengnan Du",
      "date": "2026-01-07",
      "url": "https://arxiv.org/abs/2601.03671v1",
      "abstract": "Neuron-level interpretation in large language models (LLMs) is fundamentally challenged by widespread polysemanticity, where individual neurons respond to multiple distinct semantic concepts. Existing single-pass interpretation methods struggle to faithfully capture such multi-concept behavior. In this work, we propose NeuronScope, a multi-agent framework that reformulates neuron interpretation as an iterative, activation-guided process. NeuronScope explicitly deconstructs neuron activations int",
      "tags": [
        "features"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2601-03798v1",
      "title": "Where meaning lives: Layer-wise accessibility of psycholinguistic features in encoder and decoder language models",
      "authors": "Taisiia Tikhomirova, Dirk U. Wulff",
      "date": "2026-01-07",
      "url": "https://arxiv.org/abs/2601.03798v1",
      "abstract": "Understanding where transformer language models encode psychologically meaningful aspects of meaning is essential for both theory and practice. We conduct a systematic layer-wise probing study of 58 psycholinguistic features across 10 transformer models, spanning encoder-only and decoder-only architectures, and compare three embedding extraction methods. We find that apparent localization of meaning is strongly method-dependent: contextualized embeddings yield higher feature-specific selectivity",
      "tags": [
        "features",
        "probing",
        "theory"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2601-04398v1",
      "title": "Interpreting Transformers Through Attention Head Intervention",
      "authors": "Mason Kadem, Rong Zheng",
      "date": "2026-01-07",
      "url": "https://arxiv.org/abs/2601.04398v1",
      "abstract": "Neural networks are growing more capable on their own, but we do not understand their neural mechanisms. Understanding these mechanisms' decision-making processes, or mechanistic interpretability, enables (1) accountability and control in high-stakes domains, (2) the study of digital brains and the emergence of cognition, and (3) discovery of new knowledge when AI systems outperform humans.",
      "tags": [
        "attention"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2601-03047v1",
      "title": "When the Coffee Feature Activates on Coffins: An Analysis of Feature Extraction and Steering for Mechanistic Interpretability",
      "authors": "Raphael Ronge, Markus Maier, Frederick Eberhardt",
      "date": "2026-01-06",
      "url": "https://arxiv.org/abs/2601.03047v1",
      "abstract": "Recent work by Anthropic on Mechanistic interpretability claims to understand and control Large Language Models by extracting human-interpretable features from their neural activation patterns using sparse autoencoders (SAEs). If successful, this approach offers one of the most promising routes for human oversight in AI safety. We conduct an initial stress-test of these claims by replicating their main results with open-source SAEs for Llama 3.1. While we successfully reproduce basic feature ext",
      "tags": [
        "SAE",
        "features",
        "safety",
        "steering"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2601-02989v1",
      "title": "Mechanistic Interpretability of Large-Scale Counting in LLMs through a System-2 Strategy",
      "authors": "Hosein Hasani, Mohammadali Banayeeanzade, Ali Nafisi, Sadegh Mohammadian, Fatemeh Askari, et al.",
      "date": "2026-01-06",
      "url": "https://arxiv.org/abs/2601.02989v1",
      "abstract": "Large language models (LLMs), despite strong performance on complex mathematical problems, exhibit systematic limitations in counting tasks. This issue arises from architectural limits of transformers, where counting is performed across layers, leading to degraded precision for larger counting problems due to depth constraints. To address this limitation, we propose a simple test-time strategy inspired by System-2 cognitive processes that decomposes large counting tasks into smaller, independent",
      "tags": [],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2601-02978v1",
      "title": "Mechanistic Knobs in LLMs: Retrieving and Steering High-Order Semantic Features via Sparse Autoencoders",
      "authors": "Ruikang Zhang, Shuo Wang, Qi Su",
      "date": "2026-01-06",
      "url": "https://arxiv.org/abs/2601.02978v1",
      "abstract": "Recent work in Mechanistic Interpretability (MI) has enabled the identification and intervention of internal features in Large Language Models (LLMs). However, a persistent challenge lies in linking such internal features to the reliable control of complex, behavior-level semantic attributes in language generation. In this paper, we propose a Sparse Autoencoder-based framework for retrieving and steering semantically interpretable internal features associated with high-level linguistic behaviors",
      "tags": [
        "SAE",
        "features",
        "steering"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2601-02896v1",
      "title": "Bridging Mechanistic Interpretability and Prompt Engineering with Gradient Ascent for Interpretable Persona Control",
      "authors": "Harshvardhan Saini, Yiming Tang, Dianbo Liu",
      "date": "2026-01-06",
      "url": "https://arxiv.org/abs/2601.02896v1",
      "abstract": "Controlling emergent behavioral personas (e.g., sycophancy, hallucination) in Large Language Models (LLMs) is critical for AI safety, yet remains a persistent challenge. Existing solutions face a dilemma: manual prompt engineering is intuitive but unscalable and imprecise, while automatic optimization methods are effective but operate as \"black boxes\" with no interpretable connection to model internals. We propose a novel framework that adapts gradient ascent to LLMs, enabling targeted prompt di",
      "tags": [
        "safety"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2601-02668v1",
      "title": "MAFS: Multi-head Attention Feature Selection for High-Dimensional Data via Deep Fusion of Filter Methods",
      "authors": "Xiaoyan Sun, Qingyu Meng, Yalu Wen",
      "date": "2026-01-06",
      "url": "https://arxiv.org/abs/2601.02668v1",
      "abstract": "Feature selection is essential for high-dimensional biomedical data, enabling stronger predictive performance, reduced computational cost, and improved interpretability in precision medicine applications. Existing approaches face notable challenges. Filter methods are highly scalable but cannot capture complex relationships or eliminate redundancy. Deep learning-based approaches can model nonlinear patterns but often lack stability, interpretability, and efficiency at scale. Single-head attentio",
      "tags": [
        "features"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2601-03300v1",
      "title": "TRYLOCK: Defense-in-Depth Against LLM Jailbreaks via Layered Preference and Representation Engineering",
      "authors": "Scott Thornton",
      "date": "2026-01-06",
      "url": "https://arxiv.org/abs/2601.03300v1",
      "abstract": "Large language models remain vulnerable to jailbreak attacks, and single-layer defenses often trade security for usability. We present TRYLOCK, the first defense-in-depth architecture that combines four heterogeneous mechanisms across the inference stack: weight-level safety alignment via DPO, activation-level control via Representation Engineering (RepE) steering, adaptive steering strength selected by a lightweight sidecar classifier, and input canonicalization to neutralize encoding-based byp",
      "tags": [
        "safety",
        "probing",
        "steering"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2601-01089v1",
      "title": "Central Dogma Transformer: Towards Mechanism-Oriented AI for Cellular Understanding",
      "authors": "Nobuyuki Ota",
      "date": "2026-01-03",
      "url": "https://arxiv.org/abs/2601.01089v1",
      "abstract": "Understanding cellular mechanisms requires integrating information across DNA, RNA, and protein - the three molecular systems linked by the Central Dogma of molecular biology. While domain-specific foundation models have achieved success for each modality individually, they remain isolated, limiting our ability to model integrated cellular processes. Here we present the Central Dogma Transformer (CDT), an architecture that integrates pre-trained language models for DNA, RNA, and protein followin",
      "tags": [
        "biology"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2601-06111v1",
      "title": "LLM-Powered Social Digital Twins: A Framework for Simulating Population Behavioral Response to Policy Interventions",
      "authors": "Aayush Gupta, Farahan Raza Sheikh",
      "date": "2026-01-03",
      "url": "https://arxiv.org/abs/2601.06111v1",
      "abstract": "Predicting how populations respond to policy interventions is a fundamental challenge in computational social science and public policy. Traditional approaches rely on aggregate statistical models that capture historical correlations but lack mechanistic interpretability and struggle with novel policy scenarios. We present a general framework for constructing Social Digital Twins - virtual population replicas where Large Language Models (LLMs) serve as cognitive engines for individual agents. Ea",
      "tags": [],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2601-06109v1",
      "title": "CBMAS: Cognitive Behavioral Modeling via Activation Steering",
      "authors": "Ahmed H. Ismail, Anthony Kuang, Ayo Akinkugbe, Kevin Zhu, Sean O'Brien",
      "date": "2026-01-03",
      "url": "https://arxiv.org/abs/2601.06109v1",
      "abstract": "Large language models (LLMs) often encode cognitive behaviors unpredictably across prompts, layers, and contexts, making them difficult to diagnose and control. We present CBMAS, a diagnostic framework for continuous activation steering, which extends cognitive bias analysis from discrete before/after interventions to interpretable trajectories. By combining steering vector construction with dense \u03b1-sweeps, logit lens-based bias curves, and layer-site sensitivity analysis, our approach can revea",
      "tags": [
        "steering"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2601-06111v2",
      "title": "LLM Powered Social Digital Twins: A Framework for Simulating Population Behavioral Response to Policy Interventions",
      "authors": "Fatima Koaik, Aayush Gupta, Farahan Raza Sheikh",
      "date": "2026-01-03",
      "url": "https://arxiv.org/abs/2601.06111v2",
      "abstract": "Predicting how populations respond to policy interventions is a fundamental challenge in computational social science and public policy. Traditional approaches rely on aggregate statistical models that capture historical correlations but lack mechanistic interpretability and struggle with novel policy scenarios. We present a general framework for constructing Social Digital Twins - virtual population replicas where Large Language Models (LLMs) serve as cognitive engines for individual agents. Ea",
      "tags": [],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2601-00968v1",
      "title": "Explainability-Guided Defense: Attribution-Aware Model Refinement Against Adversarial Data Attacks",
      "authors": "Longwei Wang, Mohammad Navid Nayyem, Abdullah Al Rakin, KC Santosh, Chaowei Zhang, et al.",
      "date": "2026-01-02",
      "url": "https://arxiv.org/abs/2601.00968v1",
      "abstract": "The growing reliance on deep learning models in safety-critical domains such as healthcare and autonomous navigation underscores the need for defenses that are both robust to adversarial perturbations and transparent in their decision-making. In this paper, we identify a connection between interpretability and robustness that can be directly leveraged during training. Specifically, we observe that spurious, unstable, or semantically irrelevant features identified through Local Interpretable Mode",
      "tags": [
        "features",
        "safety"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2601-00267v1",
      "title": "ActErase: A Training-Free Paradigm for Precise Concept Erasure via Activation Patching",
      "authors": "Yi Sun, Xinhao Zhong, Hongyan Li, Yimin Zhou, Junhao Li, et al.",
      "date": "2026-01-01",
      "url": "https://arxiv.org/abs/2601.00267v1",
      "abstract": "Recent advances in text-to-image diffusion models have demonstrated remarkable generation capabilities, yet they raise significant concerns regarding safety, copyright, and ethical implications. Existing concept erasure methods address these risks by removing sensitive concepts from pre-trained models, but most of them rely on data-intensive and computationally expensive fine-tuning, which poses a critical limitation. To overcome these challenges, inspired by the observation that the model's act",
      "tags": [
        "safety",
        "vision"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2512-24842v1",
      "title": "Triangulation as an Acceptance Rule for Multilingual Mechanistic Interpretability",
      "authors": "Yanan Long",
      "date": "2025-12-31",
      "url": "https://arxiv.org/abs/2512.24842v1",
      "abstract": "Multilingual language models achieve strong aggregate performance yet often behave unpredictably across languages, scripts, and cultures. We argue that mechanistic explanations for such models should satisfy a \\emph{causal} standard: claims must survive causal interventions and must \\emph{cross-reference} across environments that perturb surface form while preserving meaning. We formalize \\emph{reference families} as predicate-preserving variants and introduce \\emph{triangulation}, an acceptance",
      "tags": [],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2512-24975v1",
      "title": "Attribution-Guided Distillation of Matryoshka Sparse Autoencoders",
      "authors": "Cristina P. Martin-Linares, Jonathan P. Ling",
      "date": "2025-12-31",
      "url": "https://arxiv.org/abs/2512.24975v1",
      "abstract": "Sparse autoencoders (SAEs) aim to disentangle model activations into monosemantic, human-interpretable features. In practice, learned features are often redundant and vary across training runs and sparsity levels, which makes interpretations difficult to transfer and reuse. We introduce Distilled Matryoshka Sparse Autoencoders (DMSAEs), a training pipeline that distills a compact core of consistently useful features and reuses it to train new SAEs. DMSAEs run an iterative distillation cycle: tra",
      "tags": [
        "SAE",
        "features"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2512-24711v1",
      "title": "MEIC-DT: Memory-Efficient Incremental Clustering for Long-Text Coreference Resolution with Dual-Threshold Constraints",
      "authors": "Kangyang Luo, Shuzheng Si, Yuzhuo Bai, Cheng Gao, Zhitong Wang, et al.",
      "date": "2025-12-31",
      "url": "https://arxiv.org/abs/2512.24711v1",
      "abstract": "In the era of large language models (LLMs), supervised neural methods remain the state-of-the-art (SOTA) for Coreference Resolution. Yet, their full potential is underexplored, particularly in incremental clustering, which faces the critical challenge of balancing efficiency with performance for long texts. To address the limitation, we propose \\textbf{MEIC-DT}, a novel dual-threshold, memory-efficient incremental clustering approach based on a lightweight Transformer. MEIC-DT features a dual-th",
      "tags": [
        "features"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2512-23988v1",
      "title": "Fantastic Reasoning Behaviors and Where to Find Them: Unsupervised Discovery of the Reasoning Process",
      "authors": "Zhenyu Zhang, Shujian Zhang, John Lambert, Wenxuan Zhou, Zhangyang Wang, et al.",
      "date": "2025-12-30",
      "url": "https://arxiv.org/abs/2512.23988v1",
      "abstract": "Despite the growing reasoning capabilities of recent large language models (LLMs), their internal mechanisms during the reasoning process remain underexplored. Prior approaches often rely on human-defined concepts (e.g., overthinking, reflection) at the word level to analyze reasoning in a supervised manner. However, such methods are limited, as it is infeasible to capture the full spectrum of potential reasoning behaviors, many of which are difficult to define in token space. In this work, we p",
      "tags": [
        "reasoning"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2512-23837v1",
      "title": "Adversarial Lens: Exploiting Attention Layers to Generate Adversarial Examples for Evaluation",
      "authors": "Kaustubh Dhole",
      "date": "2025-12-29",
      "url": "https://arxiv.org/abs/2512.23837v1",
      "abstract": "Recent advances in mechanistic interpretability suggest that intermediate attention layers encode token-level hypotheses that are iteratively refined toward the final output. In this work, we exploit this property to generate adversarial examples directly from attention-layer token distributions. Unlike prompt-based or gradient-based attacks, our approach leverages model-internal token predictions, producing perturbations that are both plausible and internally consistent with the model's own gen",
      "tags": [],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2512-23260v1",
      "title": "Interpretable Safety Alignment via SAE-Constructed Low-Rank Subspace Adaptation",
      "authors": "Dianyun Wang, Qingsen Ma, Yuhu Shang, Zhifeng Lu, Lechen Ning, et al.",
      "date": "2025-12-29",
      "url": "https://arxiv.org/abs/2512.23260v1",
      "abstract": "Parameter-efficient fine-tuning has become the dominant paradigm for adapting large language models to downstream tasks. Low-rank adaptation methods such as LoRA operate under the assumption that task-relevant weight updates reside in a low-rank subspace, yet this subspace is learned implicitly from data in a black-box manner, offering no interpretability or direct control. We hypothesize that this difficulty stems from polysemanticity--individual dimensions encoding multiple entangled concepts.",
      "tags": [
        "features",
        "safety"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "ss-fe33646738431bec8b74",
      "title": "Analysis of Short-Circuit Impedance in Multi-Winding Transformer",
      "authors": "Emir Y\u00fckselen",
      "date": "2025-12-29",
      "url": "https://www.semanticscholar.org/paper/fe33646738431bec8b7421c6b91815e74f327618",
      "abstract": "Due to green energy policies, investments in renewable energy are rapidly increasing, which in turn raises the demand for efficient power plant integration into electrical grids. Multi-winding distribution transformers are crucial in this process, as they enable operation at various voltage levels to match different grid requirements. However, designing such transformers, mainly calculating short-circuit impedance in accordance with industry standards, is a highly complex and challenging task wh",
      "tags": [
        "circuits"
      ],
      "source": "Semantic Scholar",
      "featured": false
    },
    {
      "id": "arxiv-2512-23043v1",
      "title": "Mechanistic Analysis of Circuit Preservation in Federated Learning",
      "authors": "Muhammad Haseeb, Salaar Masood, Muhammad Abdullah Sohail",
      "date": "2025-12-28",
      "url": "https://arxiv.org/abs/2512.23043v1",
      "abstract": "Federated Learning (FL) enables collaborative training of models on decentralized data, but its performance degrades significantly under Non-IID (non-independent and identically distributed) data conditions. While this accuracy loss is well-documented, the internal mechanistic causes remain a black box. This paper investigates the canonical FedAvg algorithm through the lens of Mechanistic Interpretability (MI) to diagnose this failure mode. We hypothesize that the aggregation of conflicting clie",
      "tags": [
        "circuits"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2512-22511v1",
      "title": "Decomposing Task Vectors for Refined Model Editing",
      "authors": "Hamed Damirchi, Ehsan Abbasnejad, Zhen Zhang, Javen Shi",
      "date": "2025-12-27",
      "url": "https://arxiv.org/abs/2512.22511v1",
      "abstract": "Large pre-trained models have transformed machine learning, yet adapting these models effectively to exhibit precise, concept-specific behaviors remains a significant challenge. Task vectors, defined as the difference between fine-tuned and pre-trained model parameters, provide a mechanism for steering neural networks toward desired behaviors. This has given rise to large repositories dedicated to task vectors tailored for specific behaviors. The arithmetic operation of these task vectors allows",
      "tags": [
        "steering",
        "editing"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2512-21670v1",
      "title": "The Deepfake Detective: Interpreting Neural Forensics Through Sparse Features and Manifolds",
      "authors": "Subramanyam Sahoo, Jared Junkin",
      "date": "2025-12-25",
      "url": "https://arxiv.org/abs/2512.21670v1",
      "abstract": "Deepfake detection models have achieved high accuracy in identifying synthetic media, but their decision processes remain largely opaque. In this paper we present a mechanistic interpretability framework for deepfake detection applied to a vision-language model. Our approach combines a sparse autoencoder (SAE) analysis of internal network representations with a novel forensic manifold analysis that probes how the model's features respond to controlled forensic artifact manipulations. We demonstr",
      "tags": [
        "SAE",
        "features",
        "probing",
        "vision"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2512-21643v2",
      "title": "Omni-Weather: Unified Multimodal Foundation Model for Weather Generation and Understanding",
      "authors": "Zhiwang Zhou, Yuandong Pu, Xuming He, Yidi Liu, Yixin Chen, et al.",
      "date": "2025-12-25",
      "url": "https://arxiv.org/abs/2512.21643v2",
      "abstract": "Weather modeling requires both accurate prediction and mechanistic interpretation, yet existing methods treat these goals in isolation, separating generation from understanding. To address this gap, we present Omni-Weather, the first multimodal foundation model that unifies weather generation and understanding within a single architecture. Omni-Weather integrates a radar encoder for weather generation tasks, followed by unified processing using a shared self-attention mechanism. Moreover, we con",
      "tags": [
        "vision"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2512-22293v1",
      "title": "Learning from Negative Examples: Why Warning-Framed Training Data Teaches What It Warns Against",
      "authors": "Tsogt-Ochir Enkhbayar",
      "date": "2025-12-25",
      "url": "https://arxiv.org/abs/2512.22293v1",
      "abstract": "Warning-framed content in training data (e.g., \"DO NOT USE - this code is vulnerable\") does not, it turns out, teach language models to avoid the warned-against behavior. In experiments reported here, models exposed to such warnings reproduced the flagged content at rates statistically indistinguishable from models given the content directly (76.7% vs. 83.3%). Why? Sparse autoencoder analysis points to a failure of orthogonalization: \"describing X\" and \"performing X\" activate overlapping latent ",
      "tags": [
        "SAE"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "ss-0544168cf2012e31794a",
      "title": "Mechanistic Interpretability of Fine-Tuned Protein Language Models for Nanobody Thermostability Prediction",
      "authors": "Taihei Murakami, Yuki Hashidate, Yasuhiro Matsunaga",
      "date": "2025-12-24",
      "url": "https://www.semanticscholar.org/paper/0544168cf2012e31794a69547a7537994417b47d",
      "abstract": "",
      "tags": [
        "biology"
      ],
      "source": "Semantic Scholar",
      "featured": false
    },
    {
      "id": "arxiv-2512-20796v1",
      "title": "Measuring Mechanistic Independence: Can Bias Be Removed Without Erasing Demographics?",
      "authors": "Zhengyang Shan, Aaron Mueller",
      "date": "2025-12-23",
      "url": "https://arxiv.org/abs/2512.20796v1",
      "abstract": "We investigate how independent demographic bias mechanisms are from general demographic recognition in language models. Using a multi-task evaluation setup where demographics are associated with names, professions, and education levels, we measure whether models can be debiased while preserving demographic detection capabilities. We compare attribution-based and correlation-based methods for locating bias features. We find that targeted sparse autoencoder feature ablations in Gemma-2-9B reduce b",
      "tags": [
        "SAE",
        "features"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2512-22227v1",
      "title": "Hierarchical Geometry of Cognitive States in Transformer Embedding Spaces",
      "authors": "Sophie Zhao",
      "date": "2025-12-23",
      "url": "https://arxiv.org/abs/2512.22227v1",
      "abstract": "Recent work has shown that transformer-based language models learn rich geometric structure in their embedding spaces, yet the presence of higher-level cognitive organization within these representations remains underexplored. In this work, we investigate whether sentence embeddings encode a graded, hierarchical structure aligned with human-interpretable cognitive or psychological attributes. We construct a dataset of 480 natural-language sentences annotated with continuous ordinal energy scores",
      "tags": [],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2512-20328v1",
      "title": "Toward Explaining Large Language Models in Software Engineering Tasks",
      "authors": "Antonio Vitale, Khai-Nguyen Nguyen, Denys Poshyvanyk, Rocco Oliveto, Simone Scalabrino, et al.",
      "date": "2025-12-23",
      "url": "https://arxiv.org/abs/2512.20328v1",
      "abstract": "Recent progress in Large Language Models (LLMs) has substantially advanced the automation of software engineering (SE) tasks, enabling complex activities such as code generation and code summarization. However, the black-box nature of LLMs remains a major barrier to their adoption in high-stakes and safety-critical domains, where explainability and transparency are vital for trust, accountability, and effective human supervision. Despite increasing interest in explainable AI for software enginee",
      "tags": [
        "safety",
        "vision"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "ss-ca269e74084894dd4cc6",
      "title": "Cracking the Circuits: Mechanistic Interpretability in Large Language Models",
      "authors": "Dost Muhammad, Muhammad Salman, Mushtaq Ali, Malika Bendechache",
      "date": "2025-12-23",
      "url": "https://www.semanticscholar.org/paper/ca269e74084894dd4cc60134b50cebf52647849f",
      "abstract": "Mechanistic interpretability aims to uncover how internal components of large language models (LLMs) contribute to their overall behaviour. While previous research has revealed interpretable circuits in small-scale transformers, the field still lacks a systematic and mathematically grounded framework for understanding, validating, and comparing interpretability techniques. In this paper, we introduce a unified formalism and taxonomy for mechanistic interpretability, defining core concepts such a",
      "tags": [
        "circuits"
      ],
      "source": "Semantic Scholar",
      "featured": false
    },
    {
      "id": "arxiv-2512-19115v1",
      "title": "Generative Giants, Retrieval Weaklings: Why do Multimodal Large Language Models Fail at Multimodal Retrieval?",
      "authors": "Hengyi Feng, Zeang Sheng, Meiyi Qiang, Wentao Zhang",
      "date": "2025-12-22",
      "url": "https://arxiv.org/abs/2512.19115v1",
      "abstract": "Despite the remarkable success of multimodal large language models (MLLMs) in generative tasks, we observe that they exhibit a counterintuitive deficiency in the zero-shot multimodal retrieval task. In this work, we investigate the underlying mechanisms that hinder MLLMs from serving as effective retrievers. With the help of sparse autoencoders (SAEs), we decompose MLLM output representations into interpretable semantic concepts to probe their intrinsic behavior. Our analysis reveals that the re",
      "tags": [
        "SAE",
        "probing",
        "vision"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2512-18930v1",
      "title": "LouvreSAE: Sparse Autoencoders for Interpretable and Controllable Style Transfer",
      "authors": "Raina Panda, Daniel Fein, Arpita Singhal, Mark Fiore, Maneesh Agrawala, et al.",
      "date": "2025-12-22",
      "url": "https://arxiv.org/abs/2512.18930v1",
      "abstract": "Artistic style transfer in generative models remains a significant challenge, as existing methods often introduce style only via model fine-tuning, additional adapters, or prompt engineering, all of which can be computationally expensive and may still entangle style with subject matter. In this paper, we introduce a training- and inference-light, interpretable method for representing and transferring artistic style. Our approach leverages an art-specific Sparse Autoencoder (SAE) on top of latent",
      "tags": [
        "SAE"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2512-19125v1",
      "title": "SAP: Syntactic Attention Pruning for Transformer-based Language Models",
      "authors": "Tzu-Yun Lee, Ding-Yong Hong, Jan-Jan Wu",
      "date": "2025-12-22",
      "url": "https://arxiv.org/abs/2512.19125v1",
      "abstract": "This paper introduces Syntactic Attention Pruning (SAP), a novel method for effectively pruning attention heads in Transformer models. Unlike conventional approaches that rely solely on mathematical analysis of model weights and activations, SAP incorporates both the syntactic structure and attention patterns of sentences to guide the pruning process. By leveraging these linguistic features, SAP not only achieves performance comparable to state-of-the-art methods but also enhances the interpreta",
      "tags": [
        "features",
        "attention"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2512-18092v1",
      "title": "Faithful and Stable Neuron Explanations for Trustworthy Mechanistic Interpretability",
      "authors": "Ge Yan, Tuomas Oikarinen,  Tsui-Wei,  Weng",
      "date": "2025-12-19",
      "url": "https://arxiv.org/abs/2512.18092v1",
      "abstract": "Neuron identification is a popular tool in mechanistic interpretability, aiming to uncover the human-interpretable concepts represented by individual neurons in deep networks. While algorithms such as Network Dissection and CLIP-Dissect achieve great empirical success, a rigorous theoretical foundation remains absent, which is crucial to enable trustworthy and reliable explanations. In this work, we observe that neuron identification can be viewed as the inverse process of machine learning, whic",
      "tags": [
        "theory"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2512-17325v1",
      "title": "Task Schema and Binding: A Double Dissociation Study of In-Context Learning",
      "authors": "Chaeha Kim",
      "date": "2025-12-19",
      "url": "https://arxiv.org/abs/2512.17325v1",
      "abstract": "We provide causal mechanistic validation that in-context learning (ICL) decomposes into two separable mechanisms: Task Schema (abstract task type recognition) and Binding (specific input-output associations). Through activation patching experiments across 9 models from 7 Transformer families plus Mamba (370M-13B parameters), we establish three key findings:   1. Double dissociation: Task Schema transfers at 100% via late MLP patching; Binding transfers at 62% via residual stream patching -- prov",
      "tags": [],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2512-15938v1",
      "title": "SALVE: Sparse Autoencoder-Latent Vector Editing for Mechanistic Control of Neural Networks",
      "authors": "Vegard Flovik",
      "date": "2025-12-17",
      "url": "https://arxiv.org/abs/2512.15938v1",
      "abstract": "Deep neural networks achieve impressive performance but remain difficult to interpret and control. We present SALVE (Sparse Autoencoder-Latent Vector Editing), a unified \"discover, validate, and control\" framework that bridges mechanistic interpretability and model editing. Using an $\\ell_1$-regularized autoencoder, we learn a sparse, model-native feature basis without supervision. We validate these features with Grad-FAM, a feature-level saliency mapping method that visually grounds latent feat",
      "tags": [
        "SAE",
        "features",
        "editing",
        "vision"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2512-19734v1",
      "title": "The Deleuzian Representation Hypothesis",
      "authors": "Cl\u00e9ment Cornet, Romaric Besan\u00e7on, Herv\u00e9 Le Borgne",
      "date": "2025-12-17",
      "url": "https://arxiv.org/abs/2512.19734v1",
      "abstract": "We propose an alternative to sparse autoencoders (SAEs) as a simple and effective unsupervised method for extracting interpretable concepts from neural networks. The core idea is to cluster differences in activations, which we formally justify within a discriminant analysis framework. To enhance the diversity of extracted concepts, we refine the approach by weighting the clustering using the skewness of activations. The method aligns with Deleuze's modern view of concepts as differences. We eval",
      "tags": [
        "SAE"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2512-15134v1",
      "title": "From Isolation to Entanglement: When Do Interpretability Methods Identify and Disentangle Known Concepts?",
      "authors": "Aaron Mueller, Andrew Lee, Shruti Joshi, Ekdeep Singh Lubana, Dhanya Sridhar, et al.",
      "date": "2025-12-17",
      "url": "https://arxiv.org/abs/2512.15134v1",
      "abstract": "A central goal of interpretability is to recover representations of causally relevant concepts from the activations of neural networks. The quality of these concept representations is typically evaluated in isolation, and under implicit independence assumptions that may not hold in practice. Thus, it is unclear whether common featurization methods - including sparse autoencoders (SAEs) and sparse probes - recover disentangled representations of these concepts. This study proposes a multi-concept",
      "tags": [
        "SAE",
        "probing"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2512-14880v1",
      "title": "Task Matrices: Linear Maps for Cross-Model Finetuning Transfer",
      "authors": "Darrin O' Brien, Dhikshith Gajulapalli, Eric Xia",
      "date": "2025-12-16",
      "url": "https://arxiv.org/abs/2512.14880v1",
      "abstract": "Results in interpretability suggest that large vision and language models learn implicit linear encodings when models are biased by in-context prompting. However, the existence of similar linear representations in more general adaptation regimes has not yet been demonstrated. In this work, we develop the concept of a task matrix, a linear transformation from a base to finetuned embedding state. We demonstrate that for vision and text models and ten different datasets, a base model augmented with",
      "tags": [
        "vision"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2512-13979v1",
      "title": "ReflCtrl: Controlling LLM Reflection via Representation Engineering",
      "authors": "Ge Yan, Chung-En Sun,  Tsui-Wei,  Weng",
      "date": "2025-12-16",
      "url": "https://arxiv.org/abs/2512.13979v1",
      "abstract": "Large language models (LLMs) with Chain-of-Thought (CoT) reasoning have achieved strong performance across diverse tasks, including mathematics, coding, and general reasoning. A distinctive ability of these reasoning models is self-reflection: the ability to review and revise previous reasoning steps. While self-reflection enhances reasoning performance, it also increases inference cost. In this work, we study self-reflection through the lens of representation engineering. We segment the model's",
      "tags": [
        "survey",
        "reasoning"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2601-08837v1",
      "title": "From Adversarial Poetry to Adversarial Tales: An Interpretability Research Agenda",
      "authors": "Piercosma Bisconti, Marcello Galisai, Matteo Prandi, Federico Pierucci, Olga Sorokoletova, et al.",
      "date": "2025-12-16",
      "url": "https://arxiv.org/abs/2601.08837v1",
      "abstract": "Safety mechanisms in LLMs remain vulnerable to attacks that reframe harmful requests through culturally coded structures. We introduce Adversarial Tales, a jailbreak technique that embeds harmful content within cyberpunk narratives and prompts models to perform functional analysis inspired by Vladimir Propp's morphology of folktales. By casting the task as structural decomposition, the attack induces models to reconstruct harmful procedures as legitimate narrative interpretation. Across 26 front",
      "tags": [
        "safety"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2512-15783v1",
      "title": "AI Epidemiology: achieving explainable AI through expert oversight patterns",
      "authors": "Kit Tempest-Walters",
      "date": "2025-12-15",
      "url": "https://arxiv.org/abs/2512.15783v1",
      "abstract": "AI Epidemiology is a framework for governing and explaining advanced AI systems by applying population-level surveillance methods to AI outputs. The approach mirrors the way in which epidemiologists enable public health interventions through statistical evidence before molecular mechanisms are understood. This bypasses the problem of model complexity which plagues current interpretability methods (such as SHAP and mechanistic interpretability) at the scale of deployed models.   AI Epidemiology a",
      "tags": [],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2512-13568v1",
      "title": "Superposition as Lossy Compression: Measure with Sparse Autoencoders and Connect to Adversarial Vulnerability",
      "authors": "Leonard Bereska, Zoe Tzifa-Kratira, Reza Samavi, Efstratios Gavves",
      "date": "2025-12-15",
      "url": "https://arxiv.org/abs/2512.13568v1",
      "abstract": "Neural networks achieve remarkable performance through superposition: encoding multiple features as overlapping directions in activation space rather than dedicating individual neurons to each feature. This challenges interpretability, yet we lack principled methods to measure superposition. We present an information-theoretic framework measuring a neural representation's effective degrees of freedom. We apply Shannon entropy to sparse autoencoder activations to compute the number of effective f",
      "tags": [
        "SAE",
        "superposition",
        "features"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2512-13442v1",
      "title": "XNNTab -- Interpretable Neural Networks for Tabular Data using Sparse Autoencoders",
      "authors": "Khawla Elhadri, J\u00f6rg Schl\u00f6tterer, Christin Seifert",
      "date": "2025-12-15",
      "url": "https://arxiv.org/abs/2512.13442v1",
      "abstract": "In data-driven applications relying on tabular data, where interpretability is key, machine learning models such as decision trees and linear regression are applied. Although neural networks can provide higher predictive performance, they are not used because of their blackbox nature. In this work, we present XNNTab, a neural architecture that combines the expressiveness of neural networks and interpretability. XNNTab first learns highly non-linear feature representations, which are decomposed i",
      "tags": [
        "SAE",
        "features"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2512-10547v1",
      "title": "Unlocking the Address Book: Dissecting the Sparse Semantic Structure of LLM Key-Value Caches via Sparse Autoencoders",
      "authors": "Qingsen Ma, Dianyun Wang, Jiaming Lyu, Yaoye Wang, Lechen Ning, et al.",
      "date": "2025-12-11",
      "url": "https://arxiv.org/abs/2512.10547v1",
      "abstract": "The Key-Value (KV) cache is the primary memory bottleneck in long-context Large Language Models, yet it is typically treated as an opaque numerical tensor. In this work, we propose \\textbf{STA-Attention}, a framework that utilizes Top-K Sparse Autoencoders (SAEs) to decompose the KV cache into interpretable ``semantic atoms.'' Unlike standard $L_1$-regularized SAEs, our Top-K approach eliminates shrinkage bias, preserving the precise dot-product geometry required for attention. Our analysis unco",
      "tags": [
        "SAE"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2512-10720v1",
      "title": "Beyond the Black Box: Identifiable Interpretation and Control in Generative Models via Causal Minimality",
      "authors": "Lingjing Kong, Shaoan Xie, Guangyi Chen, Yuewen Sun, Xiangchen Song, et al.",
      "date": "2025-12-11",
      "url": "https://arxiv.org/abs/2512.10720v1",
      "abstract": "Deep generative models, while revolutionizing fields like image and text generation, largely operate as opaque black boxes, hindering human understanding, control, and alignment. While methods like sparse autoencoders (SAEs) show remarkable empirical success, they often lack theoretical guarantees, risking subjective insights. Our primary objective is to establish a principled foundation for interpretable generative models. We demonstrate that the principle of causal minimality -- favoring the s",
      "tags": [
        "SAE",
        "safety",
        "theory",
        "vision"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2512-10300v1",
      "title": "Investigating The Functional Roles of Attention Heads in Vision Language Models: Evidence for Reasoning Modules",
      "authors": "Yanbei Jiang, Xueqi Ma, Shu Liu, Sarah Monazam Erfani, Tongliang Liu, et al.",
      "date": "2025-12-11",
      "url": "https://arxiv.org/abs/2512.10300v1",
      "abstract": "Despite excelling on multimodal benchmarks, vision-language models (VLMs) largely remain a black box. In this paper, we propose a novel interpretability framework to systematically analyze the internal mechanisms of VLMs, focusing on the functional roles of attention heads in multimodal reasoning. To this end, we introduce CogVision, a dataset that decomposes complex multimodal questions into step-by-step subquestions designed to simulate human reasoning through a chain-of-thought paradigm, with",
      "tags": [
        "attention",
        "vision",
        "reasoning"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "sae-embeddings-toolkit",
      "title": "Interpretable Embeddings with Sparse Autoencoders: A Data Analysis Toolkit",
      "authors": "Various",
      "date": "2025-12-10",
      "url": "https://arxiv.org/abs/2512.10092",
      "abstract": "Proposes using SAEs to create embeddings whose dimensions map to interpretable concepts. Shows SAE embeddings are more cost-effective and reliable than LLMs for data analysis.",
      "tags": [
        "SAE",
        "embeddings",
        "tools"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2512-10098v1",
      "title": "MedXAI: A Retrieval-Augmented and Self-Verifying Framework for Knowledge-Guided Medical Image Analysis",
      "authors": "Midhat Urooj, Ayan Banerjee, Farhat Shaikh, Kuntal Thakur, Sandeep Gupta",
      "date": "2025-12-10",
      "url": "https://arxiv.org/abs/2512.10098v1",
      "abstract": "Accurate and interpretable image-based diagnosis remains a fundamental challenge in medical AI, particularly under domain shifts and rare-class conditions. Deep learning models often struggle with real-world distribution changes, exhibit bias against infrequent pathologies, and lack the transparency required for deployment in safety-critical clinical environments. We introduce MedXAI (An Explainable Framework for Medical Imaging Classification), a unified expert knowledge based framework that in",
      "tags": [
        "safety",
        "vision"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2512-09148v1",
      "title": "Detecting Hallucinations in Graph Retrieval-Augmented Generation via Attention Patterns and Semantic Alignment",
      "authors": "Shanghao Li, Jinda Han, Yibo Wang, Yuanjie Zhu, Zihe Song, et al.",
      "date": "2025-12-09",
      "url": "https://arxiv.org/abs/2512.09148v1",
      "abstract": "Graph-based Retrieval-Augmented Generation (GraphRAG) enhances Large Language Models (LLMs) by incorporating external knowledge from linearized subgraphs retrieved from knowledge graphs. However, LLMs struggle to interpret the relational and topological information in these inputs, resulting in hallucinations that are inconsistent with the retrieved knowledge. To analyze how LLMs attend to and retain structured knowledge during generation, we propose two lightweight interpretability metrics: Pat",
      "tags": [
        "safety",
        "attention"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2512-09142v2",
      "title": "SDialog: A Python Toolkit for End-to-End Agent Building, User Simulation, Dialog Generation, and Evaluation",
      "authors": "Sergio Burdisso, S\u00e9verin Baroudi, Yanis Labrak, David Grunert, Pawel Cyrta, et al.",
      "date": "2025-12-09",
      "url": "https://arxiv.org/abs/2512.09142v2",
      "abstract": "We present SDialog, an MIT-licensed open-source Python toolkit that unifies dialog generation, evaluation and mechanistic interpretability into a single end-to-end framework for building and analyzing LLM-based conversational agents. Built around a standardized \\texttt{Dialog} representation, SDialog provides: (1) persona-driven multi-agent simulation with composable orchestration for controlled, synthetic dialog generation, (2) comprehensive evaluation combining linguistic metrics, LLM-as-a-jud",
      "tags": [],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2512-08892v1",
      "title": "Toward Faithful Retrieval-Augmented Generation with Sparse Autoencoders",
      "authors": "Guangzhi Xiong, Zhenghao He, Bohan Liu, Sanchit Sinha, Aidong Zhang",
      "date": "2025-12-09",
      "url": "https://arxiv.org/abs/2512.08892v1",
      "abstract": "Retrieval-Augmented Generation (RAG) improves the factuality of large language models (LLMs) by grounding outputs in retrieved evidence, but faithfulness failures, where generations contradict or extend beyond the provided sources, remain a critical challenge. Existing hallucination detection methods for RAG often rely either on large-scale detector training, which requires substantial annotated data, or on querying external LLM judges, which leads to high inference costs. Although some approach",
      "tags": [
        "SAE"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "save-hallucination",
      "title": "SAVE: Sparse Autoencoder-Driven Visual Information Enhancement for Mitigating Object Hallucination",
      "authors": "Various",
      "date": "2025-12-08",
      "url": "https://arxiv.org/abs/2512.07730",
      "abstract": "Uses SAE-driven visual information enhancement to mitigate object hallucination in vision-language models, achieving 10% improvement in CHAIR_S benchmarks.",
      "tags": [
        "SAE",
        "vision",
        "hallucination",
        "safety"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2512-08077v1",
      "title": "Unveiling Latent Knowledge in Chemistry Language Models through Sparse Autoencoders",
      "authors": "Jaron Cohen, Alexander G. Hasson, Sara Tanovic",
      "date": "2025-12-08",
      "url": "https://arxiv.org/abs/2512.08077v1",
      "abstract": "Since the advent of machine learning, interpretability has remained a persistent challenge, becoming increasingly urgent as generative models support high-stakes applications in drug and material discovery. Recent advances in large language model (LLM) architectures have yielded chemistry language models (CLMs) with impressive capabilities in molecular property prediction and molecular generation. However, how these models internally represent chemical knowledge remains poorly understood. In thi",
      "tags": [
        "SAE"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2512-07245v1",
      "title": "Zero-Shot Textual Explanations via Translating Decision-Critical Features",
      "authors": "Toshinori Yamauchi, Hiroshi Kera, Kazuhiko Kawamoto",
      "date": "2025-12-08",
      "url": "https://arxiv.org/abs/2512.07245v1",
      "abstract": "Textual explanations make image classifier decisions transparent by describing the prediction rationale in natural language. Large vision-language models can generate captions but are designed for general visual understanding, not classifier-specific reasoning. Existing zero-shot explanation methods align global image features with language, producing descriptions of what is visible rather than what drives the prediction. We propose TEXTER, which overcomes this limitation by isolating decision-c",
      "tags": [
        "features",
        "probing",
        "vision",
        "reasoning"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2512-07355v1",
      "title": "A Geometric Unification of Concept Learning with Concept Cones",
      "authors": "Alexandre Rocchi--Henry, Thomas Fel, Gianni Franchi",
      "date": "2025-12-08",
      "url": "https://arxiv.org/abs/2512.07355v1",
      "abstract": "Two traditions of interpretability have evolved side by side but seldom spoken to each other: Concept Bottleneck Models (CBMs), which prescribe what a concept should be, and Sparse Autoencoders (SAEs), which discover what concepts emerge. While CBMs use supervision to align activations with human-labeled concepts, SAEs rely on sparse coding to uncover emergent ones. We show that both paradigms instantiate the same geometric structure: each learns a set of linear directions in activation space wh",
      "tags": [
        "SAE",
        "vision"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2512-07462v2",
      "title": "Understanding LLM Agent Behaviours via Game Theory: Strategy Recognition, Biases and Multi-Agent Dynamics",
      "authors": "Trung-Kiet Huynh, Duy-Minh Dao-Sy, Thanh-Bang Cao, Phong-Hao Le, Hong-Dan Nguyen, et al.",
      "date": "2025-12-08",
      "url": "https://arxiv.org/abs/2512.07462v2",
      "abstract": "As Large Language Models (LLMs) increasingly operate as autonomous decision-makers in interactive and multi-agent systems and human societies, understanding their strategic behaviour has profound implications for safety, coordination, and the design of AI-driven social and economic infrastructures. Assessing such behaviour requires methods that capture not only what LLMs output, but the underlying intentions that guide their decisions. In this work, we extend the FAIRGAME framework to systematic",
      "tags": [
        "safety",
        "theory"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2512-07141v1",
      "title": "Think-Reflect-Revise: A Policy-Guided Reflective Framework for Safety Alignment in Large Vision Language Models",
      "authors": "Fenghua Weng, Chaochao Lu, Xia Hu, Wenqi Shao, Wenjie Wang",
      "date": "2025-12-08",
      "url": "https://arxiv.org/abs/2512.07141v1",
      "abstract": "As multimodal reasoning improves the overall capabilities of Large Vision Language Models (LVLMs), recent studies have begun to explore safety-oriented reasoning, aiming to enhance safety awareness by analyzing potential safety risks during the reasoning process before generating the final response. Although such approaches improve safety awareness and interpretability, this single-pass think-then-answer paradigm remains vulnerable to contextual or visual jailbreak attacks. This reveals a critic",
      "tags": [
        "safety",
        "vision",
        "reasoning"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "gpt2-sentiment-mi",
      "title": "Mechanistic Interpretability of GPT-2: Lexical and Contextual Layers in Sentiment Analysis",
      "authors": "Amartya Hatua",
      "date": "2025-12-07",
      "url": "https://arxiv.org/abs/2512.06681",
      "abstract": "Using systematic activation patching across all 12 layers, confirms early layers (0-3) act as lexical sentiment detectors with position-specific polarity signals.",
      "tags": [
        "circuits",
        "sentiment",
        "GPT-2"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2512-06655v1",
      "title": "GSAE: Graph-Regularized Sparse Autoencoders for Robust LLM Safety Steering",
      "authors": "Jehyeok Yeon, Federico Cinus, Yifan Wu, Luca Luceri",
      "date": "2025-12-07",
      "url": "https://arxiv.org/abs/2512.06655v1",
      "abstract": "Large language models (LLMs) face critical safety challenges, as they can be manipulated to generate harmful content through adversarial prompts and jailbreak attacks. Many defenses are typically either black-box guardrails that filter outputs, or internals-based methods that steer hidden activations by operationalizing safety as a single latent feature or dimension. While effective for simple concepts, this assumption is limiting, as recent evidence shows that abstract concepts such as refusal ",
      "tags": [
        "SAE",
        "features",
        "safety",
        "steering"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2512-20638v1",
      "title": "Uncovering Competency Gaps in Large Language Models and Their Benchmarks",
      "authors": "Matyas Bohacek, Nino Scherrer, Nicholas Dufour, Thomas Leung, Christoph Bregler, et al.",
      "date": "2025-12-06",
      "url": "https://arxiv.org/abs/2512.20638v1",
      "abstract": "The evaluation of large language models (LLMs) relies heavily on standardized benchmarks. These benchmarks provide useful aggregated metrics for a given capability, but those aggregated metrics can obscure (i) particular sub-areas where the LLMs are weak (\"model gaps\") and (ii) imbalanced coverage in the benchmarks themselves (\"benchmark gaps\"). We propose a new method that uses sparse autoencoders (SAEs) to automatically uncover both types of gaps. By extracting SAE concept activations and comp",
      "tags": [
        "SAE"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "sparse-dictionary-theory",
      "title": "On the Theoretical Foundation of Sparse Dictionary Learning in Mechanistic Interpretability",
      "authors": "Yiming Tang et al.",
      "date": "2025-12-05",
      "url": "https://arxiv.org/abs/2512.05534",
      "abstract": "First closed-form theoretical analysis of SAEs, transcoders, and crosscoders. Reveals SAEs may fail to fully recover ground truth features unless extremely sparse.",
      "tags": [
        "SAE",
        "theory",
        "transcoders"
      ],
      "source": "arXiv",
      "featured": true
    },
    {
      "id": "sparse-attention-post-training",
      "title": "Sparse Attention Post-Training for Mechanistic Interpretability",
      "authors": "Florent Draye et al.",
      "date": "2025-12-05",
      "url": "https://arxiv.org/abs/2512.05865",
      "abstract": "Post-training method making transformer attention sparse without sacrificing performance. Reduces attention connectivity to ~0.3% of edges on models up to 1B parameters.",
      "tags": [
        "attention",
        "sparsity",
        "methods"
      ],
      "source": "arXiv",
      "featured": true
    },
    {
      "id": "arxiv-2512-05371v1",
      "title": "ChipMind: Retrieval-Augmented Reasoning for Long-Context Circuit Design Specifications",
      "authors": "Changwen Xing, SamZaak Wong, Xinlai Wan, Yanfeng Lu, Mengli Zhang, et al.",
      "date": "2025-12-05",
      "url": "https://arxiv.org/abs/2512.05371v1",
      "abstract": "While Large Language Models (LLMs) demonstrate immense potential for automating integrated circuit (IC) development, their practical deployment is fundamentally limited by restricted context windows. Existing context-extension methods struggle to achieve effective semantic modeling and thorough multi-hop reasoning over extensive, intricate circuit specifications. To address this, we introduce ChipMind, a novel knowledge graph-augmented reasoning framework specifically designed for lengthy IC spe",
      "tags": [
        "circuits",
        "reasoning"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2512-05534v3",
      "title": "A Unified Theory of Sparse Dictionary Learning in Mechanistic Interpretability: Piecewise Biconvexity and Spurious Minima",
      "authors": "Yiming Tang, Harshvardhan Saini, Zhaoqian Yao, Zheng Lin, Yizhen Liao, et al.",
      "date": "2025-12-05",
      "url": "https://arxiv.org/abs/2512.05534v3",
      "abstract": "As AI models achieve remarkable capabilities across diverse domains, understanding what representations they learn and how they encode concepts has become increasingly important for both scientific progress and trustworthy deployment. Recent works in mechanistic interpretability have widely reported that neural networks represent meaningful concepts as linear directions in their representation spaces and often encode diverse concepts in superposition. Various sparse dictionary learning (SDL) met",
      "tags": [
        "superposition",
        "theory"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "ss-963b7ebba47eb530160f",
      "title": "Frequency-Dependent Parameter-Incorporated Equivalent Circuit Modeling and Fault Simulation for Transformer Windings Based on Frequency Response Analysis",
      "authors": "Xiaoxiao Luo, Haishan Zheng, Xiong Liu, Siquan Li",
      "date": "2025-12-05",
      "url": "https://www.semanticscholar.org/paper/963b7ebba47eb530160fad4881afcc1153e456a8",
      "abstract": "",
      "tags": [
        "circuits"
      ],
      "source": "Semantic Scholar",
      "featured": false
    },
    {
      "id": "ethical-multiagent-mi",
      "title": "Towards Ethical Multi-Agent Systems of Large Language Models: A Mechanistic Interpretability Perspective",
      "authors": "Various",
      "date": "2025-12-04",
      "url": "https://arxiv.org/abs/2512.04691",
      "abstract": "Research agenda for ensuring ethical behavior of multi-agent LLM systems using mechanistic interpretability, identifying key research challenges.",
      "tags": [
        "safety",
        "multi-agent",
        "ethics"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2512-04441v2",
      "title": "MindDrive: An All-in-One Framework Bridging World Models and Vision-Language Model for End-to-End Autonomous Driving",
      "authors": "Bin Sun, Yaoguang Cao, Yan Wang, Rui Wang, Jiachen Shang, et al.",
      "date": "2025-12-04",
      "url": "https://arxiv.org/abs/2512.04441v2",
      "abstract": "End-to-End autonomous driving (E2E-AD) has emerged as a new paradigm, where trajectory planning plays a crucial role. Existing studies mainly follow two directions: trajectory generation oriented, which focuses on producing high-quality trajectories with simple decision mechanisms, and trajectory selection oriented, which performs multi-dimensional evaluation to select the best trajectory yet lacks sufficient generative capability. In this work, we propose MindDrive, a harmonized framework that ",
      "tags": [
        "vision"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2512-03994v2",
      "title": "Training-Free Policy Violation Detection via Activation-Space Whitening in LLMs",
      "authors": "Oren Rachmil, Roy Betser, Itay Gershon, Omer Hofman, Nitay Yakoby, et al.",
      "date": "2025-12-03",
      "url": "https://arxiv.org/abs/2512.03994v2",
      "abstract": "Aligning proprietary large language models (LLMs) with internal organizational policies has become an urgent priority as organizations increasingly deploy LLMs in sensitive domains such as legal support, finance, and medical services. Beyond generic safety filters, enterprises require reliable mechanisms to detect policy violations within their regulatory and operational frameworks, where breaches can trigger legal and reputational risks. Existing content moderation frameworks, such as guardrail",
      "tags": [
        "safety"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2512-04165v3",
      "title": "Mitigating the Curse of Detail: Scaling Arguments for Feature Learning and Sample Complexity",
      "authors": "Noa Rubin, Orit Davidovich, Zohar Ringel",
      "date": "2025-12-03",
      "url": "https://arxiv.org/abs/2512.04165v3",
      "abstract": "Two pressing topics in the theory of deep learning are the interpretation of feature learning mechanisms and the determination of implicit bias of networks in the rich regime. Current theories of rich feature learning, often appear in the form of high-dimensional non-linear equations, which require computationally intensive numerical solutions. Given the many details that go into defining a deep learning problem, this complexity is a significant and often unavoidable challenge. Here, we propose ",
      "tags": [
        "features",
        "theory"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2512-10978v1",
      "title": "Cognitive Mirrors: Exploring the Diverse Functional Roles of Attention Heads in LLM Reasoning",
      "authors": "Xueqi Ma, Jun Wang, Yanbei Jiang, Sarah Monazam Erfani, Tongliang Liu, et al.",
      "date": "2025-12-03",
      "url": "https://arxiv.org/abs/2512.10978v1",
      "abstract": "Large language models (LLMs) have achieved state-of-the-art performance in a variety of tasks, but remain largely opaque in terms of their internal mechanisms. Understanding these mechanisms is crucial to improve their reasoning abilities. Drawing inspiration from the interplay between neural processes and human cognition, we propose a novel interpretability framework to systematically analyze the roles and behaviors of attention heads, which are key components of LLMs. We introduce CogQA, a dat",
      "tags": [
        "attention",
        "reasoning"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2512-03276v1",
      "title": "Too Late to Recall: Explaining the Two-Hop Problem in Multimodal Knowledge Retrieval",
      "authors": "Constantin Venhoff, Ashkan Khakzar, Sonia Joseph, Philip Torr, Neel Nanda",
      "date": "2025-12-02",
      "url": "https://arxiv.org/abs/2512.03276v1",
      "abstract": "Training vision language models (VLMs) aims to align visual representations from a vision encoder with the textual representations of a pretrained large language model (LLM). However, many VLMs exhibit reduced factual recall performance compared to their LLM backbones, raising the question of how effective multimodal fine-tuning is at extending existing mechanisms within the LLM to visual inputs. We argue that factual recall based on visual inputs requires VLMs to solve a two-hop problem: (1) fo",
      "tags": [
        "vision"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "unsupervised-decoding-reasoning",
      "title": "Unsupervised Decoding of Encoded Reasoning Using Language Model Interpretability",
      "authors": "Ching Fang et al.",
      "date": "2025-12-01",
      "url": "https://arxiv.org/abs/2512.01222",
      "abstract": "Evaluates logit lens and other MI methods on decoding hidden reasoning in models fine-tuned to reason in ROT-13 encryption while outputting English.",
      "tags": [
        "reasoning",
        "probing",
        "methods"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2512-02004v1",
      "title": "AlignSAE: Concept-Aligned Sparse Autoencoders",
      "authors": "Minglai Yang, Xinyu Guo, Mihai Surdeanu, Liangming Pan",
      "date": "2025-12-01",
      "url": "https://arxiv.org/abs/2512.02004v1",
      "abstract": "Large Language Models (LLMs) encode factual knowledge within hidden parametric spaces that are difficult to inspect or control. While Sparse Autoencoders (SAEs) can decompose hidden activations into more fine-grained, interpretable features, they often struggle to reliably align these features with human-defined concepts, resulting in entangled and distributed feature representations. To address this, we introduce AlignSAE, a method that aligns SAE features with a defined ontology through a \"pre",
      "tags": [
        "SAE",
        "features"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2512-02194v1",
      "title": "Enforcing Orderedness to Improve Feature Consistency",
      "authors": "Sophie L. Wang, Alex Quach, Nithin Parsan, John J. Yang",
      "date": "2025-12-01",
      "url": "https://arxiv.org/abs/2512.02194v1",
      "abstract": "Sparse autoencoders (SAEs) have been widely used for interpretability of neural networks, but their learned features often vary across seeds and hyperparameter settings. We introduce Ordered Sparse Autoencoders (OSAE), which extend Matryoshka SAEs by (1) establishing a strict ordering of latent features and (2) deterministically using every feature dimension, avoiding the sampling-based approximations of prior nested SAE methods. Theoretically, we show that OSAEs resolve permutation non-identifi",
      "tags": [
        "SAE",
        "features",
        "theory"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2512-01282v2",
      "title": "Kardia-R1: Unleashing LLMs to Reason toward Understanding and Empathy for Emotional Support via Rubric-as-Judge Reinforcement Learning",
      "authors": "Jiahao Yuan, Zhiqing Cui, Hanqing Wang, Yuansheng Gao, Yucheng Zhou, et al.",
      "date": "2025-12-01",
      "url": "https://arxiv.org/abs/2512.01282v2",
      "abstract": "As web platforms evolve towards greater personalization and emotional complexity, conversational agents must transcend superficial empathy to demonstrate identity-aware emotional reasoning. However, existing systems face two limitations: (1) reliance on situation-centric datasets lacking persistent user identity, which hampers the capture of personalized affective nuances; and (2) dependence on opaque, coarse reward signals that hinder development of verifiable empathetic reasoning. To address t",
      "tags": [
        "reasoning"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2512-00686v3",
      "title": "Using physics-inspired Singular Learning Theory to understand grokking & other phase transitions in modern neural networks",
      "authors": "Anish Lakkapragada",
      "date": "2025-11-30",
      "url": "https://arxiv.org/abs/2512.00686v3",
      "abstract": "Classical statistical inference and learning theory often fail to explain the success of modern neural networks. A key reason is that these models are non-identifiable (singular), violating core assumptions behind PAC bounds and asymptotic normality. Singular learning theory (SLT), a physics-inspired framework grounded in algebraic geometry, has gained popularity for its ability to close this theory-practice gap. In this paper, we empirically study SLT in toy settings relevant to interpretabilit",
      "tags": [
        "theory"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2511-23231v1",
      "title": "Unlocking Multilingual Reasoning Capability of LLMs and LVLMs through Representation Engineering",
      "authors": "Qiming Li, Xiaocheng Feng, Yixuan Ma, Zekai Ye, Ruihan Chen, et al.",
      "date": "2025-11-28",
      "url": "https://arxiv.org/abs/2511.23231v1",
      "abstract": "Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) demonstrate strong reasoning capabilities, yet their performance in English significantly outperforms that in low-resource languages, raising fairness concerns in multilingual applications. Existing approaches either rely on costly multilingual training or employ prompting with external translation tools, both of which are resource-intensive and sensitive to translation quality. To address these limitations, we propose a train",
      "tags": [
        "vision",
        "reasoning"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2511-23375v1",
      "title": "Optimizing Multimodal Language Models through Attention-based Interpretability",
      "authors": "Alexander Sergeev, Evgeny Kotelnikov",
      "date": "2025-11-28",
      "url": "https://arxiv.org/abs/2511.23375v1",
      "abstract": "Modern large language models become multimodal, analyzing various data formats like text and images. While fine-tuning is effective for adapting these multimodal language models (MLMs) to downstream tasks, full fine-tuning is computationally expensive. Parameter-Efficient Fine-Tuning (PEFT) methods address this by training only a small portion of model weights. However, MLMs are difficult to interpret, making it challenging to identify which components are most effective for training to balance ",
      "tags": [
        "vision"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2511-22697v1",
      "title": "Mechanistic Finetuning of Vision-Language-Action Models via Few-Shot Demonstrations",
      "authors": "Chancharik Mitra, Yusen Luo, Raj Saravanan, Dantong Niu, Anirudh Pai, et al.",
      "date": "2025-11-27",
      "url": "https://arxiv.org/abs/2511.22697v1",
      "abstract": "Vision-Language Action (VLAs) models promise to extend the remarkable success of vision-language models (VLMs) to robotics. Yet, unlike VLMs in the vision-language domain, VLAs for robotics require finetuning to contend with varying physical factors like robot embodiment, environment characteristics, and spatial relationships of each task. Existing fine-tuning methods lack specificity, adapting the same set of parameters regardless of a task's visual, linguistic, and physical characteristics. In",
      "tags": [
        "vision"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2511-22519v1",
      "title": "FoldSAE: Learning to Steer Protein Folding Through Sparse Representations",
      "authors": "Wojciech Zarzecki, Paulina Szymczak, Ewa Szczurek, Kamil Deja",
      "date": "2025-11-27",
      "url": "https://arxiv.org/abs/2511.22519v1",
      "abstract": "RFdiffusion is a popular and well-established model for generation of protein structures. However, this generative process offers limited insight into its internal representations and how they contribute to the final protein structure. Concurrently, recent work in mechanistic interpretability has successfully used Sparse Autoencoders (SAEs) to discover interpretable features within neural networks. We combine these concepts by applying SAE to the internal representations of RFdiffusion to uncove",
      "tags": [
        "SAE",
        "features",
        "biology"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2511-21974v1",
      "title": "Start Making Sense(s): A Developmental Probe of Attention Specialization Using Lexical Ambiguity",
      "authors": "Pamela D. Rivi\u00e8re, Sean Trott",
      "date": "2025-11-26",
      "url": "https://arxiv.org/abs/2511.21974v1",
      "abstract": "Despite an in-principle understanding of self-attention matrix operations in Transformer language models (LMs), it remains unclear precisely how these operations map onto interpretable computations or functions--and how or when individual attention heads develop specialized attention patterns. Here, we present a pipeline to systematically probe attention mechanisms, and we illustrate its value by leveraging lexical ambiguity--where a single word has multiple meanings--to isolate attention mechan",
      "tags": [
        "probing",
        "attention"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2511-20798v2",
      "title": "Physics Steering: Causal Control of Cross-Domain Concepts in a Physics Foundation Model",
      "authors": "Rio Alexa Fear, Payel Mukhopadhyay, Michael McCabe, Alberto Bietti, Miles Cranmer",
      "date": "2025-11-25",
      "url": "https://arxiv.org/abs/2511.20798v2",
      "abstract": "Recent advances in mechanistic interpretability have revealed that large language models (LLMs) develop internal representations corresponding not only to concrete entities but also distinct, human-understandable abstract concepts and behaviour. Moreover, these hidden features can be directly manipulated to steer model behaviour. However, it remains an open question whether this phenomenon is unique to models trained on inherently structured data (ie. language, images) or if it is a general prop",
      "tags": [
        "features",
        "steering",
        "vision"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2511-20820v1",
      "title": "SAGE: An Agentic Explainer Framework for Interpreting SAE Features in Language Models",
      "authors": "Jiaojiao Han, Wujiang Xu, Mingyu Jin, Mengnan Du",
      "date": "2025-11-25",
      "url": "https://arxiv.org/abs/2511.20820v1",
      "abstract": "Large language models (LLMs) have achieved remarkable progress, yet their internal mechanisms remain largely opaque, posing a significant challenge to their safe and reliable deployment. Sparse autoencoders (SAEs) have emerged as a promising tool for decomposing LLM representations into more interpretable features, but explaining the features captured by SAEs remains a challenging task. In this work, we propose SAGE (SAE AGentic Explainer), an agent-based framework that recasts feature interpret",
      "tags": [
        "SAE",
        "features"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2511-19972v2",
      "title": "Boosting Reasoning in Large Multimodal Models via Activation Replay",
      "authors": "Yun Xing, Xiaobin Hu, Qingdong He, Jiangning Zhang, Shuicheng Yan, et al.",
      "date": "2025-11-25",
      "url": "https://arxiv.org/abs/2511.19972v2",
      "abstract": "Recently, Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as an effective approach to incentivizing reasoning capability in Large Multimodal Models (LMMs), while the underlying mechanisms behind this post-training paradigm are poorly understood. We begin by exploring how input activations are affected by RLVR through the perspective of logit lens. Our systematic investigations across multiple post-trained LMMs suggest that RLVR shifts low-entropy activations unexpectedly, while",
      "tags": [
        "vision",
        "reasoning"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2511-21756v1",
      "title": "Dissecting the Ledger: Locating and Suppressing \"Liar Circuits\" in Financial Large Language Models",
      "authors": "Soham Mirajkar",
      "date": "2025-11-24",
      "url": "https://arxiv.org/abs/2511.21756v1",
      "abstract": "Large Language Models (LLMs) are increasingly deployed in high-stakes financial domains, yet they suffer from specific, reproducible hallucinations when performing arithmetic operations. Current mitigation strategies often treat the model as a black box. In this work, we propose a mechanistic approach to intrinsic hallucination detection. By applying Causal Tracing to the GPT-2 XL architecture on the ConvFinQA benchmark, we identify a dual-stage mechanism for arithmetic reasoning: a distributed ",
      "tags": [
        "circuits",
        "reasoning"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2511-18024v1",
      "title": "Extracting Interaction-Aware Monosemantic Concepts in Recommender Systems",
      "authors": "Dor Arviv, Yehonatan Elisha, Oren Barkan, Noam Koenigstein",
      "date": "2025-11-22",
      "url": "https://arxiv.org/abs/2511.18024v1",
      "abstract": "We present a method for extracting \\emph{monosemantic} neurons, defined as latent dimensions that align with coherent and interpretable concepts, from user and item embeddings in recommender systems. Our approach employs a Sparse Autoencoder (SAE) to reveal semantic structure within pretrained representations. In contrast to work on language models, monosemanticity in recommendation must preserve the interactions between separate user and item embeddings. To achieve this, we introduce a \\emph{pr",
      "tags": [
        "SAE",
        "features"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2511-17735v1",
      "title": "Towards Open-Ended Visual Scientific Discovery with Sparse Autoencoders",
      "authors": "Samuel Stevens, Jacob Beattie, Tanya Berger-Wolf, Yu Su",
      "date": "2025-11-21",
      "url": "https://arxiv.org/abs/2511.17735v1",
      "abstract": "Scientific archives now contain hundreds of petabytes of data across genomics, ecology, climate, and molecular biology that could reveal undiscovered patterns if systematically analyzed at scale. Large-scale, weakly-supervised datasets in language and vision have driven the development of foundation models whose internal representations encode structure (patterns, co-occurrences and statistical regularities) beyond their training objectives. Most existing methods extract structure only for pre-s",
      "tags": [
        "SAE",
        "vision",
        "biology"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2511-17699v1",
      "title": "Understanding Counting Mechanisms in Large Language and Vision-Language Models",
      "authors": "Hosein Hasani, Amirmohammad Izadi, Fatemeh Askari, Mobin Bagherian, Sadegh Mohammadian, et al.",
      "date": "2025-11-21",
      "url": "https://arxiv.org/abs/2511.17699v1",
      "abstract": "This paper examines how large language models (LLMs) and large vision-language models (LVLMs) represent and compute numerical information in counting tasks. We use controlled experiments with repeated textual and visual items and analyze model behavior through causal mediation and activation patching. To this end, we design a specialized tool, CountScope, for mechanistic interpretability of numerical content. Results show that individual tokens or visual features encode latent positional count i",
      "tags": [
        "features",
        "vision"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2511-16309v1",
      "title": "Sparse Autoencoders are Topic Models",
      "authors": "Leander Girrbach, Zeynep Akata",
      "date": "2025-11-20",
      "url": "https://arxiv.org/abs/2511.16309v1",
      "abstract": "Sparse autoencoders (SAEs) are used to analyze embeddings, but their role and practical value are debated. We propose a new perspective on SAEs by demonstrating that they can be naturally understood as topic models. We extend Latent Dirichlet Allocation to embedding spaces and derive the SAE objective as a maximum a posteriori estimator under this model. This view implies SAE features are thematic components rather than steerable directions. Based on this, we introduce SAE-TM, a topic modeling f",
      "tags": [
        "SAE",
        "features"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2511-15210v1",
      "title": "Unveiling Intrinsic Dimension of Texts: from Academic Abstract to Creative Story",
      "authors": "Vladislav Pedashenko, Laida Kushnareva, Yana Khassan Nibal, Eduard Tulchinskii, Kristian Kuznetsov, et al.",
      "date": "2025-11-19",
      "url": "https://arxiv.org/abs/2511.15210v1",
      "abstract": "Intrinsic dimension (ID) is an important tool in modern LLM analysis, informing studies of training dynamics, scaling behavior, and dataset structure, yet its textual determinants remain underexplored. We provide the first comprehensive study grounding ID in interpretable text properties through cross-encoder analysis, linguistic features, and sparse autoencoders (SAEs). In this work, we establish three key findings. First, ID is complementary to entropy-based metrics: after controlling for leng",
      "tags": [
        "SAE",
        "features"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2511-15895v1",
      "title": "Decomposing Theory of Mind: How Emotional Processing Mediates ToM Abilities in LLMs",
      "authors": "Ivan Chulo, Ananya Joshi",
      "date": "2025-11-19",
      "url": "https://arxiv.org/abs/2511.15895v1",
      "abstract": "Recent work shows activation steering substantially improves language models' Theory of Mind (ToM) (Bortoletto et al. 2024), yet the mechanisms of what changes occur internally that leads to different outputs remains unclear. We propose decomposing ToM in LLMs by comparing steered versus baseline LLMs' activations using linear probes trained on 45 cognitive actions. We applied Contrastive Activation Addition (CAA) steering to Gemma-3-4B and evaluated it on 1,000 BigToM forward belief scenarios (",
      "tags": [
        "probing",
        "steering",
        "theory"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "ss-60bf86289fb968e8de2e",
      "title": "M-AIDE: Mechanistic Agentic Interpretability for Decoding Empathy in Language Models",
      "authors": "N. Mirnateghi, Sharjeel Tahir, Syed Mohammed Shamsul Islam, Syed Afaq Ali Shah",
      "date": "2025-11-19",
      "url": "https://www.semanticscholar.org/paper/60bf86289fb968e8de2efb1b4e452c191f7ca8a6",
      "abstract": "Large language models (LLMs) have transformed conversational agents, powering applications from everyday assistants to domain-specific systems. Yet, their internal mechanisms remain opaque, limiting our understanding of how complex behaviours are represented. Therapeutic conversational agents provide a compelling setting to study this problem, as they require models to encode empathic behaviours. For a better understanding of these behaviours, we present M-AIDE, an agentic framework designed to ",
      "tags": [],
      "source": "Semantic Scholar",
      "featured": false
    },
    {
      "id": "nnterp-interface",
      "title": "nnterp: A Standardized Interface for Mechanistic Interpretability of Transformers",
      "authors": "Various",
      "date": "2025-11-18",
      "url": "https://arxiv.org/abs/2511.14465",
      "abstract": "Lightweight wrapper around NNsight providing unified interface for transformer analysis while preserving HuggingFace implementations.",
      "tags": [
        "tools",
        "library",
        "methods"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2511-14685v1",
      "title": "Encoding and Understanding Astrophysical Information in Large Language Model-Generated Summaries",
      "authors": "Kiera McCormick, Rafael Mart\u00ednez-Galarza",
      "date": "2025-11-18",
      "url": "https://arxiv.org/abs/2511.14685v1",
      "abstract": "Large Language Models have demonstrated the ability to generalize well at many levels across domains, modalities, and even shown in-context learning capabilities. This enables research questions regarding how they can be used to encode physical information that is usually only available from scientific measurements, and loosely encoded in textual descriptions. Using astrophysics as a test bed, we investigate if LLM embeddings can codify physical summary statistics that are obtained from scientif",
      "tags": [],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "time-series-mi",
      "title": "Mechanistic Interpretability for Transformer-based Time Series Classification",
      "authors": "Various",
      "date": "2025-11-15",
      "url": "https://arxiv.org/abs/2511.21514",
      "abstract": "Extends mechanistic interpretability techniques to time series transformers, analyzing how they process temporal patterns.",
      "tags": [
        "time-series",
        "circuits",
        "methods"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2511-11356v1",
      "title": "SEAL: Subspace-Anchored Watermarks for LLM Ownership",
      "authors": "Yanbo Dai, Zongjie Li, Zhenlan Ji, Shuai Wang",
      "date": "2025-11-14",
      "url": "https://arxiv.org/abs/2511.11356v1",
      "abstract": "Large language models (LLMs) have achieved remarkable success across a wide range of natural language processing tasks, demonstrating human-level performance in text generation, reasoning, and question answering. However, training such models requires substantial computational resources, large curated datasets, and sophisticated alignment procedures. As a result, they constitute highly valuable intellectual property (IP) assets that warrant robust protection mechanisms. Existing IP protection ap",
      "tags": [
        "safety",
        "reasoning"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2511-10453v1",
      "title": "Reasoning About Intent for Ambiguous Requests",
      "authors": "Irina Saparina, Mirella Lapata",
      "date": "2025-11-13",
      "url": "https://arxiv.org/abs/2511.10453v1",
      "abstract": "Large language models often respond to ambiguous requests by implicitly committing to one interpretation. Intent misunderstandings can frustrate users and create safety risks. To address this, we propose generating multiple interpretation-answer pairs in a single structured response to ambiguous requests. Our models are trained with reinforcement learning and customized reward functions using multiple valid answers as supervision. Experiments on conversational question answering and semantic par",
      "tags": [
        "safety",
        "vision",
        "reasoning"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2511-10094v2",
      "title": "How does My Model Fail? Automatic Identification and Interpretation of Physical Plausibility Failure Modes with Matryoshka Transcoders",
      "authors": "Yiming Tang, Abhijeet Sinha, Dianbo Liu",
      "date": "2025-11-13",
      "url": "https://arxiv.org/abs/2511.10094v2",
      "abstract": "Although recent generative models are remarkably capable of producing instruction-following and realistic outputs, they remain prone to notable physical plausibility failures. Though critical in applications, these physical plausibility errors often escape detection by existing evaluation methods. Furthermore, no framework exists for automatically identifying and interpreting specific physical error patterns in natural language, preventing targeted model improvements. We introduce Matryoshka Tra",
      "tags": [],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2511-09432v1",
      "title": "Group Equivariance Meets Mechanistic Interpretability: Equivariant Sparse Autoencoders",
      "authors": "Ege Erdogan, Ana Lucic",
      "date": "2025-11-12",
      "url": "https://arxiv.org/abs/2511.09432v1",
      "abstract": "Sparse autoencoders (SAEs) have proven useful in disentangling the opaque activations of neural networks, primarily large language models, into sets of interpretable features. However, adapting them to domains beyond language, such as scientific data with group symmetries, introduces challenges that can hinder their effectiveness. We show that incorporating such group symmetries into the SAEs yields features more useful in downstream tasks. More specifically, we train autoencoders on synthetic i",
      "tags": [
        "SAE",
        "features"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2511-10694v2",
      "title": "Where does an LLM begin computing an instruction?",
      "authors": "Aditya Pola, Vineeth N. Balasubramanian",
      "date": "2025-11-12",
      "url": "https://arxiv.org/abs/2511.10694v2",
      "abstract": "Following an instruction involves distinct sub-processes, such as reading content, reading the instruction, executing it, and producing an answer. We ask where, along the layer stack, instruction following begins, the point where reading gives way to doing. We introduce three simple datasets (Key-Value, Quote Attribution, Letter Selection) and two hop compositions of these tasks. Using activation patching on minimal-contrast prompt pairs, we measure a layer-wise flip rate that indicates when sub",
      "tags": [],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2511-09394v1",
      "title": "A multimodal AI agent for clinical decision support in ophthalmology",
      "authors": "Danli Shi, Xiaolan Chen, Bingjie Yan, Weiyi Zhang, Pusheng Xu, et al.",
      "date": "2025-11-12",
      "url": "https://arxiv.org/abs/2511.09394v1",
      "abstract": "Artificial intelligence has shown promise in medical imaging, yet most existing systems lack flexibility, interpretability, and adaptability - challenges especially pronounced in ophthalmology, where diverse imaging modalities are essential. We present EyeAgent, the first agentic AI framework for comprehensive and interpretable clinical decision support in ophthalmology. Using a large language model (DeepSeek-V3) as its central reasoning engine, EyeAgent interprets user queries and dynamically o",
      "tags": [
        "vision",
        "reasoning"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2511-07896v1",
      "title": "SparseRM: A Lightweight Preference Modeling with Sparse Autoencoder",
      "authors": "Dengcan Liu, Jiahao Li, Zheren Fu, Yi Tu, Jiajun Li, et al.",
      "date": "2025-11-11",
      "url": "https://arxiv.org/abs/2511.07896v1",
      "abstract": "Reward models (RMs) are a core component in the post-training of large language models (LLMs), serving as proxies for human preference evaluation and guiding model alignment. However, training reliable RMs under limited resources remains challenging due to the reliance on large-scale preference annotations and the high cost of fine-tuning LLMs. To address this, we propose SparseRM, which leverages Sparse Autoencoder (SAE) to extract preference-relevant information encoded in model representation",
      "tags": [
        "SAE",
        "safety"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2511-08379v2",
      "title": "SOM Directions are Better than One: Multi-Directional Refusal Suppression in Language Models",
      "authors": "Giorgio Piras, Raffaele Mura, Fabio Brau, Luca Oneto, Fabio Roli, et al.",
      "date": "2025-11-11",
      "url": "https://arxiv.org/abs/2511.08379v2",
      "abstract": "Refusal refers to the functional behavior enabling safety-aligned language models to reject harmful or unethical prompts. Following the growing scientific interest in mechanistic interpretability, recent work encoded refusal behavior as a single direction in the model's latent space; e.g., computed as the difference between the centroids of harmful and harmless prompt representations. However, emerging evidence suggests that concepts in LLMs often appear to be encoded as a low-dimensional manifo",
      "tags": [
        "safety"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2511-06739v1",
      "title": "Rank-1 LoRAs Encode Interpretable Reasoning Signals",
      "authors": "Jake Ward, Paul Riechers, Adam Shai",
      "date": "2025-11-10",
      "url": "https://arxiv.org/abs/2511.06739v1",
      "abstract": "Reasoning models leverage inference-time compute to significantly enhance the performance of language models on difficult logical tasks, and have become a dominating paradigm in frontier LLMs. Despite their wide adoption, the mechanisms underpinning the enhanced performance of these reasoning models are not well understood. In this work, we show that the majority of new capabilities in reasoning models can be elicited by small, single-rank changes to base model parameters, with many of these cha",
      "tags": [
        "reasoning"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2511-06997v1",
      "title": "Beyond Phasors: Solving Non-Sinusoidal Electrical Circuits using Geometry",
      "authors": "Javier Castillo-Mart\u00ednez, Raul Ba\u00f1os, Francisco G. Montoya",
      "date": "2025-11-10",
      "url": "https://arxiv.org/abs/2511.06997v1",
      "abstract": "Classical phasor analysis is fundamentally limited to sinusoidal single-frequency conditions, which poses challenges when working in the presence of harmonics. Furthermore, the conventional solution, which consists of decomposing signals using Fourier series and applying superposition, is a fragmented process that does not provide a unified solution in the frequency domain. This paper overcomes this limitation by introducing a complete and direct approach for multi-harmonic AC circuits using Geo",
      "tags": [
        "circuits",
        "superposition"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2511-07572v1",
      "title": "SCALAR: Benchmarking SAE Interaction Sparsity in Toy LLMs",
      "authors": "Sean P. Fillingham, Andrew Gordon, Peter Lai, Xavier Poncini, David Quarel, et al.",
      "date": "2025-11-10",
      "url": "https://arxiv.org/abs/2511.07572v1",
      "abstract": "Mechanistic interpretability aims to decompose neural networks into interpretable features and map their connecting circuits. The standard approach trains sparse autoencoders (SAEs) on each layer's activations. However, SAEs trained in isolation don't encourage sparse cross-layer connections, inflating extracted circuits where upstream features needlessly affect multiple downstream features. Current evaluations focus on individual SAE performance, leaving interaction sparsity unexamined. We intr",
      "tags": [
        "SAE",
        "circuits",
        "features"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2511-07498v2",
      "title": "Focusing on Language: Revealing and Exploiting Language Attention Heads in Multilingual Large Language Models",
      "authors": "Xin Liu, Qiyang Song, Qihang Zhou, Haichao Du, Shaowen Xu, et al.",
      "date": "2025-11-10",
      "url": "https://arxiv.org/abs/2511.07498v2",
      "abstract": "Large language models (LLMs) increasingly support multilingual understanding and generation. Meanwhile, efforts to interpret their internal mechanisms have emerged, offering insights to enhance multilingual performance. While multi-head self-attention (MHA) has proven critical in many areas, its role in multilingual capabilities remains underexplored. In this work, we study the contribution of MHA in supporting multilingual processing in LLMs. We propose Language Attention Head Importance Scores",
      "tags": [
        "attention"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2511-06048v1",
      "title": "Visual Exploration of Feature Relationships in Sparse Autoencoders with Curated Concepts",
      "authors": "Xinyuan Yan, Shusen Liu, Kowshik Thopalli, Bei Wang",
      "date": "2025-11-08",
      "url": "https://arxiv.org/abs/2511.06048v1",
      "abstract": "Sparse autoencoders (SAEs) have emerged as a powerful tool for uncovering interpretable features in large language models (LLMs) through the sparse directions they learn. However, the sheer number of extracted directions makes comprehensive exploration intractable. While conventional embedding techniques such as UMAP can reveal global structure, they suffer from limitations including high-dimensional compression artifacts, overplotting, and misleading neighborhood distortions. In this work, we p",
      "tags": [
        "SAE",
        "features",
        "vision"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "arxiv-2511-05923v3",
      "title": "Causal Tracing of Object Representations in Large Vision Language Models: Mechanistic Interpretability and Hallucination Mitigation",
      "authors": "Qiming Li, Zekai Ye, Xiaocheng Feng, Weihong Zhong, Weitao Ma, et al.",
      "date": "2025-11-08",
      "url": "https://arxiv.org/abs/2511.05923v3",
      "abstract": "Despite the remarkable advancements of Large Vision-Language Models (LVLMs), the mechanistic interpretability remains underexplored. Existing analyses are insufficiently comprehensive and lack examination covering visual and textual tokens, model components, and the full range of layers. This limitation restricts actionable insights to improve the faithfulness of model output and the development of downstream tasks, such as hallucination mitigation. To address this limitation, we introduce Fine-",
      "tags": [
        "vision"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "ss-48878e7db51b49256fc5",
      "title": "Analysis of gas pressure decline SF6 in 70kV circuit breaker bay transformer 3 at Babakan substation",
      "authors": "Arif Ramdhan Achmad, Muhammad Soleh, Sugeng Suprijadi",
      "date": "2025-10-29",
      "url": "https://www.semanticscholar.org/paper/48878e7db51b49256fc5bf294f17bd45a6034974",
      "abstract": "This study analyzes the decrease in SF\u2086 gas pressure in the 70 kV circuit breaker Transformer Bay 3 at Babakan Substation, which affects the reliability of the power system. SF\u2086 gas is used as an insulating medium due to its high dielectric strength but is susceptible to micro-leakage, which degrades equipment performance. This research employs a qualitative approach with a case study strategy based on primary data obtained from field observations, interviews, and testing documentation before an",
      "tags": [
        "circuits"
      ],
      "source": "Semantic Scholar",
      "featured": false
    },
    {
      "id": "ss-5634557d0ad623f77c52",
      "title": "Equivalent Circuit-Based Analysis of Current Sharing in Planar Transformer Parallel Windings",
      "authors": "Yan Liang, Yi-Hsun Hsieh, Qiang Li",
      "date": "2025-10-19",
      "url": "https://www.semanticscholar.org/paper/5634557d0ad623f77c528c1f7e98aace74550bef",
      "abstract": "Planar transformers with PCB windings are increasingly popular in modern power conversion systems due to their controllable design and high power density. The small core volume of planar transformers benefits from high-frequency operation. However, high-frequency operation also introduces undesirable effects such as current redistribution within the windings-specifically, induced eddy currents within individual winding layers and uneven current sharing among parallel layers. These phenomena sign",
      "tags": [
        "circuits"
      ],
      "source": "Semantic Scholar",
      "featured": false
    },
    {
      "id": "circuits-october-2025",
      "title": "Circuits Updates - October 2025",
      "authors": "Anthropic Interpretability Team",
      "date": "2025-10-15",
      "url": "https://transformer-circuits.pub/2025/october-update/index.html",
      "abstract": "Explores how LLMs perceive higher-level semantic concepts encoded visually in text, including ASCII art recognition and SVG understanding.",
      "tags": [
        "circuits",
        "vision",
        "update"
      ],
      "source": "Anthropic",
      "featured": false
    },
    {
      "id": "ss-4776d9326ce4c2152fff",
      "title": "Loss Analysis of the Clamping Circuit in an IGCT-Based High Step-Up Resonant DC transformer",
      "authors": "Yiqing Ma, Jialiang Hu, Bin Cui, Xueteng Tang, Hongjie Gong, et al.",
      "date": "2025-10-14",
      "url": "https://www.semanticscholar.org/paper/4776d9326ce4c2152fffbad8e2f7ce3405f5ef90",
      "abstract": "This paper investigates the loss behavior of the clamping circuit in a high step-up resonant DC transformer (IGCT-SRDCT) based on integrated gate commutated thyristor (IGCT) and series-connected diodes. The proposed topology features a simple structure, high voltage gain, and high efficiency, making it highly suitable for medium-voltage DC collection in renewable energy systems. Due to the zero-current switching (ZCS) characteristics of the IGCT-SRDCT, the clamping circuit is triggered only duri",
      "tags": [
        "circuits",
        "features"
      ],
      "source": "Semantic Scholar",
      "featured": false
    },
    {
      "id": "ss-963261e1e7f0ec0444e1",
      "title": "Leveraging extra high voltage grid supplies for the dynamic performance analysis of large power transformer in short-circuit scenario",
      "authors": "Narendra Kotta, Jayaram Nakka",
      "date": "2025-09-24",
      "url": "https://www.semanticscholar.org/paper/963261e1e7f0ec0444e1d43f15578b2c86dd0d1b",
      "abstract": "Abstract The current study introduces a novel approach to evaluating the short-circuit (SC) withstand capability of 3-phase, 400/220/33\u202fkV, 500\u202fMVA power transformer through dynamic short-circuit (DSC) tests conducted under actual grid conditions. Departing from conventional lab-based methods, it utilizes a 765\u202fkV grid supply for high-voltage to medium-voltage (HV-MV) windings (i.e., 400kV\u2013220kV) and a 400\u202fkV grid supply for medium-voltage to low-voltage (MV-LV) windings (i.e., 220kV\u201333kV), repl",
      "tags": [
        "circuits"
      ],
      "source": "Semantic Scholar",
      "featured": false
    },
    {
      "id": "ss-1573e19c12ff834341c0",
      "title": "Short-circuit Stress Analysis and Evaluation of 500kV Transformer Windings",
      "authors": "Shijun Wang, Fan Yang, Pengbo Wang, Yuchen Huang, Fanhan Liu",
      "date": "2025-09-19",
      "url": "https://www.semanticscholar.org/paper/1573e19c12ff834341c01496fce1ad9e191dade0",
      "abstract": "When an electrical transformer is short-circuited, the windings are subjected to electromagnetic forces that are tens of times greater than their normal operating conditions. If the mechanical strength of the transformer is insufficient, it may cause the structural instability of the transformer windings. High-voltage level transformers typically have larger capacities, and during short-circuit faults, they are subjected to greater short-circuit electromagnetic forces. Therefore, the assessment ",
      "tags": [
        "circuits"
      ],
      "source": "Semantic Scholar",
      "featured": false
    },
    {
      "id": "ss-d1345252f2341d8e379a",
      "title": "Fault handling of frequent Automatic pressurization of 220kV circuit breakers in a 220kV transformer substation",
      "authors": "Ke Wang, Yuqi Xie, Junxian Huang, Xiangyin Liu, Weipeng Tang, et al.",
      "date": "2025-09-19",
      "url": "https://www.semanticscholar.org/paper/d1345252f2341d8e379a0e5da578c296c956a04e",
      "abstract": "In response to the abnormal phenomenon of frequent automatic pressurization of the hydraulic mechanism of the 604 circuit breaker in a 220kV transformer substation (with an average of 140 pressure replenishment times per day, far exceeding the standard value by 4 to 6 times), this paper conducted systematic fault tracing through pressure monitoring, hydraulic circuit analysis, and disassembly inspection. Field tests indicated that the pressure holding of the C-phase mechanism failed, with a cont",
      "tags": [
        "circuits"
      ],
      "source": "Semantic Scholar",
      "featured": false
    },
    {
      "id": "circuits-august-2025",
      "title": "Circuits Updates - August 2025",
      "authors": "Anthropic Interpretability Team",
      "date": "2025-08-15",
      "url": "https://transformer-circuits.pub/2025/august-update/index.html",
      "abstract": "Updates on SAE training improvements, crosscoder developments, and applications of MI to biological systems.",
      "tags": [
        "circuits",
        "SAE",
        "update"
      ],
      "source": "Anthropic",
      "featured": false
    },
    {
      "id": "salmon-domain-sae",
      "title": "Resurrecting the Salmon: Rethinking Mechanistic Interpretability with Domain-Specific Sparse Autoencoders",
      "authors": "Various",
      "date": "2025-08-15",
      "url": "https://arxiv.org/abs/2508.09363",
      "abstract": "Shows that restricting SAE training to well-defined domains (e.g., medical text) reallocates capacity to domain-specific features, reducing 'dark matter' in reconstruction error.",
      "tags": [
        "SAE",
        "domain-specific",
        "methods"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "ss-715ce9e24a552ad614cb",
      "title": "Single-Phase Transformer Circuit Solving using Harmonic Interpolation Method",
      "authors": "Srdjan Divac, M. Najgebauer, Krzystof Chwastek, B. Koprivica",
      "date": "2025-08-04",
      "url": "https://www.semanticscholar.org/paper/715ce9e24a552ad614cbf07e2f96c41747d44caf",
      "abstract": "The aim of this paper is to present a way of solving in the time domain an electrical circuit with a single-phase transformer. The considered circuit consists of an AC power supply, a ring-shaped transformer and a resistor in the transformer primary and secondary circuit. Three working regimes of the transformer have been considered \u2013 when it is loaded and when it is close to no-load and short-circuit working conditions. Calculations have been made at industrial frequency of 50 Hz using an itera",
      "tags": [
        "circuits"
      ],
      "source": "Semantic Scholar",
      "featured": false
    },
    {
      "id": "ss-78f88d03bc66e3bcd38b",
      "title": "Autoencoder-like Sparse Non-Negative Matrix Factorization with Structure Relationship Preservation",
      "authors": "Ling Zhong, Haiyan Gao",
      "date": "2025-08-01",
      "url": "https://www.semanticscholar.org/paper/78f88d03bc66e3bcd38b2b900f775cb3d88f070b",
      "abstract": "Clustering algorithms based on non-negative matrix factorization (NMF) have garnered significant attention in data mining due to their strong interpretability and computational simplicity. However, traditional NMF often struggles to effectively capture and preserve topological structure information between data during low-dimensional representation. Therefore, this paper proposes an autoencoder-like sparse non-negative matrix factorization with structure relationship preservation (ASNMF-SRP). Fi",
      "tags": [],
      "source": "Semantic Scholar",
      "featured": false
    },
    {
      "id": "ss-88197609439a077faa78",
      "title": "Analysis of the impact of integrating high-sensitivity short-circuit protection devices on transformer protection",
      "authors": "Tianqi Liu, Shuyan Pan, Xiaobao Liu, Wenhua Lu",
      "date": "2025-08-01",
      "url": "https://www.semanticscholar.org/paper/88197609439a077faa789cc03fc8f9bdf58d1cc9",
      "abstract": "With the expansion of power grids, increasing short-circuit currents pose significant risks to the safety and stability of transformers. Traditional methods, such as impedance adjustment and series reactors, face limitations in control capability and economic feasibility, while the application of fault current limiters (FCL) offers a new solution. This paper takes the operational parameters of an existing 110 kV substation as a case study, constructing a simulation model to analyze the short-cir",
      "tags": [
        "circuits",
        "safety"
      ],
      "source": "Semantic Scholar",
      "featured": false
    },
    {
      "id": "ss-dc547a1ff2089d94633c",
      "title": "Power Transformer Short-Circuit Force Calculation Using Three and Two-Dimensional Finite-Element Analysis",
      "authors": "Jian Wang, Junchi He, Xiaohan Chen, Tian Tian, Chenguo Yao, et al.",
      "date": "2025-07-22",
      "url": "https://www.semanticscholar.org/paper/dc547a1ff2089d94633c1124d339381d6b1afc6b",
      "abstract": "In a power transformer short-circuit, transient current and magnetic flux interactions create strong electromagnetic forces that can deform windings and the core, risking failure. Accurate calculation of these forces during design is critical to prevent such outcomes. This paper employs two-dimensional (2D) and three-dimensional (3D) finite-element analysis (FEA) to model a 110 kV, 40 MVA three-phase transformer, calculating magnetic flux density, short-circuit current, and electromagnetic force",
      "tags": [
        "circuits"
      ],
      "source": "Semantic Scholar",
      "featured": false
    },
    {
      "id": "circuits-july-2025",
      "title": "Circuits Updates - July 2025",
      "authors": "Anthropic Interpretability Team",
      "date": "2025-07-01",
      "url": "https://transformer-circuits.pub/2025/july-update/index.html",
      "abstract": "Findings on arithmetic heuristics, lookup table features, and applications to biology including Evo 2 DNA foundation models.",
      "tags": [
        "circuits",
        "arithmetic",
        "biology"
      ],
      "source": "Anthropic",
      "featured": false
    },
    {
      "id": "ss-27b5c94a919aa50ae2da",
      "title": "Analysis of a Trip Accident Caused by Short Circuit Impact of 35kV Side Outlet Cable of the Main Transformer",
      "authors": "Dewen Zhang, Hang Lu, Yuenan Guo, Guangcheng Zhu, Lei Yu, et al.",
      "date": "2025-06-28",
      "url": "https://www.semanticscholar.org/paper/27b5c94a919aa50ae2da7327d196192c364b5401",
      "abstract": "Power cable is the backbone of the power system, once the failure will produce significant losses. The analysis of power system faults can not only prevent similar faults from happening again, reduce the loss caused by power faults, but also find the hidden dangers of transmission lines in time, so as to prevent them before they occur. In this paper, a trip accident of main transformer caused by cable short circuit impact is introduced in detail. Firstly, the anti-short circuit ability of transf",
      "tags": [
        "circuits"
      ],
      "source": "Semantic Scholar",
      "featured": false
    },
    {
      "id": "arch-degeneracy",
      "title": "Mechanistic Interpretability in the Presence of Architectural Degeneracy",
      "authors": "Various",
      "date": "2025-06-23",
      "url": "https://arxiv.org/abs/2506.18053",
      "abstract": "Addresses challenges of mechanistic interpretability when multiple architectures can implement the same function.",
      "tags": [
        "theory",
        "challenges"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "ss-c61dec28ac0c5720f035",
      "title": "Analysis and Diagnosis of Inner-Turn Short Circuit of UHV Converter Transformer Windings",
      "authors": "Tao Tong, Yang Liu, Yan Liu, Junhui Xiong",
      "date": "2025-06-20",
      "url": "https://www.semanticscholar.org/paper/c61dec28ac0c5720f035108e7ac013ef340c0565",
      "abstract": "As the core component of the UHV DC transmission system, the internal failure of the converter may cause serious power failure. This paper focuses on the sudden abnormal increase of total hydrocarbons in a UHV converter, and comprehensively utilizes oil chromatography analysis, on-site diagnostic test, disassembly inspection at the factory, and multi-physics field simulation verification to investigate the fault mechanism of the short-circuiting between strands of the valve-side coil of the conv",
      "tags": [
        "circuits"
      ],
      "source": "Semantic Scholar",
      "featured": false
    },
    {
      "id": "protein-saes",
      "title": "Sparse Autoencoders Uncover Biologically Interpretable Features in Protein Language Models",
      "authors": "Various",
      "date": "2025-06-15",
      "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC12403088/",
      "abstract": "SAEs applied to protein language models reveal interpretable biological concepts and missing database annotations. Published in PNAS.",
      "tags": [
        "SAE",
        "biology",
        "proteins"
      ],
      "source": "PNAS",
      "featured": true
    },
    {
      "id": "open-source-circuit-tracing",
      "title": "Open-Sourcing Circuit-Tracing Tools",
      "authors": "Anthropic",
      "date": "2025-06-01",
      "url": "https://www.anthropic.com/research/open-source-circuit-tracing",
      "abstract": "Anthropic open-sources their circuit tracing library, enabling generation of attribution graphs on popular open-weights models like Gemma-2-2b and Llama-3.2-1b.",
      "tags": [
        "circuits",
        "tools",
        "open-source"
      ],
      "source": "Anthropic",
      "featured": true
    },
    {
      "id": "ss-9884470cb4c65e097dd0",
      "title": "Error analysis and accuracy improvement strategy of transformer winding short circuit electromagnetic force calculation model",
      "authors": "Dengfeng Zhu, She Chen, Yudi Wu, Hang Guo, Lipeng Zhong, et al.",
      "date": "2025-05-16",
      "url": "https://www.semanticscholar.org/paper/9884470cb4c65e097dd07811d4cf4cfd160d1369",
      "abstract": "",
      "tags": [
        "circuits"
      ],
      "source": "Semantic Scholar",
      "featured": false
    },
    {
      "id": "ss-79a61c84ab5564024453",
      "title": "Example analysis and preventive proposal of 110kV transformer short circuit fault",
      "authors": "Jinghui Ruan, Puming Xu, Ping Peng, Yun Liu, Lipeng Zhong",
      "date": "2025-04-18",
      "url": "https://www.semanticscholar.org/paper/79a61c84ab5564024453dd8a9d36c6ef0bb71400",
      "abstract": "A typical case of short-circuit fault of 110kV main transformer is introduced, and the equipment inspection, diagnostic test results and suspension hood disassembly are analyzed. The initial cause and development process of transformer under short-circuit current impact, as well as the vibration and arc ablation morphology of winding under electrodynamic action are sorted out, and the short-circuit damage causes of transformer are identified. According to the operation status of the main transfo",
      "tags": [
        "circuits"
      ],
      "source": "Semantic Scholar",
      "featured": false
    },
    {
      "id": "circuits-april-2025",
      "title": "Circuits Updates - April 2025",
      "authors": "Anthropic Interpretability Team",
      "date": "2025-04-15",
      "url": "https://transformer-circuits.pub/2025/april-update/index.html",
      "abstract": "Latest research updates on circuit analysis and feature discovery.",
      "tags": [
        "circuits",
        "update"
      ],
      "source": "Anthropic",
      "featured": false
    },
    {
      "id": "ss-7348f7b3b4f9e09d492d",
      "title": "Implementation of a Dual Transformer Model Based on Unified Magnetic Equivalent Circuit for Saturation Analysis and Transient Electromagnetic Simulation",
      "authors": "Lu Gao, Feng Ji, Jindi Pang, Chen Jia, Yingying Wang",
      "date": "2025-04-15",
      "url": "https://www.semanticscholar.org/paper/7348f7b3b4f9e09d492db428cd1c038b9d948395",
      "abstract": "",
      "tags": [
        "circuits"
      ],
      "source": "Semantic Scholar",
      "featured": false
    },
    {
      "id": "ss-18b2cd9e4b1738c98e7f",
      "title": "Analysis of Transformer Tank Deformation and Rupture, Part 2: Checking of Ability of Transformer Tank Withstand Internal Short-Circuit Arc Fault",
      "authors": "Zhigang Zhao",
      "date": "2025-04-15",
      "url": "https://www.semanticscholar.org/paper/18b2cd9e4b1738c98e7f4f2d4eebc6649ecb627e",
      "abstract": "In this paper, by comparing the transmission capacity of different voltage levels and the requirements of system power supply configuration, it is discussed that the risk of oil tank deformation and rupture increases when facing potential internal short-circuit arc fault, which puts forward higher requirements for large oil-filled transformer equipment oil tank's ability to withstand internal short-circuit arc fault. Through the calculation of single-phase short-circuit current, analysis of inst",
      "tags": [
        "circuits"
      ],
      "source": "Semantic Scholar",
      "featured": false
    },
    {
      "id": "ss-63832a9155720a52f9b2",
      "title": "Exploring Mechanistic Interpretability in Large Language Models: Challenges, Approaches, and Insights",
      "authors": "Sandeep Reddy Gantla",
      "date": "2025-03-28",
      "url": "https://www.semanticscholar.org/paper/63832a9155720a52f9b2f209a25f5d686f6d7b84",
      "abstract": "Recent advances in large language models (LLMs) have significantly enhanced their performance across a wide array of tasks. However, the lack of interpretability has become a critical concern, particularly as these models grow in size and complexity. Mechanistic interpretability seeks to uncover the internal workings of neural networks, offering valuable insights into their decision-making processes, biases, and potential safety risks. This survey delves into the emerging field of mechanistic in",
      "tags": [
        "safety",
        "survey"
      ],
      "source": "Semantic Scholar",
      "featured": false
    },
    {
      "id": "biology-llm",
      "title": "On the Biology of a Large Language Model",
      "authors": "Anthropic Interpretability Team",
      "date": "2025-03-27",
      "url": "https://transformer-circuits.pub/2025/attribution-graphs/biology.html",
      "abstract": "Landmark paper using attribution graphs to study Claude 3.5 Haiku's internal reasoning, finding shared conceptual spaces where reasoning happens before translation to language.",
      "tags": [
        "circuits",
        "reasoning",
        "biology"
      ],
      "source": "Anthropic",
      "featured": true
    },
    {
      "id": "sae-survey-2025",
      "title": "A Survey on Sparse Autoencoders: Interpreting the Internal Mechanisms of Large Language Models",
      "authors": "Various",
      "date": "2025-03-15",
      "url": "https://arxiv.org/abs/2503.05613",
      "abstract": "Comprehensive survey presenting SAEs for interpreting and understanding the internal workings of LLMs.",
      "tags": [
        "SAE",
        "survey",
        "review"
      ],
      "source": "arXiv",
      "featured": true
    },
    {
      "id": "saebench",
      "title": "SAEBench: A Comprehensive Benchmark for Sparse Autoencoders in Language Model Interpretability",
      "authors": "Various",
      "date": "2025-03-10",
      "url": "https://arxiv.org/abs/2503.09532",
      "abstract": "Introduces comprehensive benchmarks for evaluating SAE quality, interpretability, and downstream task performance.",
      "tags": [
        "SAE",
        "benchmark",
        "evaluation"
      ],
      "source": "arXiv",
      "featured": true
    },
    {
      "id": "ss-85d3d9064e142dbd0164",
      "title": "Modeling, Analysis, and Online Detection of\u00a0Interturn Short-Circuit Fault in Medium-Frequency Transformer of Dual-Active-Bridge Converter",
      "authors": "T. J. Nistane, Samarjeet Singh, S. Payami, Kalaiselvi Jayaraman",
      "date": "2025-03-01",
      "url": "https://www.semanticscholar.org/paper/85d3d9064e142dbd01647ebeda295f5e836bb197",
      "abstract": "The strident working condition of the dual-active-bridge (DAB) converter has elevated the risk of interturn short-circuit fault (ITTSCF) in medium-frequency transformer (MFT). Thus, it is crucial to analyze the DAB converter under ITTSCF in MFT to avoid the complete winding short-circuit due to insulation failure. Thus, the presented work analyses the variations in the magnetizing and leakage inductance of MFT ranging from low to high severity levels of ITTSCF. Thereafter, its impact on the curr",
      "tags": [
        "circuits"
      ],
      "source": "Semantic Scholar",
      "featured": false
    },
    {
      "id": "ss-2ac42be0b5fb0b61f4be",
      "title": "From Mechanistic Interpretability to Mechanistic Biology: Training, Evaluating, and Interpreting Sparse Autoencoders on Protein Language Models",
      "authors": "Etowah Adams, Liam Bai, Minji Lee, Yiyang Yu, Mohammed Alquraishi",
      "date": "2025-02-08",
      "url": "https://www.semanticscholar.org/paper/2ac42be0b5fb0b61f4be220bbd65322bed59ecbf",
      "abstract": "Protein language models (pLMs) are powerful predictors of protein structure and function, learning through unsupervised training on millions of protein sequences. pLMs are thought to capture common motifs in protein sequences, but the specifics of pLM features are not well understood. Identifying these features would not only shed light on how pLMs work, but potentially uncover novel protein biology\u2013\u2013studying the model to study the biology. Motivated by this, we train sparse autoencoders (SAEs) ",
      "tags": [
        "SAE",
        "features",
        "biology"
      ],
      "source": "Semantic Scholar",
      "featured": false
    },
    {
      "id": "ss-e7acedb59ac28db1f029",
      "title": "Periodic Segmentation Transformer-Based Internal Short Circuit Detection Method for Battery Packs",
      "authors": "Zhekang Dong, Shenyu Gu, Shiqi Zhou, Mengjie Yang, Chun Sing Lai, et al.",
      "date": "2025-02-01",
      "url": "https://www.semanticscholar.org/paper/e7acedb59ac28db1f02936f5d23172d59d0650b4",
      "abstract": "With the rapid evolution of electric vehicles (EVs), assuring the security and dependability of battery packs has acquired paramount significance. Internal short circuit (ISC) within EV battery packs poses a threat to the safety and reliability of EVs. Most of the existing ISC detection methods still suffer from two limitations, i.e., the dataset incompleteness and poor feature representation. To address these challenges, we develop a periodic segmentation Transformer-based ISC detection method ",
      "tags": [
        "circuits",
        "features",
        "safety"
      ],
      "source": "Semantic Scholar",
      "featured": false
    },
    {
      "id": "open-problems-mi",
      "title": "Open Problems in Mechanistic Interpretability",
      "authors": "Lee Sharkey, Bilal Chughtai, Joshua Batson, Jack Lindamood, et al.",
      "date": "2025-01-27",
      "url": "https://arxiv.org/abs/2501.16496",
      "abstract": "Comprehensive survey of open problems requiring solutions before scientific and practical benefits of mechanistic interpretability can be realized.",
      "tags": [
        "survey",
        "open-problems",
        "methodology"
      ],
      "source": "arXiv",
      "featured": true
    },
    {
      "id": "transcoders-beat-sae",
      "title": "Transcoders Beat Sparse Autoencoders for Interpretability",
      "authors": "Various",
      "date": "2025-01-25",
      "url": "https://arxiv.org/abs/2501.18823",
      "abstract": "Shows transcoder features are significantly more interpretable than SAE features, and proposes 'skip transcoders' with lower reconstruction loss.",
      "tags": [
        "transcoders",
        "SAE",
        "methods"
      ],
      "source": "arXiv",
      "featured": true
    },
    {
      "id": "apd-2025",
      "title": "Interpretability in Parameter Space: Attribution-based Parameter Decomposition",
      "authors": "Various",
      "date": "2025-01-24",
      "url": "https://arxiv.org/abs/2501.14926",
      "abstract": "Identifies minimal circuits in superposition and separates compressed computations.",
      "tags": [
        "methods",
        "superposition",
        "circuits"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "low-rank-sae",
      "title": "Low-rank Adapting Models for Sparse Autoencoders",
      "authors": "M. Chen, J. Engels, M. Tegmark",
      "date": "2025-01-20",
      "url": "https://arxiv.org/abs/2501.19406",
      "abstract": "Combines LoRA with SAE training for more efficient feature extraction.",
      "tags": [
        "SAE",
        "efficiency",
        "LoRA"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "circuit-tracing-2025",
      "title": "Circuit Tracing: Revealing Computational Graphs in Language Models",
      "authors": "Anthropic Interpretability Team",
      "date": "2025-01-15",
      "url": "https://transformer-circuits.pub/2025/attribution-graphs/methods.html",
      "abstract": "Breakthrough method for understanding how features interact through attribution graphs, enabling analysis of computational structure.",
      "tags": [
        "circuits",
        "attribution",
        "methods"
      ],
      "source": "Anthropic",
      "featured": true
    },
    {
      "id": "ss-0fb8bc41122f793a390a",
      "title": "Simulation Analysis and Scaling Test of Transformer Turn to Turn Short\u2010Circuit Transient Characteristics Based on Field\u2010Circuit Cooperation",
      "authors": "Zhiwei Chen, Ruibo Yang, Junxian Wang, Nianwen Xiang, Xin Liu",
      "date": "2025-01-14",
      "url": "https://www.semanticscholar.org/paper/0fb8bc41122f793a390a4b02d995523f9d0c13e3",
      "abstract": "It is a common problem that transformers have different degrees of the turn to turn short\u2010circuit (TTS). The fault severity depends on the location and size of the short\u2010circuit. In this paper, the position of TTS is controlled in the middle of the transformer winding. And then, one transformer with 5%, 10% and 20% TTS on the high voltage side of the phase B was constructed. The transformer TTS faults from slight to deep are simulated, and the effect of different degrees of TTS on the transforme",
      "tags": [
        "circuits"
      ],
      "source": "Semantic Scholar",
      "featured": false
    },
    {
      "id": "axbench",
      "title": "AxBench: Steering LLMs? Even Simple Baselines Outperform Sparse Autoencoders",
      "authors": "Various",
      "date": "2025-01-10",
      "url": "https://arxiv.org/abs/2501.17148",
      "abstract": "Benchmark showing that simple steering methods can outperform SAE-based steering, raising questions about SAE utility for control.",
      "tags": [
        "SAE",
        "steering",
        "benchmark"
      ],
      "source": "ICML 2025",
      "featured": true
    },
    {
      "id": "cb-sae",
      "title": "Interpretable and Steerable Concept Bottleneck Sparse Autoencoders",
      "authors": "Various",
      "date": "2024-12-20",
      "url": "https://arxiv.org/abs/2512.10805",
      "abstract": "Proposes Concept Bottleneck SAEs to address the finding that majority of SAE neurons exhibit low interpretability or low steerability.",
      "tags": [
        "SAE",
        "concepts",
        "steering"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "antibody-sae",
      "title": "Mechanistic Interpretability of Antibody Language Models Using SAEs",
      "authors": "Various",
      "date": "2024-12-18",
      "url": "https://arxiv.org/abs/2512.05794",
      "abstract": "Uses TopK and Ordered SAEs to investigate antibody language models, finding biologically meaningful latent features.",
      "tags": [
        "SAE",
        "biology",
        "antibodies"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "stage-wise-diffing",
      "title": "Stage-Wise Model Diffing",
      "authors": "Anthropic Interpretability Team",
      "date": "2024-12-10",
      "url": "https://transformer-circuits.pub/2024/stage-diffing/",
      "abstract": "Method for comparing model behavior across training stages using sparse features.",
      "tags": [
        "methods",
        "training-dynamics",
        "diffing"
      ],
      "source": "Anthropic",
      "featured": false
    },
    {
      "id": "singular-vectors-interp",
      "title": "Beyond Components: Singular Vector-Based Interpretability of Transformer Circuits",
      "authors": "Various",
      "date": "2024-11-30",
      "url": "https://arxiv.org/abs/2511.20273",
      "abstract": "Shows transformer computation is distributed along interpretable, low-rank axes that can be independently manipulated.",
      "tags": [
        "circuits",
        "linear-algebra",
        "low-rank"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "weight-sparse-transformers",
      "title": "Weight-Sparse Transformers Have Interpretable Circuits",
      "authors": "Various",
      "date": "2024-11-25",
      "url": "https://arxiv.org/abs/2511.13653",
      "abstract": "Shows that training transformers with weight sparsity leads to more interpretable circuits without post-hoc analysis.",
      "tags": [
        "circuits",
        "sparsity",
        "training"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "gemma-scope",
      "title": "Gemma Scope: Open Sparse Autoencoders Everywhere All At Once on Gemma 2",
      "authors": "Google DeepMind",
      "date": "2024-11-20",
      "url": "https://arxiv.org/abs/2408.05147",
      "abstract": "Release of comprehensive open-source SAEs trained on all layers of Gemma 2 models, enabling community research.",
      "tags": [
        "SAE",
        "open-source",
        "Gemma"
      ],
      "source": "DeepMind",
      "featured": true
    },
    {
      "id": "crosscoders",
      "title": "Sparse Crosscoders for Cross-Layer Features and Model Diffing",
      "authors": "Anthropic Interpretability Team",
      "date": "2024-10-15",
      "url": "https://transformer-circuits.pub/2024/crosscoders/",
      "abstract": "Dictionary learning to find features shared across models and layers, providing evidence for feature universality.",
      "tags": [
        "SAE",
        "universality",
        "features"
      ],
      "source": "Anthropic",
      "featured": true
    },
    {
      "id": "llama-scope",
      "title": "Llama Scope: Extracting Millions of Features from Llama-3.1-8B with Sparse Autoencoders",
      "authors": "Various",
      "date": "2024-10-12",
      "url": "https://arxiv.org/abs/2410.20526",
      "abstract": "Open-source SAEs for Llama-3.1-8B with millions of interpretable features.",
      "tags": [
        "SAE",
        "open-source",
        "Llama"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "td-learning-sae",
      "title": "Sparse Autoencoders Reveal Temporal Difference Learning in Large Language Models",
      "authors": "Various",
      "date": "2024-10-10",
      "url": "https://arxiv.org/abs/2410.06543",
      "abstract": "Discovers that LLMs implement TD-learning-like mechanisms, revealed through SAE analysis.",
      "tags": [
        "SAE",
        "learning",
        "RL"
      ],
      "source": "ICLR 2025",
      "featured": true
    },
    {
      "id": "feature-steering-eval",
      "title": "Evaluating Feature Steering: A Case Study in Mitigating Social Biases",
      "authors": "Anthropic",
      "date": "2024-10-08",
      "url": "https://www.anthropic.com/research/feature-steering-bias",
      "abstract": "Evaluates using SAE features to steer models away from social biases.",
      "tags": [
        "SAE",
        "steering",
        "bias",
        "safety"
      ],
      "source": "Anthropic",
      "featured": false
    },
    {
      "id": "sae-unlearning",
      "title": "Applying Sparse Autoencoders to Unlearn Knowledge in Language Models",
      "authors": "Various",
      "date": "2024-10-05",
      "url": "https://arxiv.org/abs/2410.04321",
      "abstract": "Uses SAE features to selectively unlearn specific knowledge from models.",
      "tags": [
        "SAE",
        "unlearning",
        "safety"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "group-sae",
      "title": "Group-SAE: Efficient Training of Sparse Autoencoders for Large Language Models via Layer Groups",
      "authors": "Various",
      "date": "2024-10-01",
      "url": "https://aclanthology.org/2025.emnlp-main.942.pdf",
      "abstract": "Efficient SAE training method that groups layers for shared feature learning.",
      "tags": [
        "SAE",
        "efficiency",
        "training"
      ],
      "source": "EMNLP 2025",
      "featured": false
    },
    {
      "id": "circuit-compositions",
      "title": "Circuit Compositions: Exploring Modular Structures in Transformer-Based Language Models",
      "authors": "Various",
      "date": "2024-10-01",
      "url": "https://arxiv.org/abs/2410.01434",
      "abstract": "Studies how similar circuits relate to each other and whether models implement reusable functions through composable subnetworks.",
      "tags": [
        "circuits",
        "modularity",
        "composition"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "sae-universal-languages",
      "title": "Sparse Autoencoders Reveal Universal Feature Spaces Across Languages",
      "authors": "Various",
      "date": "2024-09-12",
      "url": "https://arxiv.org/abs/2409.08101",
      "abstract": "SAE features are remarkably consistent across models trained on different languages.",
      "tags": [
        "SAE",
        "universality",
        "multilingual"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "jumprelu-sae",
      "title": "Jumping Ahead: Improving Reconstruction Fidelity with JumpReLU Sparse Autoencoders",
      "authors": "Google DeepMind",
      "date": "2024-08-20",
      "url": "https://arxiv.org/abs/2408.09543",
      "abstract": "Introduces JumpReLU activation for SAEs, improving reconstruction fidelity while maintaining sparsity.",
      "tags": [
        "SAE",
        "architecture",
        "methods"
      ],
      "source": "DeepMind",
      "featured": false
    },
    {
      "id": "practical-review-mi",
      "title": "A Practical Review of Mechanistic Interpretability for Transformer-Based Language Models",
      "authors": "Rai, Zhou, et al.",
      "date": "2024-07-03",
      "url": "https://arxiv.org/abs/2407.02646",
      "abstract": "A practical guide to mechanistic interpretability research, from problem formulation to validation.",
      "tags": [
        "survey",
        "tutorial",
        "methodology"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "july-2024-update",
      "title": "Circuits Updates - July 2024",
      "authors": "Anthropic Interpretability Team",
      "date": "2024-07-01",
      "url": "https://transformer-circuits.pub/2024/july-update/index.html",
      "abstract": "Research updates on factual recall mechanisms, detokenization in early layers, and attention heads.",
      "tags": [
        "circuits",
        "factual-recall",
        "attention"
      ],
      "source": "Anthropic",
      "featured": false
    },
    {
      "id": "transcoders",
      "title": "Transcoders Find Interpretable LLM Feature Circuits",
      "authors": "Jacob Dunefsky, et al.",
      "date": "2024-06-17",
      "url": "https://arxiv.org/abs/2406.11944",
      "abstract": "Introduces transcoders that approximate MLP layers with wider, sparsely-activating layers for better circuit discovery.",
      "tags": [
        "transcoders",
        "circuits",
        "methods"
      ],
      "source": "arXiv",
      "featured": true
    },
    {
      "id": "openai-sae",
      "title": "Scaling and Evaluating Sparse Autoencoders",
      "authors": "OpenAI",
      "date": "2024-06-10",
      "url": "https://openai.com/research/sparse-autoencoders",
      "abstract": "OpenAI's comprehensive study on scaling SAEs and evaluating their properties.",
      "tags": [
        "SAE",
        "scaling",
        "evaluation"
      ],
      "source": "OpenAI",
      "featured": true
    },
    {
      "id": "scaling-monosemanticity",
      "title": "Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet",
      "authors": "Anthropic Interpretability Team",
      "date": "2024-05-21",
      "url": "https://transformer-circuits.pub/2024/scaling-monosemanticity/",
      "abstract": "Landmark paper extracting millions of interpretable features from Claude 3 Sonnet, finding multilingual, multimodal, and safety-relevant features.",
      "tags": [
        "SAE",
        "features",
        "scaling",
        "safety"
      ],
      "source": "Anthropic",
      "featured": true
    },
    {
      "id": "sae-circuits-scalable",
      "title": "Sparse Autoencoders Enable Scalable and Reliable Circuit Identification in Language Models",
      "authors": "Various",
      "date": "2024-05-12",
      "url": "https://arxiv.org/abs/2405.12522",
      "abstract": "Achieves higher precision and recall in circuit discovery while reducing runtime from hours to seconds.",
      "tags": [
        "SAE",
        "circuits",
        "efficiency"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "principled-sae-eval",
      "title": "Towards Principled Evaluations of Sparse Autoencoders for Interpretability and Control",
      "authors": "Various",
      "date": "2024-05-08",
      "url": "https://arxiv.org/abs/2405.06543",
      "abstract": "Develops principled evaluation frameworks for SAE quality beyond reconstruction loss.",
      "tags": [
        "SAE",
        "evaluation",
        "methods"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "mi-safety-review",
      "title": "Mechanistic Interpretability for AI Safety - A Review",
      "authors": "Leonard Bereska, et al.",
      "date": "2024-04-22",
      "url": "https://arxiv.org/abs/2404.14082",
      "abstract": "Comprehensive review of mechanistic interpretability from an AI safety perspective.",
      "tags": [
        "survey",
        "safety",
        "review"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "gated-sae",
      "title": "Improving Dictionary Learning with Gated Sparse Autoencoders",
      "authors": "Google DeepMind",
      "date": "2024-04-10",
      "url": "https://arxiv.org/abs/2404.16014",
      "abstract": "Introduces gated SAE architecture for improved feature learning.",
      "tags": [
        "SAE",
        "architecture",
        "methods"
      ],
      "source": "DeepMind",
      "featured": false
    },
    {
      "id": "jan-2024-update",
      "title": "Circuits Updates - January 2024",
      "authors": "Anthropic Interpretability Team",
      "date": "2024-01-15",
      "url": "https://transformer-circuits.pub/2024/jan-update/index.html",
      "abstract": "Updates on interpretability research including progress on understanding model computations.",
      "tags": [
        "circuits",
        "update"
      ],
      "source": "Anthropic",
      "featured": false
    },
    {
      "id": "linear-representations",
      "title": "The Linear Representation Hypothesis and the Geometry of Large Language Models",
      "authors": "Various",
      "date": "2023-11-06",
      "url": "https://arxiv.org/abs/2311.03658",
      "abstract": "Investigates the linear representation hypothesis and implications for how LLMs encode concepts.",
      "tags": [
        "theory",
        "representations",
        "geometry"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "attribution-patching",
      "title": "Attribution Patching: Activation Patching At Industrial Scale",
      "authors": "Various",
      "date": "2023-10-16",
      "url": "https://arxiv.org/abs/2310.10348",
      "abstract": "Scales activation patching using gradient-based attribution for efficient circuit discovery.",
      "tags": [
        "methods",
        "circuits",
        "scaling"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "towards-monosemanticity",
      "title": "Towards Monosemanticity: Decomposing Language Models With Dictionary Learning",
      "authors": "Anthropic Interpretability Team",
      "date": "2023-10-04",
      "url": "https://transformer-circuits.pub/2023/monosemantic-features/",
      "abstract": "Foundational paper showing sparse autoencoders can extract monosemantic features from transformers.",
      "tags": [
        "SAE",
        "features",
        "dictionary-learning"
      ],
      "source": "Anthropic",
      "featured": true
    },
    {
      "id": "representation-engineering",
      "title": "Representation Engineering: A Top-Down Approach to AI Transparency",
      "authors": "Dan Hendrycks, Collin Burns, et al.",
      "date": "2023-10-02",
      "url": "https://arxiv.org/abs/2310.01405",
      "abstract": "Introduces representation engineering for identifying and manipulating high-level concepts like honesty.",
      "tags": [
        "representation",
        "concepts",
        "methods"
      ],
      "source": "arXiv",
      "featured": true
    },
    {
      "id": "sae-original",
      "title": "Sparse Autoencoders Find Highly Interpretable Features in Language Models",
      "authors": "Various",
      "date": "2023-09-15",
      "url": "https://arxiv.org/abs/2309.08600",
      "abstract": "Foundational paper on using SAEs to resolve superposition and find monosemantic features.",
      "tags": [
        "SAE",
        "features",
        "foundational"
      ],
      "source": "arXiv",
      "featured": true
    },
    {
      "id": "tuned-lens",
      "title": "Eliciting Latent Predictions from Transformers with the Tuned Lens",
      "authors": "Nostalgebraist, et al.",
      "date": "2023-03-14",
      "url": "https://arxiv.org/abs/2303.08112",
      "abstract": "The tuned lens for projecting intermediate activations to vocabulary space.",
      "tags": [
        "methods",
        "probing",
        "predictions"
      ],
      "source": "arXiv",
      "featured": false
    },
    {
      "id": "superposition",
      "title": "Toy Models of Superposition",
      "authors": "Anthropic Interpretability Team",
      "date": "2022-09-14",
      "url": "https://transformer-circuits.pub/2022/toy_model/index.html",
      "abstract": "Foundational paper on how neural networks represent more features than dimensions using nearly-orthogonal directions.",
      "tags": [
        "superposition",
        "theory",
        "foundational"
      ],
      "source": "Anthropic",
      "featured": true
    },
    {
      "id": "induction-heads",
      "title": "In-context Learning and Induction Heads",
      "authors": "Anthropic Interpretability Team",
      "date": "2022-03-08",
      "url": "https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/",
      "abstract": "Identifies induction heads as a key circuit for in-context learning, forming via phase transition during training.",
      "tags": [
        "circuits",
        "in-context-learning",
        "attention"
      ],
      "source": "Anthropic",
      "featured": true
    },
    {
      "id": "rome",
      "title": "Locating and Editing Factual Associations in GPT",
      "authors": "Kevin Meng, David Bau, Alex Andonian, Yonatan Belinkov",
      "date": "2022-02-10",
      "url": "https://arxiv.org/abs/2202.05262",
      "abstract": "Introduces causal tracing and ROME for locating and editing factual knowledge.",
      "tags": [
        "editing",
        "factual-knowledge",
        "methods"
      ],
      "source": "arXiv",
      "featured": true
    },
    {
      "id": "mathematical-framework",
      "title": "A Mathematical Framework for Transformer Circuits",
      "authors": "Anthropic Interpretability Team",
      "date": "2021-12-22",
      "url": "https://transformer-circuits.pub/2021/framework/",
      "abstract": "Foundational paper establishing the mathematical framework for analyzing transformer circuits.",
      "tags": [
        "theory",
        "foundational",
        "circuits"
      ],
      "source": "Anthropic",
      "featured": true
    }
  ]
}